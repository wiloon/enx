# Generic SQLite P2P Sync Service (ENX Data Service)

**üìå Quick Summary**: This document designs a **universal, reusable SQLite synchronization service** with P2P capabilities. While originally designed for the ENX vocabulary learning app, the service is completely **generic and table-agnostic**, making it suitable for any SQLite-based application that needs multi-device data synchronization.

**üéØ Design Philosophy**:
- ‚úÖ **Sync-Only Architecture**: data-service is a background sync daemon, not a data access layer
- ‚úÖ **Pull-Based Sync**: Periodic polling for changes (simple, reliable, fault-tolerant)
- ‚úÖ **Direct Database Access**: enx-api accesses local SQLite directly (optimal performance)
- ‚úÖ **Configuration-Driven**: YAML config instead of code changes
- ‚úÖ **ENX as User**: ENX is the first application using this generic service
- ‚úÖ **Open-Source Goal**: Build for the broader SQLite community

## Document Information

| Field | Value |
|-------|-------|
| **Created** | 2025-11-12 |
| **Last Updated** | 2026-01-03 (Architecture redesign: sync-only service) |
| **Author** | wiloon |
| **AI Assisted** | Yes (GitHub Copilot) |
| **AI Model** | Claude Sonnet 4.5 |
| **Version** | 0.2.0 |
| **Status** | Proposed |

## üö® Architecture Update (2026-01-03)

**Critical Design Change**: data-service is now a **sync-only background daemon**, not a data access gateway.

### Previous Architecture (‚ùå Rejected)
```
enx-api ‚Üí gRPC ‚Üí data-service ‚Üí SQLite
         (overhead)
```
- **Problems**: 
  - Every CRUD operation requires gRPC call (network overhead)
  - High-frequency queries (translations) suffer from serialization cost
  - Over-engineered for single-user multi-device scenario

### New Architecture (‚úÖ Adopted)
```
enx-api ‚Üí Direct SQLite access (local)
          ‚Üì Writes update `updated_at`
          
data-service ‚Üí Monitors changes ‚Üí Pull from peers ‚Üí Merge
              (background daemon)
```
- **Benefits**:
  - ‚úÖ Optimal performance: Local database access (microseconds)
  - ‚úÖ Simple architecture: API handles business logic, data-service handles sync
  - ‚úÖ Clear separation: CRUD vs synchronization
  - ‚úÖ Pull-based sync: Simple, reliable, fault-tolerant

### Sync Strategy: Pull-Based (Polling)

**Why Pull over Push:**
- ‚úÖ **Simplicity**: Each node independently polls peers (no coordination needed)
- ‚úÖ **Fault Tolerance**: Node offline? No problem, resumes when back online
- ‚úÖ **Batch Efficiency**: Pull multiple changes in one request
- ‚úÖ **No Push Queue**: No need for retry logic, persistent queues, or failure handling

**Implementation:**
```go
// Each node runs this periodically (every 30 seconds)
func SyncWithPeers() {
    for _, peer := range config.Peers {
        lastSync := getLastSyncTime(peer)
        
        // HTTP GET: /sync/changes?since=<timestamp>
        changes := pullChangesFrom(peer, lastSync)
        
        // Merge using timestamp-based conflict resolution
        applyChangesToLocalDB(changes)
        
        // Update last sync timestamp
        updateLastSyncTime(peer, now())
    }
}
```

**Trade-offs:**
- ‚ö†Ô∏è Sync delay: 30 seconds (acceptable for this use case)
- ‚ö†Ô∏è Polling overhead: Minimal (one HTTP request per peer per 30s)
- ‚úÖ Reliability: Simple, proven pattern
- ‚úÖ Scalability: 2-3 nodes (perfect for personal use)

### Table Management: Which Service Owns What?

**Current Table Assignment:**

| Table | Managed By | Schema Location | Sync Support | Rationale |
|-------|-----------|----------------|--------------|-----------|
| `users` | enx-api | enx-api/schema.sql | ‚ùå No | User accounts are environment-specific (no need to sync) |
| `sessions` | enx-api | enx-api/schema.sql | ‚ùå No | Sessions are temporary, local only |
| `words` | data-service | data-service/schema/schema.sql | ‚úÖ Yes | Core vocabulary data, needs P2P sync across devices |
| `user_dicts` | data-service | data-service/schema/schema.sql | ‚úÖ Yes | User learning progress, needs P2P sync across devices |
| `sync_state` | data-service | data-service/schema/schema.sql | ‚úÖ Internal | Tracks sync timestamps for P2P coordination |

**Migration History:**
- **2026-01-03**: Migrated `user_dicts` from enx-api to data-service
  - **Reason**: Enable P2P sync of user learning progress across devices
  - **Changed**: `word_id` from `INTEGER` to `TEXT` (UUID) to avoid ID conflicts
  - **Impact**: Discarded historical data (acceptable for side project)

**Design Principles:**
1. ‚úÖ **Data that needs cross-device sync** ‚Üí data-service manages it
2. ‚úÖ **Environment-specific data** (users, sessions) ‚Üí enx-api manages it
3. ‚úÖ **UUID primary keys required** for synced tables (avoid ID conflicts)
4. ‚úÖ **timestamp-based conflict resolution** (updated_at field required)

**Access Pattern:**
```
enx-api behavior:
‚îú‚îÄ Direct SQLite access (GORM) for:
‚îÇ  ‚îú‚îÄ users (authentication, local accounts)
‚îÇ  ‚îî‚îÄ sessions (temporary login state)
‚îÇ
‚îî‚îÄ Direct SQLite access for:
   ‚îú‚îÄ words (read/write via GORM)
   ‚îî‚îÄ user_dicts (read/write via GORM)

data-service behavior (background daemon):
‚îú‚îÄ Pull sync every 30 seconds:
‚îÇ  ‚îú‚îÄ Fetch changes from peer nodes
‚îÇ  ‚îî‚îÄ Merge using timestamp conflict resolution
‚îÇ
‚îî‚îÄ Manage schema for:
   ‚îú‚îÄ words (vocabulary entries)
   ‚îú‚îÄ user_dicts (learning progress)
   ‚îî‚îÄ sync_state (sync coordination)
```

**Why user_dicts moved to data-service:**
- **Before**: INTEGER word_id incompatible with UUID-based words table
- **After**: TEXT word_id (UUID), consistent with words table
- **Benefit**: Learning progress (`query_count`, `already_acquainted`) syncs across all devices

## ‚ö° Critical Design Decision

**This data service is designed as a GENERIC, reusable tool, not ENX-specific.**

| Aspect | Decision |
|--------|----------|
| **Service Type** | ‚úÖ Generic SQLite P2P sync service (universal) |
| **API Design** | ‚úÖ Table-agnostic (Find/Query/Insert/Update/Delete) |
| **Configuration** | ‚úÖ YAML-driven (no hardcoded schemas) |
| **Business Logic** | ‚ùå None in data service (stays in enx-api) |
| **Target Users** | ‚úÖ Any SQLite-based application needing P2P sync |
| **ENX Role** | First real-world user & validation case |

## üö® Critical Prerequisites

**‚ö†Ô∏è WARNING: These requirements are MANDATORY for the sync system to work correctly. Failure to meet them will result in data inconsistencies and potential data loss.**

### 1. Clock Synchronization (CRITICAL)

**Why it matters**: The entire conflict resolution system is based on timestamp comparison. Without synchronized clocks, the system will merge data in the wrong order, potentially causing data loss.

| Requirement | Details |
|-------------|----------|
| **NTP Sync** | ‚úÖ REQUIRED: All nodes MUST have NTP enabled |
| **Max Clock Skew** | ‚ö†Ô∏è Must be < 5 seconds between any two nodes |
| **Verification** | ‚úÖ Automatic check on service startup (fails if NTP disabled) |
| **Consequences** | ‚ùå Wrong merge order, data overwritten incorrectly, silent data loss |

**Before enabling P2P sync, verify on EACH node:**

```bash
# 1. Check NTP status (REQUIRED)
timedatectl status

# Expected output:
#   System clock synchronized: yes  ‚úÖ
#   NTP service: active            ‚úÖ

# If NTP is disabled, enable it NOW:
sudo timedatectl set-ntp true

# 2. Compare times across all nodes (should differ by < 1 second)
date +"%Y-%m-%d %H:%M:%S.%3N"

# Example output from 3 nodes:
# Desktop: 2025-12-30 15:30:45.123
# MacBook: 2025-12-30 15:30:45.234  (diff: 111ms ‚úÖ)
# Ubuntu:  2025-12-30 15:30:45.089  (diff: 34ms ‚úÖ)

# ‚ö†Ô∏è If times differ by > 5 seconds, DO NOT enable sync until fixed!
```

**Startup Check (Automatic)**:

The data service will automatically verify clock synchronization on startup:

```go
// Service startup sequence
func main() {
    log.Println("üöÄ Starting enx-data-service...")
    
    // STEP 1: Verify clock synchronization (BLOCKING)
    if err := verifyClockSync(); err != nil {
        log.Fatalf("‚ùå CRITICAL: Clock sync check failed: %v", err)
        log.Fatal("   Please enable NTP: sudo timedatectl set-ntp true")
        // Service will NOT start if NTP is disabled
    }
    log.Println("‚úÖ Clock synchronization verified")
    
    // STEP 2: Initialize database
    db := initDatabase("./enx.db")
    
    // STEP 3: Start services
    // ...
}
```

**What happens if clocks are not synchronized:**

```
‚ùå Real Example of Data Loss:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Scenario: Desktop clock is 10 minutes fast (NTP disabled)

Node A (Desktop, clock fast):      Node B (MacBook, correct):
  10:10 AM - Add word "algorithm"    10:00 AM - Add word "database"
  
Sync happens:
  Node B receives "algorithm" with timestamp 10:10
  10:10 > 10:00 ‚Üí Desktop's record wins ‚úÖ
  
But 5 minutes later...
  Node A (Desktop):                  Node B (MacBook):
  10:15 - Mark "algorithm" learned   10:05 - Mark "database" learned
  
Sync happens again:
  10:15 > 10:05 ‚Üí Desktop wins again
  
Problem: Desktop's clock being fast means it ALWAYS wins,
         even if MacBook's changes are actually newer!
         MacBook's "database" might never sync properly.

‚úÖ Solution: Enable NTP on all nodes BEFORE enabling sync.
```

### 2. Network Configuration

**For Home LAN Setup (recommended for intermittent connectivity):**

| Requirement | Details |
|-------------|----------|
| **Network Segment** | All nodes should be on same subnet (e.g., 192.168.1.x) |
| **Firewall** | Allow TCP port 8091 between nodes |
| **Service Discovery** | Use static IP or hostname in config (mDNS optional for future) |
| **Connectivity Test** | Nodes should be able to ping each other |

**Verify network connectivity:**

```bash
# From each node, ping other nodes
ping 192.168.1.10  # Desktop
ping 192.168.1.20  # MacBook  
ping 192.168.1.30  # Ubuntu laptop

# Test port connectivity
nc -zv 192.168.1.10 8091
nc -zv 192.168.1.20 8091
nc -zv 192.168.1.30 8091
```

### 3. Database Schema Requirements

**All tables that need P2P sync MUST have:**

| Field | Type | Requirement | Purpose |
|-------|------|-------------|----------|
| `id` | TEXT (UUID) | ‚úÖ MANDATORY | Primary key, avoid ID conflicts |
| `update_datetime` | INTEGER | ‚úÖ MANDATORY | Unix timestamp (milliseconds) for conflict resolution |
| `deleted_at` | INTEGER | ‚ö†Ô∏è RECOMMENDED | Soft delete support (NULL = not deleted) |

**‚ùå Tables with auto-increment INTEGER IDs cannot be synced** (will cause ID conflicts)

### 4. Backup Before First Sync

**‚ö†Ô∏è IMPORTANT: Always backup your database before enabling P2P sync for the first time**

```bash
# Backup current database
cp ~/.local/share/enx/enx.db ~/.local/share/enx/enx.db.backup-$(date +%Y%m%d)

# Verify backup
ls -lh ~/.local/share/enx/*.backup*
```

### Quick Start Checklist

Before enabling P2P sync, verify:

- [ ] ‚úÖ NTP enabled on ALL nodes (`timedatectl status`)
- [ ] ‚úÖ Clock skew < 5 seconds between nodes
- [ ] ‚úÖ Nodes can ping each other
- [ ] ‚úÖ Port 8091 open on all nodes
- [ ] ‚úÖ Database schema has UUID primary keys
- [ ] ‚úÖ Database schema has `update_datetime` field
- [ ] ‚úÖ Database backed up
- [ ] ‚úÖ Static IPs or hostnames configured

**Only proceed with sync setup after all items are checked!**

**What this means:**
- **enx-data-service**: Generic CRUD + P2P sync for ANY SQLite database
- **enx-api**: ENX-specific business logic (word learning, user management, etc.)
- **Future**: Open-source the data service for broader community benefit

**Quick Comparison:**

```
‚ùå ENX-Specific Approach (Rejected):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
service ENXDataService {
  rpc GetWord(...)           // Only works with words table
  rpc MarkWordLearned(...)   // ENX business logic in data layer
  rpc GetUserStats(...)      // Hardcoded for ENX schema
}
‚Üí Problem: Not reusable, tightly coupled to ENX

‚úÖ Generic Approach (Chosen):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
service GenericDataService {
  rpc Find(...)              // Works with ANY table
  rpc Query(...)             // Raw SQL for flexibility
  rpc Update(...)            // Generic CRUD operations
}
‚Üí Benefit: Reusable for blogs, tasks, notes, any SQLite app
‚Üí ENX uses: client.Find(table="words", filter={...})
```

## Overview

This document describes the architecture design for separating ENX into two services: **enx-api** (application layer) and **enx-data-service** (generic data layer with P2P sync capabilities).

**üéØ Key Design Decision**: enx-data-service is designed as a **generic, reusable SQLite synchronization service** that works with any SQLite database, not just ENX. This allows us to:
- Build a universal tool for the SQLite community
- Benefit from broader testing and community contributions
- Use ENX as the first real-world validation case
- Potentially open-source the tool to help other developers

**ENX-specific business logic** (word learning, user preferences, etc.) remains in **enx-api**, while **generic data operations** (CRUD, sync, storage) are handled by **enx-data-service**.

## Problem Statement

### The Challenge

ENX is a **side project** with specific multi-environment development challenges:

1. **Long Development Cycle**: Development will continue over an extended period (months to years)
2. **Multiple Development Environments**:
   - **Desktop Linux**: Primary development environment (always connected to home LAN)
   - **MacBook**: Development + usage while traveling (intermittent connection)
   - **Ubuntu Laptop (Intermittent Isolation)**: Used in restricted network environment for hours, then connects to home LAN for sync
3. **Active Usage During Development**: The application is actively used while being developed (common for side projects)
4. **Data Fragmentation Across Environments**: Different environments accumulate different data over time, requiring intelligent merging
5. **Offline-First Requirement**: Network-isolated environment must work without internet connection
6. **Intermittent Connectivity** (Key Scenario): Ubuntu laptop usage pattern
   - **Typical usage**: Work offline for hours (e.g., 9 AM - 3 PM)
   - **Environment**: Isolated network with no external access
   - **Reconnection**: Join home LAN later, automatic sync triggers
   - **Implementation**: Network monitoring + opportunistic sync
7. **No Concurrent Access (Currently)**: Only one environment is used at any given time
   - **Current state**: No production environment yet, so no concurrent writes
   - **Future consideration**: If production environment is added, concurrent access may become a requirement
   - **Design implication**: Current design focuses on eventual consistency, not real-time multi-master sync

### Real-World Scenarios

**Scenario 1: Regular Switching**
```
Monday: Working on desktop Linux
  - Added 50 new words while reading technical articles

Friday: Taking a trip, using MacBook
  - Added 20 words while reading on the plane
  - Need access to Monday's 50 words ‚ùå (not synced)
```

**Scenario 2: Intermittent Network Access**
```
Saturday 9:00 AM: Working in network-isolated Ubuntu environment
  - Disconnect from home LAN, work on isolated project
  - Cannot access cloud services (security requirement)
  - Added 30 words while working
  - Modified learning progress on 15 words

Saturday 3:00 PM: Project work finished, back home
  - Ubuntu reconnects to home LAN ‚úÖ
  - Automatic P2P sync triggers
  - 30 words + progress sync to Desktop/MacBook ‚úÖ
  - Receive changes from other nodes ‚úÖ

Result: All nodes synchronized after reconnection
```

**Scenario 3: Intermittent Network (Ubuntu Laptop)**
```
Saturday Morning (Ubuntu disconnected from LAN):
  - Working on isolated project for 4 hours
  - Added 30 new words
  - Marked 15 words as learned
  - All changes stored locally in SQLite ‚úÖ

Saturday Afternoon (Ubuntu reconnects to home LAN):
  - enx-data-service detects network available
  - Connects to Desktop/MacBook on LAN
  - Pulls changes since last sync (Desktop added 20 words)
  - Pushes local changes (30 words + learning progress)
  - Merges using timestamps ‚úÖ
  
Result: All 3 nodes have 50 new words total, fully synchronized
```

**Scenario 4: Data Inconsistency**
```
Current state:
  - Desktop Linux: 1000 words, 500 marked as learned
  - MacBook: 950 words, 480 marked as learned
  - Ubuntu laptop: 920 words, 450 marked as learned (was offline)

Problem: Which is the "correct" version?
Answer: All of them! Each has unique data that should be merged.
```

### üîë Key Use Case: Intermittent Network Connectivity

**The Ubuntu Laptop Scenario** (Primary design driver):

```
Real-world usage pattern:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Saturday Morning (9:00 AM):
  ‚úÖ Ubuntu laptop connected to home LAN (192.168.1.x)
  ‚úÖ All nodes synced (Desktop, MacBook, Ubuntu all at 1000 words)
  
  üì¥ Disconnect laptop, go to isolated work environment
  
During Isolated Work (9:15 AM - 3:00 PM):
  üîí No external network access (security requirement)
  ‚úçÔ∏è  Continue using ENX: add words, mark learning progress
  üíæ All changes stored locally in SQLite
  ‚è≥ Cannot sync with other nodes (offline)
  
Back Home (3:00 PM):
  üîå Reconnect to home LAN
  üîç Network monitor detects connection restored
  üîÑ Automatic sync triggers:
      ‚Ä¢ Pull changes from Desktop (4 words added during day)
      ‚Ä¢ Push changes to Desktop/MacBook (5 words added offline)
      ‚Ä¢ Merge using timestamps (conflict-free)
  ‚úÖ All 3 nodes now synchronized (1009 words each)
  
Key Point: Works offline for 6 hours, syncs in seconds when reconnected!
```

**Why this pattern matters:**
- ‚úÖ **Realistic**: Many developers work in isolated environments (secure labs, air-gapped systems)
- ‚úÖ **Flexible**: Offline duration can be hours, days, or weeks - design handles all cases
- ‚úÖ **Automatic**: No manual export/import, no cloud dependencies
- ‚úÖ **Reliable**: Timestamp-based merge ensures data integrity

### Why This Design?

Given these challenges, the traditional solutions don't work:

‚ùå **Centralized Server**:
- Doesn't work in network-isolated environment
- Requires constant internet connection
- Single point of failure

‚ùå **Cloud File Sync (Dropbox/Google Drive)**:
- Delay in synchronization (2-10 seconds)
- Risk of file corruption with SQLite
- No intelligent conflict resolution
- Requires cloud sync client on all platforms

‚ùå **Manual Database Copy**:
- Error-prone
- Time-consuming
- No automatic conflict resolution
- Easy to forget

### Alternative Solutions Analysis

**Before committing to the custom P2P sync solution, let's evaluate other viable options for your specific use case:**

#### Option 1: Syncthing (File-Based P2P Sync) ‚≠ê‚≠ê‚≠ê‚≠ê

**What is it**: Open-source continuous file synchronization tool that works on a P2P basis.

**How it works**:
```
Desktop Linux (192.168.1.10)
‚îú‚îÄ‚îÄ ~/.local/share/enx/enx.db  ‚Üê‚Üí Syncthing ‚Üê‚Üí MacBook (192.168.1.20)
                                              ‚îî‚îÄ‚îÄ ~/.local/share/enx/enx.db
                                              
                                    ‚Üì‚Üë P2P sync when available
                                    
Ubuntu Laptop (192.168.1.30)
‚îî‚îÄ‚îÄ ~/.local/share/enx/enx.db
```

**Pros**:
- ‚úÖ **Zero code required**: Just install Syncthing and configure folder
- ‚úÖ **Battle-tested**: Used by millions, mature project
- ‚úÖ **True P2P**: No central server needed
- ‚úÖ **Offline support**: Works perfectly with intermittent connectivity
- ‚úÖ **Cross-platform**: Works on Linux, macOS, Windows
- ‚úÖ **Versioning**: Can keep old file versions for recovery
- ‚úÖ **Ignore patterns**: Can exclude temporary files
- ‚úÖ **Conflict handling**: Creates `.sync-conflict` files when needed

**Cons**:
- ‚ö†Ô∏è **SQLite corruption risk**: If two nodes modify database simultaneously (rare in your case)
- ‚ö†Ô∏è **Conflict files**: Manual merge needed if conflicts occur
- ‚ö†Ô∏è **No schema awareness**: Syncs entire file, not record-level
- ‚ö†Ô∏è **Extra daemon**: Need to run Syncthing on each device

**Setup complexity**: Low (30 minutes)

**Recommendation for your use case**: 
```
‚úÖ EXCELLENT FIT because:
  ‚Ä¢ You never use multiple devices simultaneously (no concurrent writes)
  ‚Ä¢ Intermittent connectivity is perfectly supported
  ‚Ä¢ Home LAN environment (Syncthing excels here)
  ‚Ä¢ SQLite is a single file (easy to sync)
  
‚ö†Ô∏è Risk mitigation:
  ‚Ä¢ Add "last_modified_node" field to track which device made changes
  ‚Ä¢ Use WAL mode for better concurrent access safety
  ‚Ä¢ Enable Syncthing versioning to recover from any issues
```

**Quick Start**:
```bash
# Install Syncthing on all devices
sudo apt install syncthing  # Ubuntu/Debian
brew install syncthing      # macOS

# Start Syncthing
syncthing

# Open web UI: http://localhost:8384
# Add folder: ~/.local/share/enx/
# Add devices: Desktop, MacBook, Ubuntu laptop
# Enable "Ignore Delete" on all devices (safer for databases)
```

---

#### Option 2: Git-Based Sync (Version Control) ‚≠ê‚≠ê‚≠ê

**What is it**: Use Git to version and sync SQLite database file.

**How it works**:
```bash
# On each device
cd ~/.local/share/enx/
git init
git add enx.db
git commit -m "Update from Desktop"

# When switching devices
git pull origin main --rebase
git push origin main
```

**Pros**:
- ‚úÖ **Developer-friendly**: You already know Git
- ‚úÖ **Full history**: Every change is versioned
- ‚úÖ **Conflict detection**: Git will warn about conflicts
- ‚úÖ **Works offline**: Commit locally, push when online
- ‚úÖ **Free hosting**: GitHub/GitLab for backup

**Cons**:
- ‚ùå **Binary file**: Git isn't optimized for SQLite (large repo over time)
- ‚ùå **Manual process**: Must remember to commit/push/pull
- ‚ùå **Conflict resolution**: Manual merge of binary files is impossible
- ‚ùå **Repo bloat**: Every change adds to repo size

**Setup complexity**: Medium (need to establish workflow)

**Recommendation for your use case**:
```
‚ö†Ô∏è NOT IDEAL because:
  ‚Ä¢ Binary file makes Git inefficient
  ‚Ä¢ Merge conflicts are impossible to resolve
  ‚Ä¢ Requires manual discipline (easy to forget)
  
Could work with:
  ‚Ä¢ Export/import scripts (SQL dumps instead of binary)
  ‚Ä¢ Strict single-device-at-a-time rule
  ‚Ä¢ Regular repo cleanup (git gc --aggressive)
```

---

#### Option 3: Centralized Server + Offline Queue ‚≠ê‚≠ê‚≠ê‚≠ê

**What is it**: Central PostgreSQL server, with local SQLite cache and sync queue.

**Architecture**:
```
Desktop/MacBook/Ubuntu (when online):
‚îú‚îÄ‚îÄ enx-api (HTTP REST)
‚îî‚îÄ‚îÄ PostgreSQL (central server at home)

Ubuntu Laptop (offline):
‚îú‚îÄ‚îÄ enx-api (HTTP REST)
‚îú‚îÄ‚îÄ SQLite (local cache)
‚îî‚îÄ‚îÄ Sync queue (pending operations)
    ‚îî‚îÄ‚îÄ On reconnect ‚Üí replay to PostgreSQL
```

**Pros**:
- ‚úÖ **Single source of truth**: PostgreSQL is always correct
- ‚úÖ **Offline capable**: SQLite cache works offline
- ‚úÖ **Scalable**: Can add more clients easily
- ‚úÖ **Query power**: PostgreSQL for complex queries

**Cons**:
- ‚ö†Ô∏è **Requires server**: Must run PostgreSQL at home (can be Desktop)
- ‚ö†Ô∏è **Complex sync logic**: Need to implement queue replay
- ‚ö†Ô∏è **Network dependency**: Ubuntu laptop must connect to home network
- ‚ö†Ô∏è **More moving parts**: PostgreSQL + sync service + queue

**Setup complexity**: High (need to build sync queue logic)

**Recommendation for your use case**:
```
‚úÖ VIABLE if you want:
  ‚Ä¢ Production-ready database (PostgreSQL)
  ‚Ä¢ Single source of truth
  ‚Ä¢ Future scalability
  
‚ö†Ô∏è Overhead:
  ‚Ä¢ Must implement offline queue yourself
  ‚Ä¢ Need to run PostgreSQL server (though can be on Desktop)
  ‚Ä¢ More complex than P2P or Syncthing
```

---

#### Option 4: Cloud Backend + Local Sync (PocketBase) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**What is it**: PocketBase (or similar) provides built-in real-time sync.

**How it works**:
```
PocketBase Server (on Desktop Linux):
‚îú‚îÄ‚îÄ SQLite database
‚îú‚îÄ‚îÄ Real-time subscriptions
‚îî‚îÄ‚îÄ REST + WebSocket API

All clients:
‚îú‚îÄ‚îÄ PocketBase SDK
‚îú‚îÄ‚îÄ Local cache
‚îî‚îÄ‚îÄ Auto-sync when online
```

**Pros**:
- ‚úÖ **Built-in sync**: PocketBase handles sync logic
- ‚úÖ **Real-time**: WebSocket updates
- ‚úÖ **Offline support**: SDK caches data locally
- ‚úÖ **Admin UI**: Built-in dashboard
- ‚úÖ **Authentication**: Built-in user auth
- ‚úÖ **Fast development**: REST API auto-generated

**Cons**:
- ‚ö†Ô∏è **Central server**: Must run PocketBase (can be on Desktop)
- ‚ö†Ô∏è **Technology switch**: Different from current SQLite approach
- ‚ö†Ô∏è **Learning curve**: New framework to learn
- ‚ö†Ô∏è **Lock-in**: Tied to PocketBase ecosystem

**Setup complexity**: Medium (new technology, but well documented)

**Recommendation for your use case**:
```
‚úÖ EXCELLENT if starting fresh:
  ‚Ä¢ Modern, batteries-included solution
  ‚Ä¢ Handles sync, auth, admin UI out of box
  ‚Ä¢ Active development, good community
  
‚ö†Ô∏è Consider if:
  ‚Ä¢ Willing to migrate from current architecture
  ‚Ä¢ Want real-time sync features
  ‚Ä¢ Okay with running a server (can be local)
```

---

#### Option 5: Custom P2P Data Service (Current Design) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**What it is**: Your proposed enx-data-service with gRPC and timestamp-based sync.

**Pros**:
- ‚úÖ **Full control**: Exactly what you need, no compromises
- ‚úÖ **Learning opportunity**: Build distributed sync system
- ‚úÖ **Open source potential**: Generic tool for community
- ‚úÖ **No dependencies**: Pure Go, minimal external services
- ‚úÖ **Record-level sync**: Efficient, precise merging
- ‚úÖ **Clean architecture**: Separation of concerns

**Cons**:
- ‚ö†Ô∏è **Development time**: Need to build and test sync logic
- ‚ö†Ô∏è **Complexity**: Distributed systems are hard
- ‚ö†Ô∏è **Maintenance**: You're responsible for bugs
- ‚ö†Ô∏è **Clock sync dependency**: NTP requirement adds setup step

**Setup complexity**: High (custom development required)

**Recommendation for your use case**:
```
‚úÖ BEST if:
  ‚Ä¢ Want to learn distributed systems
  ‚Ä¢ Plan to open source the tool
  ‚Ä¢ Need full customization
  ‚Ä¢ Enjoy building infrastructure
  
‚ö†Ô∏è Requires:
  ‚Ä¢ 4-8 weeks development time (MVP)
  ‚Ä¢ Testing across multiple scenarios
  ‚Ä¢ Ongoing maintenance
```

---

### Comparison Matrix

| Solution | Offline Support | Setup Time | Development Effort | Reliability | Best For |
|----------|----------------|------------|-------------------|-------------|----------|
| **Syncthing** | ‚úÖ‚úÖ‚úÖ Excellent | 30 min | None | ‚úÖ‚úÖ‚úÖ‚úÖ Battle-tested | **Pragmatic choice** |
| **Git-based** | ‚úÖ‚úÖ Good | 1 hour | Low | ‚ö†Ô∏è‚ö†Ô∏è Risk of conflicts | Learning/experimentation |
| **Central Server + Queue** | ‚úÖ‚úÖ‚úÖ Excellent | 2 weeks | High | ‚úÖ‚úÖ‚úÖ Good | Enterprise requirements |
| **PocketBase** | ‚úÖ‚úÖ‚úÖ Excellent | 3 days | Medium | ‚úÖ‚úÖ‚úÖ‚úÖ Production-ready | Modern web apps |
| **Custom P2P (Your design)** | ‚úÖ‚úÖ‚úÖ Excellent | 6-8 weeks | Very High | ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è Need to prove | **Learning project** |

---

### My Recommendation üí°

**For your specific use case, I recommend:**

#### ü•á **First Choice: Syncthing (Pragmatic)**

**Why**: 
- ‚úÖ Solves your problem in 30 minutes instead of 8 weeks
- ‚úÖ Proven reliable with millions of users
- ‚úÖ Perfect for your single-user, intermittent connectivity scenario
- ‚úÖ Zero development/maintenance burden
- ‚úÖ Can always migrate to custom solution later if needed

**Implementation**:
```bash
# One-time setup (30 minutes total)
1. Install Syncthing on all 3 devices
2. Configure sync folder: ~/.local/share/enx/
3. Add devices to each other
4. Enable versioning for safety
5. Done! üéâ

# Risk mitigation
- Enable SQLite WAL mode (reduces corruption risk)
- Never use two devices simultaneously (you already don't)
- Syncthing versioning recovers from any issues
```

**Cost-benefit analysis**:
```
Custom P2P Sync:
  Development: 6-8 weeks ‚ùå
  Learning value: High ‚úÖ
  Maintenance: Ongoing ‚ö†Ô∏è
  Risk: Untested ‚ùå

Syncthing:
  Setup: 30 minutes ‚úÖ
  Learning value: Medium ‚úÖ
  Maintenance: None (community maintained) ‚úÖ
  Risk: Minimal (battle-tested) ‚úÖ
```

---

#### ü•à **Second Choice: Custom P2P (If learning is priority)**

**Why build custom**:
- ‚úÖ Learn distributed systems concepts
- ‚úÖ Create generic open-source tool
- ‚úÖ Full control over sync logic
- ‚úÖ Portfolio/resume project

**When to choose**:
- Want to deeply understand sync challenges
- Have 6-8 weeks for MVP development
- Enjoy building infrastructure
- Plan to open source the result

**Hybrid approach** (Best of both worlds):
```
Phase 1 (Now): Use Syncthing
  ‚úÖ Solve immediate problem
  ‚úÖ Start using ENX productively
  ‚úÖ Understand real-world sync patterns

Phase 2 (3-6 months later): Build custom sync
  ‚úÖ Real usage data informs design
  ‚úÖ Can compare with Syncthing behavior
  ‚úÖ Migrate gradually with fallback
  ‚úÖ Syncthing validates your implementation
```

---

### Decision Framework

**Choose Syncthing if**:
- ‚ùì Do you need ENX working THIS WEEK? ‚Üí Yes ‚úÖ
- ‚ùì Is learning distributed systems your PRIMARY goal? ‚Üí No
- ‚ùì Want to focus on ENX features (not infrastructure)? ‚Üí Yes ‚úÖ

**Choose Custom P2P if**:
- ‚ùì Is building sync systems your LEARNING goal? ‚Üí Yes ‚úÖ
- ‚ùì Have 6-8 weeks for infrastructure work? ‚Üí Yes ‚úÖ
- ‚ùì Plan to open source generic sync tool? ‚Üí Yes ‚úÖ
- ‚ùì Okay with being blocked if bugs occur? ‚Üí Yes ‚úÖ

**Choose PocketBase if**:
- ‚ùì Starting from scratch? ‚Üí Yes ‚úÖ
- ‚ùì Want modern web framework? ‚Üí Yes ‚úÖ
- ‚ùì Need real-time features? ‚Üí Yes ‚úÖ

---

### The Solution: ENX Data Service (Custom P2P)

Since we need to solve the data synchronization problem anyway, why not:

1. **Wrap enx.db in a service** ‚Üí enx-data-service
2. **Complete decoupling** ‚Üí enx-api never touches the database directly
3. **Unified data access** ‚Üí All database operations go through enx-data-service API
4. **Built-in synchronization** ‚Üí enx-data-service handles node-to-node sync automatically

Benefits:

- ‚úÖ **Development flexibility**: Develop on any environment, data stays in sync
- ‚úÖ **Offline-first**: Work without network, sync when available
- ‚úÖ **Intermittent connectivity**: Supports nodes that connect/disconnect periodically (like Ubuntu in isolated environment)
- ‚úÖ **Opportunistic sync**: Automatically syncs when network becomes available
- ‚úÖ **Data integrity**: Intelligent merge based on timestamps
- ‚úÖ **Clean architecture**: Business logic completely separated from data management
- ‚úÖ **Future-proof**: Easy to migrate from SQLite to PostgreSQL without touching enx-api
- ‚úÖ **Service isolation**: enx-data-service can be restarted/upgraded independently

## Sync Requirements (New)

To support the P2P sync architecture, **ALL tables that need to be synced** must adhere to the following rules:

1.  **Primary Key**: Must be a **UUID** (String). ‚ö†Ô∏è **MANDATORY for all sync tables**
    *   *Reason*: Avoids ID conflicts between nodes (e.g., Node A and Node B both creating ID=100 with auto-increment).
    *   *Migration*: Existing tables with auto-increment IDs MUST be migrated to UUID before enabling sync.
    *   *Non-sync tables*: Tables that don't need P2P sync can keep auto-increment IDs.
2.  **Timestamp Field**: Must have an `update_datetime` (or similar) field.
    *   *Reason*: Used to identify changed records since the last sync.
    *   *Note*: **Clock synchronization is required** - nodes must sync system clocks (via NTP or manual) before starting sync process.
    *   *Implementation strategy*: Use system clock (simple approach), defer complex solutions (HLC, Vector Clock) until proven necessary.
3.  **Soft Delete**: Must have an `is_deleted` (boolean) or `deleted_at` (timestamp) field.
    *   *Reason*: Physical deletions cannot be synced. Soft deletes allow "deletion" events to propagate to other nodes.

## MVP Implementation Roadmap - Minimal P2P Sync

**üéØ Goal**: Implement record-level P2P sync with minimum code and maximum practicality.

**Key Principle**: Build the simplest thing that works, test with real usage, add complexity only when proven necessary.

---

### Phase 0: Database Schema Preparation (Week 1)

**What to do**: Prepare existing `enx.db` for sync capability.

**Required Changes**:

```sql
-- 1. Migrate primary keys from auto-increment to UUID
ALTER TABLE words RENAME TO words_old;

CREATE TABLE words (
    id TEXT PRIMARY KEY,              -- UUID instead of INTEGER
    english TEXT NOT NULL,
    chinese TEXT,
    pronunciation TEXT,
    update_datetime INTEGER NOT NULL, -- Unix timestamp (milliseconds)
    deleted_at INTEGER,               -- NULL = not deleted
    -- ... other existing fields
);

-- 2. Migrate data with UUID generation
INSERT INTO words 
SELECT 
    lower(hex(randomblob(16))) as id,  -- Generate UUID v4
    english,
    chinese,
    pronunciation,
    strftime('%s', 'now') * 1000 as update_datetime,
    NULL as deleted_at,
    -- ... other fields
FROM words_old;

DROP TABLE words_old;

-- 3. Create index for sync queries
CREATE INDEX idx_words_update_datetime ON words(update_datetime);
CREATE INDEX idx_words_deleted_at ON words(deleted_at);

-- 4. Do the same for user_dicts and other sync tables
```

**Migration Files Location**:
```
enx-api/
‚îú‚îÄ‚îÄ migrations/
‚îÇ   ‚îú‚îÄ‚îÄ README.md                              # Migration documentation
‚îÇ   ‚îú‚îÄ‚îÄ 20251230_migrate_words_to_p2p.sql     # Main migration script
‚îÇ   ‚îú‚îÄ‚îÄ 20251230_migrate_words_to_p2p_down.sql # Rollback script
‚îÇ   ‚îî‚îÄ‚îÄ test_migration.sh                      # Test script
‚îî‚îÄ‚îÄ enx.db                                     # Your database
```

**How to Apply**:
```bash
cd /Users/wiloon/workspace/projects/enx/enx-api

# Step 1: Test migration on a copy (recommended)
./migrations/test_migration.sh

# Step 2: If test passes, apply to real database
cp enx.db enx.db.backup-$(date +%Y%m%d)
sqlite3 enx.db < migrations/20251230_migrate_words_to_p2p.sql

# Step 3: Verify migration
sqlite3 enx.db ".schema words"
sqlite3 enx.db "SELECT COUNT(*) FROM words;"
```

**Validation**:
```bash
# Verify schema has UUID primary key and timestamp fields
sqlite3 enx.db ".schema words"

# Verify all data migrated (should be 3 records)
sqlite3 enx.db "SELECT COUNT(*) FROM words;"

# Check sample migrated data
sqlite3 enx.db "SELECT substr(id, 1, 8) as id, english, update_datetime FROM words;"
```

**Time estimate**: 2-3 hours (including testing)

---

### Phase 1: Basic Data Service (Week 2-3)

**What to build**: Standalone data service that wraps SQLite with gRPC API.

**Scope**:
- ‚úÖ Single table: `words` only (validate concept)
- ‚úÖ Basic CRUD: Find, Insert, Update, Delete
- ‚úÖ Local operation only (NO sync yet)
- ‚ùå Skip: Authentication, monitoring, REST API

**Project structure**:
```
enx-data-service/
‚îú‚îÄ‚îÄ main.go              # Service entry point
‚îú‚îÄ‚îÄ proto/
‚îÇ   ‚îî‚îÄ‚îÄ data.proto       # gRPC definitions
‚îú‚îÄ‚îÄ service/
‚îÇ   ‚îî‚îÄ‚îÄ data_service.go  # CRUD implementation
‚îî‚îÄ‚îÄ db/
    ‚îî‚îÄ‚îÄ sqlite.go        # SQLite wrapper
```

**Minimal proto definition**:
```protobuf
syntax = "proto3";
package enx.data;

// Generic CRUD service
service DataService {
  rpc Find(FindRequest) returns (FindResponse);
  rpc Insert(InsertRequest) returns (InsertResponse);
  rpc Update(UpdateRequest) returns (UpdateResponse);
  rpc Delete(DeleteRequest) returns (DeleteResponse);
}

message FindRequest {
  string table = 1;              // "words"
  string filter_json = 2;        // {"english": "hello"}
  int32 limit = 3;
}

message FindResponse {
  repeated string records = 1;    // JSON array
}

// Similar for Insert/Update/Delete...
```

**Implementation priorities**:
```go
// 1. Core: SQLite wrapper (2 days)
type SQLiteDB struct {
    db *sql.DB
}

func (s *SQLiteDB) Find(table, filter string, limit int) ([]string, error)
func (s *SQLiteDB) Insert(table, data string) error
func (s *SQLiteDB) Update(table, filter, data string) error
func (s *SQLiteDB) Delete(table, filter string) error

// 2. gRPC service (1 day)
type DataService struct {
    db *SQLiteDB
}

// Implement Find/Insert/Update/Delete handlers

// 3. Main (1 day)
func main() {
    db := NewSQLiteDB("./enx.db")
    grpcServer := grpc.NewServer()
    pb.RegisterDataServiceServer(grpcServer, NewDataService(db))
    grpcServer.Serve(lis)
}
```

**Testing**:
```bash
# Start service
go run main.go

# Test with grpcurl
grpcurl -d '{"table":"words","filter_json":"{}","limit":10}' \
  localhost:8091 enx.data.DataService/Find
```

**Success criteria**:
- ‚úÖ Can query words via gRPC
- ‚úÖ Can insert/update/delete words
- ‚úÖ All changes reflected in SQLite
- ‚úÖ update_datetime automatically set

**Time estimate**: 1-1.5 weeks

---

### Phase 2: Basic Sync Logic (Week 4-5)

**What to build**: Add change tracking and sync between 2 nodes.

**Scope**:
- ‚úÖ Track changes since last sync (using `update_datetime`)
- ‚úÖ Pull changes from peer
- ‚úÖ Merge with timestamp comparison
- ‚ùå Skip: Authentication, encryption, conflict UI

**Add to proto**:
```protobuf
service SyncService {
  rpc GetChanges(GetChangesRequest) returns (stream Change);
  rpc ApplyChanges(stream Change) returns (ApplyChangesResponse);
}

message GetChangesRequest {
  string table = 1;               // "words"
  int64 since_timestamp = 2;      // Unix milliseconds
}

message Change {
  string table = 1;
  string operation = 2;           // "insert", "update", "delete"
  string record_json = 3;         // Full record as JSON
  int64 timestamp = 4;
}
```

**Implementation**:
```go
// 1. Change tracking (1 day)
func (s *SQLiteDB) GetChanges(table string, since int64) ([]Change, error) {
    rows, err := s.db.Query(`
        SELECT * FROM ? 
        WHERE update_datetime > ? 
        ORDER BY update_datetime ASC
    `, table, since)
    // Convert to Change objects
}

// 2. Merge logic (2 days)
func (s *SyncService) ApplyChanges(stream pb.SyncService_ApplyChangesServer) error {
    for {
        change, _ := stream.Recv()
        
        // Check if local has newer version
        local := s.db.FindByID(change.Table, change.RecordID)
        
        if local.UpdateDatetime > change.Timestamp {
            // Local is newer, skip remote change
            continue
        }
        
        // Apply remote change
        switch change.Operation {
        case "insert", "update":
            s.db.Upsert(change.Table, change.RecordJSON)
        case "delete":
            s.db.SoftDelete(change.Table, change.RecordID)
        }
    }
}

// 3. Sync coordinator (2 days)
func (s *SyncService) SyncWithPeer(peerAddr string) error {
    // 1. Get last sync timestamp from local state
    lastSync := s.getLastSyncTime(peerAddr)
    
    // 2. Pull changes from peer
    conn, _ := grpc.Dial(peerAddr)
    client := pb.NewSyncServiceClient(conn)
    
    stream, _ := client.GetChanges(ctx, &pb.GetChangesRequest{
        Table: "words",
        SinceTimestamp: lastSync,
    })
    
    // 3. Apply each change
    for {
        change, err := stream.Recv()
        if err == io.EOF { break }
        s.ApplyChanges(change)
    }
    
    // 4. Push local changes to peer
    localChanges := s.db.GetChanges("words", lastSync)
    s.pushChangesToPeer(client, localChanges)
    
    // 5. Update last sync timestamp
    s.setLastSyncTime(peerAddr, time.Now().UnixMilli())
}
```

**Manual testing**:
```bash
# Terminal 1: Start Desktop node
./enx-data-service --port 8091 --db ~/desktop/enx.db

# Terminal 2: Start MacBook node  
./enx-data-service --port 8092 --db ~/macbook/enx.db

# Terminal 3: Trigger sync
grpcurl -d '{"peer_addr":"localhost:8091"}' \
  localhost:8092 enx.data.SyncService/SyncWithPeer

# Verify: Check both databases have same records
sqlite3 ~/desktop/enx.db "SELECT COUNT(*) FROM words;"
sqlite3 ~/macbook/enx.db "SELECT COUNT(*) FROM words;"
```

**Success criteria**:
- ‚úÖ Can sync 2 nodes manually
- ‚úÖ Changes from Node A appear on Node B
- ‚úÖ Timestamp-based merge works correctly
- ‚úÖ No data loss in basic scenarios

**Time estimate**: 1.5-2 weeks

---

### Phase 3: Manual Sync Trigger (Week 6)

**What to build**: HTTP API and CLI for manual sync triggering.

**Scope**:
- ‚úÖ HTTP REST API for sync control
- ‚úÖ Simple CLI wrapper script
- ‚úÖ Sync status endpoint
- ‚ùå Skip: Auto-sync (defer to Phase 4+), Web UI

**Implementation**:

**1. HTTP API Server** (alongside gRPC server):
```go
// internal/api/http_server.go
type HTTPServer struct {
    coordinator *sync.Coordinator
    config      *Config
}

func (h *HTTPServer) Start(addr string) error {
    mux := http.NewServeMux()
    
    // Trigger sync with specific peer
    mux.HandleFunc("POST /api/sync/trigger", h.handleTriggerSync)
    
    // Trigger sync with all configured peers
    mux.HandleFunc("POST /api/sync/trigger-all", h.handleTriggerSyncAll)
    
    // Get sync status
    mux.HandleFunc("GET /api/sync/status", h.handleSyncStatus)
    
    log.Printf("HTTP API listening on %s", addr)
    return http.ListenAndServe(addr, mux)
}

func (h *HTTPServer) handleTriggerSync(w http.ResponseWriter, r *http.Request) {
    var req struct {
        Peer string `json:"peer"`
    }
    
    if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
        http.Error(w, "invalid request", http.StatusBadRequest)
        return
    }
    
    if req.Peer == "" {
        http.Error(w, "peer address required", http.StatusBadRequest)
        return
    }
    
    go func() {
        if err := h.coordinator.SyncWithPeer(context.Background(), req.Peer); err != nil {
            log.Printf("Sync failed: %v", err)
        }
    }()
    
    json.NewEncoder(w).Encode(map[string]string{
        "status": "triggered",
        "peer":   req.Peer,
    })
}

func (h *HTTPServer) handleSyncStatus(w http.ResponseWriter, r *http.Request) {
    status := h.coordinator.GetSyncStatus()
    json.NewEncoder(w).Encode(status)
}
```

**2. Configuration**:
```yaml
# config.yaml
node:
  id: "desktop-001"
  grpc_port: 50051
  http_port: 8090  # HTTP API port

peers:
  - addr: "192.168.1.10:50051"  # MacBook
    name: "macbook"
  - addr: "192.168.1.20:50051"  # Ubuntu laptop
    name: "ubuntu-laptop"
```

**3. CLI Wrapper Script** (`scripts/enx-sync`):
```bash
#!/bin/bash
# Simple CLI wrapper for sync operations

API_BASE="http://localhost:8090/api/sync"

case "$1" in
  trigger)
    if [ -z "$2" ]; then
      echo "Usage: enx-sync trigger <peer-address>"
      exit 1
    fi
    curl -s -X POST "$API_BASE/trigger" \
      -H "Content-Type: application/json" \
      -d "{\"peer\": \"$2\"}" | jq
    ;;
    
  trigger-all)
    curl -s -X POST "$API_BASE/trigger-all" | jq
    ;;
    
  status)
    curl -s "$API_BASE/status" | jq
    ;;
    
  *)
    echo "Usage: enx-sync {trigger|trigger-all|status}"
    echo ""
    echo "Commands:"
    echo "  trigger <peer>   Trigger sync with specific peer"
    echo "  trigger-all      Trigger sync with all configured peers"
    echo "  status           Show sync status"
    exit 1
    ;;
esac
```

**Usage Examples**:
```bash
# Ëß¶Âèë‰∏éÁâπÂÆö peer ÁöÑÂêåÊ≠•
enx-sync trigger 192.168.1.10:50051

# Êàñ‰ΩøÁî® curl Áõ¥Êé•Ë∞ÉÁî®
curl -X POST http://localhost:8090/api/sync/trigger \
  -H "Content-Type: application/json" \
  -d '{"peer": "192.168.1.10:50051"}'

# ‰∏éÊâÄÊúâ peer ÂêåÊ≠•
enx-sync trigger-all

# Êü•ÁúãÂêåÊ≠•Áä∂ÊÄÅ
enx-sync status

# ÈõÜÊàêÂà∞ cron Êàñ launchd
# crontab: */10 * * * * /usr/local/bin/enx-sync trigger-all
```

**Success criteria**:
- ‚úÖ Can trigger sync via HTTP API
- ‚úÖ CLI script works for common operations
- ‚úÖ Returns sync status (last sync time, success/failure)
- ‚úÖ Can be integrated into automation scripts

**Time estimate**: 2-3 days

---

### Phase 4: Production Readiness (Week 7-8)

**What to add**: Minimum production essentials.

**Critical additions**:

1. **Simple Authentication** (2 days)
```go
// Pre-shared key from environment variable
apiKey := os.Getenv("ENX_SYNC_API_KEY")

// Add to gRPC interceptor
if md.Get("x-api-key")[0] != apiKey {
    return status.Error(codes.Unauthenticated, "invalid key")
}
```

2. **Basic Logging** (1 day)
```go
log.Printf("Sync started with %s", peer)
log.Printf("Applied %d changes", count)
log.Printf("Sync completed in %v", duration)
```

3. **Error Handling** (2 days)
```go
// Retry with backoff
func (s *SyncService) SyncWithRetry(peer string, maxRetries int) error {
    backoff := time.Second
    for i := 0; i < maxRetries; i++ {
        err := s.SyncWithPeer(peer)
        if err == nil { return nil }
        
        time.Sleep(backoff)
        backoff *= 2
    }
    return fmt.Errorf("sync failed after %d retries", maxRetries)
}
```

4. **Sync State Persistence** (1 day)
```go
// Store last sync timestamp in SQLite
CREATE TABLE sync_state (
    peer_id TEXT PRIMARY KEY,
    last_sync_timestamp INTEGER NOT NULL
);
```

**Time estimate**: 1 week

---

### What to SKIP (Not Needed for MVP)

‚ùå **Authentication beyond pre-shared key**
- No JWT, no mTLS
- Reason: All nodes are your devices on home LAN
- Add later if: Expose to internet or multi-user

‚ùå **Conflict resolution UI**
- No interactive conflict resolution
- Reason: Single-user, timestamp wins
- Add later if: Users report data loss

‚ùå **Compression**
- No protobuf compression
- Reason: Word records are tiny (< 1KB each)
- Add later if: Syncing large datasets

‚ùå **Incremental sync optimization**
- No delta encoding
- Reason: Full record sync is fast enough (< 10KB per word)
- Add later if: Performance issues

‚ùå **Schema versioning**
- No migration framework
- Reason: Schema is stable, manual migration okay
- Add later if: Frequent schema changes

‚ùå **Monitoring dashboard**
- No metrics, no UI
- Reason: Logs are sufficient for debugging
- Add later if: Need production visibility

‚ùå **Multi-table sync**
- Only sync `words` initially
- Reason: Validate concept first
- Add later: Copy logic for `user_dicts`, etc.

---

### Simplified Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Desktop Linux (192.168.1.10)                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ  ‚îÇ  enx-api     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ data-service ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ  (existing)  ‚îÇ  gRPC   ‚îÇ  Port: 8091  ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ                                   ‚îÇ                          ‚îÇ
‚îÇ                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ                            ‚îÇ   enx.db     ‚îÇ                 ‚îÇ
‚îÇ                            ‚îÇ  (SQLite)    ‚îÇ                 ‚îÇ
‚îÇ                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚ñ≤
                                   ‚îÇ P2P Sync
                                   ‚îÇ (gRPC, 5min interval)
                                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  MacBook (192.168.1.20)                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ  ‚îÇ  enx-api     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ data-service ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ  (existing)  ‚îÇ  gRPC   ‚îÇ  Port: 8091  ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ                                   ‚îÇ                          ‚îÇ
‚îÇ                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ                            ‚îÇ   enx.db     ‚îÇ                 ‚îÇ
‚îÇ                            ‚îÇ  (SQLite)    ‚îÇ                 ‚îÇ
‚îÇ                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Points**:
- Each node runs independently
- Sync happens via direct gRPC calls
- No central coordination
- Simple timestamp-based merge

---

### Total Time Estimate

| Phase | Task | Time | Cumulative |
|-------|------|------|------------|
| 0 | Schema migration | 3 hours | 3 hours |
| 1 | Basic data service | 1.5 weeks | 2.5 weeks |
| 2 | Basic sync logic | 2 weeks | 4.5 weeks |
| 3 | Auto sync | 4 days | 5.5 weeks |
| 4 | Production readiness | 1 week | **6.5 weeks** |

**Realistic estimate with buffer**: **8 weeks** (accounting for debugging, testing, edge cases)

---

### Success Criteria for MVP

**Must have** (non-negotiable):
- ‚úÖ Desktop ‚Üî MacBook sync works reliably
- ‚úÖ Ubuntu laptop syncs when reconnecting to home LAN
- ‚úÖ No data loss in normal usage
- ‚úÖ Timestamp-based merge handles conflicts

**Nice to have** (can defer):
- ‚ö†Ô∏è Sync completes in < 5 seconds for 1000 words
- ‚ö†Ô∏è Works with flaky network
- ‚ö†Ô∏è Logs are useful for debugging

**Explicitly out of scope for MVP**:
- ‚ùå Real-time sync (5min interval is fine)
- ‚ùå Concurrent writes (single user)
- ‚ùå Complex conflict resolution
- ‚ùå Production monitoring

---

### Risk Mitigation

**Top risks and mitigation**:

1. **Clock skew ‚Üí wrong merge order**
   - Mitigation: Document NTP requirement, verify at startup
   - Fallback: Manual conflict resolution if reported

2. **Network issues during sync**
   - Mitigation: Retry with exponential backoff
   - Fallback: Next sync cycle will catch up

3. **SQLite corruption**
   - Mitigation: Enable WAL mode, backup before first sync
   - Fallback: Restore from backup, re-sync

4. **Development taking longer than expected**
   - Mitigation: Time-box each phase, cut scope if needed
   - Fallback: Use Syncthing temporarily while building

---

### Next Steps

**Week 1 Action Items**:
1. ‚úÖ Backup existing enx.db on all devices
2. ‚úÖ Migrate schema to UUID + update_datetime
3. ‚úÖ Verify migration with test queries
4. ‚úÖ Set up new Go project: `enx-data-service/`
5. ‚úÖ Define minimal `.proto` file

**Decision point after Week 3**:
- If basic CRUD works: Continue to Phase 2
- If hitting blockers: Reassess scope or switch to Syncthing

**Ready to start?** Let me know if you want me to help with:
- Detailed migration script for schema changes
- Minimal proto definitions
- Initial project structure setup

## Implementation Strategy (Phase 1)

### Prerequisites

**‚è∞ Clock Synchronization Strategy**:

#### Phase 1 (MVP) - NTP Configuration Only ‚≠ê **CURRENT FOCUS**

**Requirements**:
- ‚úÖ All nodes must enable NTP (Network Time Protocol)
- ‚úÖ Document NTP setup instructions for users
- ‚úÖ Rely on user to verify NTP is working

**Setup Commands**:
```bash
# Ubuntu/Debian
sudo timedatectl set-ntp true
timedatectl status  # Verify: "System clock synchronized: yes"

# macOS (usually enabled by default)
sudo systemsetup -setusingnetworktime on
systemsetup -getusingnetworktime  # Verify

# Verify all nodes have similar time (manual check)
date +%s  # Check Unix timestamp on each node
```

**Why This Is Enough for Phase 1**:
- ‚úÖ Single user, low conflict probability
- ‚úÖ Modern NTP accuracy: ¬±10-100ms (sufficient for word learning data)
- ‚úÖ Operations typically minutes/hours apart (not milliseconds)
- ‚úÖ Non-critical data (word learning progress)
- ‚úÖ Keep MVP simple - add complexity only when needed

**Risk if NTP not configured**: Timestamp-based merge may choose wrong version, causing data to be overwritten.

#### Phase 2 - Automatic Detection & Protection (Future)

**Deferred features** (implement only if Phase 1 shows problems):
- ‚è≥ Startup clock sync verification
- ‚è≥ Peer-to-peer clock skew detection
- ‚è≥ Automatic warnings when clocks diverge > 5 seconds
- ‚è≥ Sync rejection if clock skew too large

#### Phase 3 - Advanced Solutions (If Necessary)

**Only if real-world issues emerge**:
- ‚è≥ Hybrid Logical Clock (HLC) for clock-skew tolerance
- ‚è≥ Vector clocks for true causality tracking
- ‚è≥ Conflict detection UI for user resolution

**Decision**: Start with documented NTP requirement, add automation only when proven necessary through real-world usage.

### Development Scope

1.  **Scope**:
    *   Create `enx-data-service` in a new directory.
    *   Implement only the **"words"** table initially.
    *   Do NOT modify `enx-api` yet.
2.  **Clock Synchronization** (Phase 1):
    *   ‚úÖ Document NTP configuration requirement in README
    *   ‚úÖ Add NTP setup verification steps
    *   ‚ùå No automatic clock checking in Phase 1 (keep it simple)
    *   ‚è≥ Defer automatic detection to Phase 2
3.  **Integration**:
    *   Develop and test `enx-data-service` independently.
    *   Once `enx-data-service` is stable, refactor `enx-api` to connect to it.


## Architecture Goals

1. **Decoupling**: Separate business logic from data synchronization
   - **enx-api**: HTTP routing, authentication, business rules, **direct local database access**
   - **enx-data-service**: **P2P sync only** (not a data access layer)

2. **P2P Sync**: Enable data synchronization across multiple nodes without central server
   - Each node (Linux desktop, MacBook, Ubuntu laptop) runs its own enx-data-service
   - **Pull-based sync model**: Each node periodically pulls changes from peers (every 30s)
   - No central server required (works in isolated environments)
   - **data-service acts as a background sync daemon**, not a data access gateway

3. **Offline Support**: Continue working when disconnected, sync when online
   - enx-api works with local SQLite even without network
   - Changes are written to local database with updated `updated_at` timestamps
   - **data-service detects changes** and syncs when connection is restored
   - **Intermittent connectivity supported**: Nodes can go offline for hours/days, sync when reconnected
   - No data loss in offline scenarios
   - Timestamps ensure correct merge order

4. **Performance**: Direct database access for optimal performance
   - **enx-api accesses local SQLite directly** (no gRPC overhead)
   - **No remote calls for CRUD operations** (fast response times)
   - data-service only involved in background sync operations
   - Best suited for single-user, multi-device scenarios

5. **Flexibility**: Each service can be deployed, scaled, and upgraded independently
   - Update enx-api without touching data service
   - Upgrade database schema without restarting sync
   - Simple architecture: api ‚Üî local db, data-service ‚Üî P2P sync

## System Architecture

```plantuml
@startuml ENX System Architecture

!define ICONURL https://raw.githubusercontent.com/tupadr3/plantuml-icon-font-sprites/v2.4.0

skinparam rectangle {
    BackgroundColor<<client>> LightBlue
    BackgroundColor<<api>> LightGreen
    BackgroundColor<<data>> LightYellow
    BackgroundColor<<db>> Wheat
}

skinparam component {
    BackgroundColor<<browser>> AliceBlue
    BackgroundColor<<service>> LightGreen
    BackgroundColor<<database>> Wheat
}

package "Client Layer" as ClientLayer {
    component "Browser\n+ Extension" as Browser1 <<browser>>
    component "Browser\n+ Extension" as Browser2 <<browser>>
    component "Browser\n+ Extension" as Browser3 <<browser>>
}

package "Application Layer" as AppLayer {
    component "enx-api\nHost A\nPort: 8090" as API_A <<service>>
    component "enx-api\nHost B\nPort: 8090" as API_B <<service>>
    component "enx-api\nHost C\nPort: 8090" as API_C <<service>>
}

package "Data Layer" as DataLayer {

    package "Host A" as HostA <<data>> {
        component "Data Service\nPort: 8091\n(Sync Daemon)" as DS_A <<service>>
        database "enx.db\n(SQLite)" as DB_A <<database>>
        DS_A -down-> DB_A : "Monitor Changes\n& Sync"
        API_A -down-> DB_A : "Direct SQL\n(CRUD)"
    }

    package "Host B" as HostB <<data>> {
        component "Data Service\nPort: 8091\n(Sync Daemon)" as DS_B <<service>>
        database "enx.db\n(SQLite)" as DB_B <<database>>
        DS_B -down-> DB_B : "Monitor Changes\n& Sync"
        API_B -down-> DB_B : "Direct SQL\n(CRUD)"
    }

    package "Host C" as HostC <<data>> {
        component "Data Service\nPort: 8091\n(Sync Daemon)" as DS_C <<service>>
        database "enx.db\n(SQLite)" as DB_C <<database>>
        DS_C -down-> DB_C : "Monitor Changes\n& Sync"
        API_C -down-> DB_C : "Direct SQL\n(CRUD)"
    }
}

' Client to API connections
Browser1 -down-> API_A : HTTP/HTTPS
Browser2 -down-> API_B : HTTP/HTTPS
Browser3 -down-> API_C : HTTP/HTTPS

' P2P Sync connections between Data Services (Pull-based)
DS_A -right-> DS_B : "Pull Changes\n(every 30s)"
DS_B -right-> DS_C : "Pull Changes\n(every 30s)"
DS_A .down.> DS_C : "Pull Changes\n(every 30s)"

note right of DataLayer
  **Key Architecture Points:**
  ‚Ä¢ Each enx-api directly accesses its local enx.db (no gRPC layer)
  ‚Ä¢ Data Services run as background sync daemons
  ‚Ä¢ **Pull-based sync**: Each node periodically queries peers for changes
  ‚Ä¢ No central server, fully P2P architecture
  ‚Ä¢ API performance: Local SQLite access (microseconds)
  ‚Ä¢ Sync happens in background (30-second intervals)
end note

note bottom of API_A
  **API Responsibilities:**
  ‚Ä¢ HTTP routing & business logic
  ‚Ä¢ Direct SQLite CRUD operations
  ‚Ä¢ Authentication & authorization
  ‚Ä¢ No dependency on data-service for reads/writes
end note

note bottom of DS_A
  **Data Service Responsibilities:**
  ‚Ä¢ Monitor local database for changes (updated_at)
  ‚Ä¢ Pull changes from peer nodes (HTTP/gRPC)
  ‚Ä¢ Merge changes using timestamp-based conflict resolution
  ‚Ä¢ Runs as background daemon (no API dependencies)
end note
  ‚Ä¢ enx.db is embedded with Data Service (same process/host)
  ‚Ä¢ No central server required
end note

@enduml
```

## Communication Protocol Selection

### Chosen Solution: Hybrid Approach ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**Strategy**:
- **gRPC for inter-service communication** (enx-api ‚Üî data-service, high frequency)
- **gRPC for node sync** (data-service ‚Üî data-service, P2P sync)
- **REST for admin/monitoring** (health checks, metrics)

**Benefits**:
- Best performance for critical paths
- Easy debugging and monitoring
- Flexibility for different use cases

```
enx-api ‚Üí gRPC ‚Üí enx-data-service  (Fast, typed)
    ‚Üì
User/Admin ‚Üí REST ‚Üí enx-api       (Easy debugging)

data-service ‚Üí gRPC ‚Üí data-service  (Efficient sync)
```

## Communication Patterns

### 1. Request-Response (Synchronous)

**Use Case**: CRUD operations, immediate response needed

#### ‚ùå **ENX-Specific API (NOT USED - This service is generic)**

```go
// ‚ùå This approach was rejected
// Reason: GetWord() is ENX-specific, not reusable for other projects
word, err := dataClient.GetWord(ctx, &pb.GetWordRequest{
    English: "hello",
})

// Problems:
// - Hardcoded "GetWord" method only works with words table
// - Not reusable for blogs, task managers, or other apps
// - Tightly coupled to ENX domain
// - Cannot open-source as universal tool
```

#### ‚úÖ **Generic Data Service API (New Design - Recommended)**

**Scenario: Query a single word by English (ENX example)**

**Method 1: Structured API (Find) - ‚≠ê Recommended for simple queries**

```go
// Generic Find() API - works with any table
resp, err := dataClient.Find(ctx, &pb.FindRequest{
    Table: "words",                   // Table name (configurable)
    Filter: `{"english": "hello"}`,  // JSON filter (flexible)
    Limit: 1,                         // Only need one result
})

if err != nil {
    return nil, fmt.Errorf("failed to query record: %w", err)
}

if len(resp.Rows) == 0 {
    return nil, ErrRecordNotFound
}

// Parse result row
row := resp.Rows[0]
word := &Word{
    English: row.Cells[0].GetStringValue(),  // Column: english
    Chinese: row.Cells[1].GetStringValue(),  // Column: chinese
    // ... other fields
}
```

**Method 2: Raw SQL (Query) - For complex queries**

```go
// Using parameterized SQL query - works with any table/columns
resp, err := dataClient.Query(ctx, &pb.QueryRequest{
    Sql: "SELECT english, chinese, phonetic, definition FROM words WHERE english = ?",
    Params: []*pb.QueryParam{
        {Value: &pb.QueryParam_StringValue{StringValue: "hello"}},
    },
})

if err != nil {
    return nil, fmt.Errorf("failed to query records: %w", err)
}

// Parse result (same as Method 1)
// Note: Column names and types determined by your SQL query
```

**Method 3: Complex query with JOIN (user's learning progress)**

```go
// Query word with user's learning status
// ENX-specific: JOIN words table with user_dicts table
resp, err := dataClient.Query(ctx, &pb.QueryRequest{
    Sql: `
        SELECT w.english, w.chinese, w.phonetic, w.definition,
               ud.learned, ud.update_time
        FROM words w
        LEFT JOIN user_dicts ud ON w.english = ud.english AND ud.user_id = ?
        WHERE w.english = ?
    `,
    Params: []*pb.QueryParam{
        {Value: &pb.QueryParam_IntValue{IntValue: userID}},      // User ID
        {Value: &pb.QueryParam_StringValue{StringValue: "hello"}}, // Word
    },
})

// Result includes both word info and learning status
```

**Comparison:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Method          ‚îÇ Use Case              ‚îÇ Pros              ‚îÇ Cons           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Find() JSON     ‚îÇ Simple queries        ‚îÇ ‚úÖ Type-safe      ‚îÇ ‚ö†Ô∏è JSON parsing ‚îÇ
‚îÇ                 ‚îÇ Single table          ‚îÇ ‚úÖ SQL-injection  ‚îÇ                ‚îÇ
‚îÇ                 ‚îÇ Basic filters         ‚îÇ    safe           ‚îÇ                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Query() Simple  ‚îÇ Single table          ‚îÇ ‚úÖ Familiar SQL   ‚îÇ ‚ö†Ô∏è Need SQL    ‚îÇ
‚îÇ                 ‚îÇ Exact SQL control     ‚îÇ ‚úÖ Flexible       ‚îÇ    knowledge   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Query() JOIN    ‚îÇ Multi-table queries   ‚îÇ ‚úÖ Full SQL power ‚îÇ ‚ö†Ô∏è More complex‚îÇ
‚îÇ                 ‚îÇ Aggregations          ‚îÇ ‚úÖ Efficient      ‚îÇ                ‚îÇ
‚îÇ                 ‚îÇ Complex logic         ‚îÇ                   ‚îÇ                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Recommendation for ENX:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚Ä¢ Simple word lookup:        Use Find() (Method 1)
‚Ä¢ Word + learning status:    Use Query() with JOIN (Method 3)
‚Ä¢ Batch operations:          Use BatchExecute()
‚Ä¢ Search/filter words:       Use Find() with complex JSON filter
```

**Real-World Example: ENX API Handler Using Generic Service**

```go
// Example: enx-api/handlers/word.go
// This shows how ENX-specific business logic uses the generic data service

package handlers

import (
    "encoding/json"
    "net/http"

    "github.com/gin-gonic/gin"
    pb "enx-data-service/proto"  // Generic data service proto
)

type WordHandler struct {
    dataClient pb.GenericDataServiceClient  // Generic client
}

// GET /api/words/:english
func (h *WordHandler) GetWord(c *gin.Context) {
    english := c.Param("english")
    userID := c.GetInt64("user_id")  // From JWT token

    // Query word with user's learning progress (Method 3)
    resp, err := h.dataClient.Query(c.Request.Context(), &pb.QueryRequest{
        Sql: `
            SELECT
                w.english, w.chinese, w.phonetic, w.definition,
                w.update_datetime,
                COALESCE(ud.learned, 0) as learned,
                ud.update_time as user_update_time
            FROM words w
            LEFT JOIN user_dicts ud ON w.english = ud.english AND ud.user_id = ?
            WHERE w.english = ?
        `,
        Params: []*pb.QueryParam{
            {Value: &pb.QueryParam_IntValue{IntValue: userID}},
            {Value: &pb.QueryParam_StringValue{StringValue: english}},
        },
    })

    if err != nil {
        c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
        return
    }

    if len(resp.Rows) == 0 {
        c.JSON(http.StatusNotFound, gin.H{"error": "word not found"})
        return
    }

    // Parse result row into Word struct
    row := resp.Rows[0]
    word := map[string]interface{}{
        "english":      row.Cells[0].GetStringValue(),
        "chinese":      row.Cells[1].GetStringValue(),
        "phonetic":     row.Cells[2].GetStringValue(),
        "definition":   row.Cells[3].GetStringValue(),
        "update_datetime": row.Cells[4].GetStringValue(),
        "learned":      row.Cells[5].GetIntValue() == 1,
        "user_update_time": row.Cells[6].GetStringValue(),
    }

    c.JSON(http.StatusOK, word)
}

// POST /api/words/search (search multiple words)
func (h *WordHandler) SearchWords(c *gin.Context) {
    var req struct {
        Query  string `json:"query"`  // Search term
        Limit  int32  `json:"limit"`
        Offset int32  `json:"offset"`
    }

    if err := c.BindJSON(&req); err != nil {
        c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
        return
    }

    // Use Find() API for simple search (Method 1)
    filter := map[string]interface{}{
        "english": map[string]interface{}{
            "$like": req.Query + "%",  // Prefix search
        },
    }
    filterJSON, _ := json.Marshal(filter)

    resp, err := h.dataClient.Find(c.Request.Context(), &pb.FindRequest{
        Table:  "words",
        Filter: string(filterJSON),
        Sort:   `{"english": 1}`,  // Sort alphabetically
        Limit:  req.Limit,
        Offset: req.Offset,
    })

    if err != nil {
        c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
        return
    }

    // Convert rows to words array
    words := make([]map[string]interface{}, 0, len(resp.Rows))
    for _, row := range resp.Rows {
        words = append(words, map[string]interface{}{
            "english": row.Cells[0].GetStringValue(),
            "chinese": row.Cells[1].GetStringValue(),
            // ... other fields
        })
    }

    c.JSON(http.StatusOK, gin.H{
        "words": words,
        "total": len(words),
    })
}
```

**Key Takeaways:**

```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Generic Data Service Design Philosophy:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚ùå ENX-Specific Approach (NOT USED):
  dataClient.GetWord(ctx, &pb.GetWordRequest{English: "hello"})
  ‚Üì
  ‚Ä¢ Hardcoded GetWord() method
  ‚Ä¢ Only works with words table
  ‚Ä¢ Not reusable for other projects
  ‚Ä¢ Tightly coupled to ENX domain

‚úÖ Generic Approach (CHOSEN):
  dataClient.Find(ctx, &pb.FindRequest{
      Table: "words",                    // Configurable
      Filter: `{"english": "hello"}`,   // Flexible
  })
  ‚Üì
  ‚Ä¢ Generic Find() method
  ‚Ä¢ Works with ANY table (words, users, posts, products, etc.)
  ‚Ä¢ Configuration-driven (no code changes for new tables)
  ‚Ä¢ Reusable across any SQLite-based project
  ‚Ä¢ Open-source potential

Design Decision:
  ‚úÖ Build: Generic data service (universal SQLite sync tool)
  ‚úÖ Use: ENX as first real-world user case
  ‚úÖ Goal: Open source for broader community
  ‚úÖ Benefit: Helps thousands of developers with same problem

Trade-offs:
  ‚úÖ Gain: Flexibility, reusability, community support
  ‚ö†Ô∏è Cost: Runtime JSON parsing, manual column mapping
  ‚ö†Ô∏è Cost: Lose compile-time type safety for specific schemas
  ‚úÖ Benefit: Configuration > Code (easier maintenance)

Conclusion:
  ‚úÖ Worth it - Building universal tool serves broader purpose
  ‚úÖ ENX benefits from battle-tested generic service
  ‚úÖ Community benefits from open-source sync solution
```

**Characteristics**:
- Blocking call
- Timeout handling required
- Suitable for: Word lookup, search, CRUD operations

### 2. Streaming (Asynchronous)

**Use Case**: Large data transfer, real-time sync

```go
// Client streaming (enx-api ‚Üí data-service)
stream, err := client.BatchCreateWords(ctx)
for _, word := range words {
    stream.Send(word)
}
response, err := stream.CloseAndRecv()

// Server streaming (data-service ‚Üí enx-api)
stream, err := client.GetChanges(ctx, &pb.GetChangesRequest{
    Since: lastSyncTime,
})
for {
    change, err := stream.Recv()
    if err == io.EOF {
        break
    }
    applyChange(change)
}

// Bidirectional streaming (node-to-node sync)
stream, err := client.SyncNodes(ctx)
go func() {
    for {
        change := <-localChanges
        stream.Send(change)
    }
}()
for {
    change, err := stream.Recv()
    applyChange(change)
}
```

**Characteristics**:
- Non-blocking
- Efficient for large datasets
- Suitable for: Sync operations, batch operations

### 3. Event-Driven (Pub/Sub)

**Use Case**: Notify other nodes of changes (future enhancement)

```go
// Option: Add Redis Pub/Sub for change notifications
pubsub := redis.PubSub()
pubsub.Subscribe("enx:changes")

for msg := range pubsub.Channel() {
    // Node publishes: "word:123:updated"
    // Other nodes receive and pull changes
}
```

**Characteristics**:
- Decoupled
- Scalable
- Suitable for: Real-time notifications, event sourcing

## Protocol Comparison Matrix

| Feature              | gRPC         | REST/HTTP    | Hybrid       |
|---------------------|--------------|--------------|--------------|
| Performance         | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê     | ‚≠ê‚≠ê‚≠ê        | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê     |
| Type Safety         | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê     | ‚≠ê‚≠ê          | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê     |
| Ease of Debugging   | ‚≠ê‚≠ê‚≠ê        | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê     | ‚≠ê‚≠ê‚≠ê‚≠ê      |
| Learning Curve      | ‚≠ê‚≠ê‚≠ê        | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê     | ‚≠ê‚≠ê‚≠ê‚≠ê      |
| Streaming Support   | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê     | ‚≠ê‚≠ê          | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê     |
| Browser Support     | ‚≠ê‚≠ê (grpc-web)| ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê     | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê     |
| Payload Size        | Small        | Large        | Optimal      |
| Setup Complexity    | Medium       | Low          | Medium       |

## Service Interfaces

### enx-data-service API

**üö® Architecture Change**: data-service NO LONGER provides CRUD APIs. It's a background sync daemon only.

#### ~~Data API (gRPC) - Generic Interface~~ **REMOVED**

**‚ùå Previous Design (Rejected)**:
```protobuf
// This was removed - data-service no longer provides CRUD operations
service GenericDataService {
  rpc Find(...);    // ‚ùå Removed - enx-api uses GORM directly
  rpc Insert(...);  // ‚ùå Removed - enx-api uses GORM directly
  rpc Update(...);  // ‚ùå Removed - enx-api uses GORM directly
  rpc Delete(...);  // ‚ùå Removed - enx-api uses GORM directly
}
```

**Why removed:**
- Performance overhead: Every CRUD operation went through gRPC
- Unnecessary complexity: Local database doesn't need RPC
- Architecture decision: Sync should be separate from data access

**‚úÖ New Pattern**:
```go
// enx-api accesses database directly using GORM
db, _ := gorm.Open(sqlite.Open("/var/lib/enx-api/enx.db"))

// Example: Mark word as learned
db.Model(&UserDict{}).
   Where("user_id = ? AND word_id = ?", userId, wordId).
   Update("already_acquainted", 1)

// data-service runs in background, periodically syncs changes
// No interaction needed from enx-api for sync
```

#### Sync API (HTTP REST) - Pull-Based

**‚úÖ Current Design**: Simple HTTP REST API for pull-based synchronization

```http
# Sync endpoint (called by peers)
GET /sync/changes?since=<unix_timestamp_ms>

Response:
{
  "changes": [
    {
      "table": "words",
      "operation": "upsert",
      "record": {
        "id": "550e8400-e29b-41d4-a716-446655440000",
        "english": "algorithm",
        "updated_at": 1704326400000
      }
    },
    {
      "table": "user_dicts", 
      "operation": "upsert",
      "record": {
        "user_id": "user-123",
        "word_id": "550e8400-e29b-41d4-a716-446655440000",
        "query_count": 5,
        "updated_at": 1704326400000
      }
    }
  ],
  "last_update": 1704326400000
}

# Health check
GET /health

# Sync status (for debugging)
GET /sync/status
```

**Pull-Based Sync Flow**:
```
data-service runs background goroutine:

func syncLoop() {
    for {
        time.Sleep(30 * time.Second)  // Poll every 30 seconds
        
        for _, peer := range config.Peers {
            lastSync := getLastSyncTime(peer)
            
            // Pull changes from peer
            resp := http.Get(peer + "/sync/changes?since=" + lastSync)
            
            // Apply changes to local database
            for _, change := range resp.Changes {
                upsertRecord(change.Table, change.Record)
            }
            
            // Update sync timestamp
            setLastSyncTime(peer, resp.LastUpdate)
        }
    }
}
```

**No Push API**: Each node pulls from others, no need for push coordination

#### Admin API (REST)

```http
# Health and status
GET  /health
GET  /metrics
GET  /nodes
GET  /sync/status

# Manual operations
POST /sync/trigger
POST /sync/full-sync
GET  /sync/conflicts
```

## Data Flow Examples

**üö® Architecture Note**: The following examples reflect the NEW architecture where enx-api accesses 
the database directly, and data-service only handles background synchronization.

### Example 1: User Marks a Word (Direct Database Access)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Browser ‚îÇ         ‚îÇ enx-api ‚îÇ         ‚îÇ SQLite DB  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ                   ‚îÇ                     ‚îÇ
     ‚îÇ POST /mark        ‚îÇ                     ‚îÇ
     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ                     ‚îÇ
     ‚îÇ                   ‚îÇ UPDATE user_dicts   ‚îÇ
     ‚îÇ                   ‚îÇ (via GORM)          ‚îÇ
     ‚îÇ                   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ
     ‚îÇ                   ‚îÇ                     ‚îÇ
     ‚îÇ                   ‚îÇ UPDATE updated_at   ‚îÇ
     ‚îÇ                   ‚îÇ (for sync tracking) ‚îÇ
     ‚îÇ                   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ
     ‚îÇ                   ‚îÇ                     ‚îÇ
     ‚îÇ                   ‚îÇ Success             ‚îÇ
     ‚îÇ                   ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
     ‚îÇ 200 OK            ‚îÇ                     ‚îÇ
     ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                     ‚îÇ
     ‚îÇ                   ‚îÇ                     ‚îÇ

Background (data-service, independent process):
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ data-service ‚îÇ                    ‚îÇ peer node    ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                                   ‚îÇ
            ‚îÇ [30 seconds later]                ‚îÇ
            ‚îÇ                                   ‚îÇ
            ‚îÇ GET /sync/changes?since=...       ‚îÇ
            ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ
            ‚îÇ                                   ‚îÇ
            ‚îÇ Return changes (including mark)   ‚îÇ
            ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
            ‚îÇ                                   ‚îÇ
            ‚îÇ Apply to local DB                 ‚îÇ
            ‚îÇ                                   ‚îÇ
```

**Key Points**:
- ‚úÖ enx-api uses GORM directly (no gRPC overhead)
- ‚úÖ data-service syncs in background (no latency impact)
- ‚úÖ 30-second sync delay is acceptable for this use case

### Example 2: Automatic P2P Sync (Pull-Based)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ data-service A ‚îÇ         ‚îÇ data-service B ‚îÇ         ‚îÇ data-service C ‚îÇ
‚îÇ  (Desktop)     ‚îÇ         ‚îÇ   (MacBook)    ‚îÇ         ‚îÇ    (Ubuntu)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                          ‚îÇ                          ‚îÇ
        ‚îÇ [Every 30 seconds]       ‚îÇ                          ‚îÇ
        ‚îÇ                          ‚îÇ                          ‚îÇ
        ‚îÇ GET /sync/changes?       ‚îÇ                          ‚îÇ
        ‚îÇ     since=1704326370000  ‚îÇ                          ‚îÇ
        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ                          ‚îÇ
        ‚îÇ                          ‚îÇ                          ‚îÇ
        ‚îÇ {"changes": [...]}       ‚îÇ                          ‚îÇ
        ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                          ‚îÇ
        ‚îÇ                          ‚îÇ                          ‚îÇ
        ‚îÇ Apply to local DB        ‚îÇ                          ‚îÇ
        ‚îÇ                          ‚îÇ                          ‚îÇ
        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ                          ‚îÇ
        ‚îÇ                          ‚îÇ Apply changes            ‚îÇ
        ‚îÇ                          ‚îÇ                          ‚îÇ
        ‚îÇ                          ‚îÇ GetChanges               ‚îÇ
        ‚îÇ                          ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ
        ‚îÇ                          ‚îÇ                          ‚îÇ (offline)
        ‚îÇ                          ‚îÇ Connection timeout       ‚îÇ
        ‚îÇ                          ‚îÇ<‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚î§
        ‚îÇ                          ‚îÇ                          ‚îÇ
```

### Example 3: Offline Node Recovery

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ data-service C ‚îÇ         ‚îÇ data-service A ‚îÇ
‚îÇ   (offline)    ‚îÇ         ‚îÇ   (online)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                          ‚îÇ
        ‚îÇ [comes online]           ‚îÇ
        ‚îÇ                          ‚îÇ
        ‚îÇ RegisterNode             ‚îÇ
        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ
        ‚îÇ                          ‚îÇ
        ‚îÇ Node info + last_sync    ‚îÇ
        ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
        ‚îÇ                          ‚îÇ
        ‚îÇ GetSnapshot(full=true)   ‚îÇ
        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ
        ‚îÇ                          ‚îÇ
        ‚îÇ Stream all changes       ‚îÇ
        ‚îÇ since last_sync          ‚îÇ
        ‚îÇ<‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚î§
        ‚îÇ Apply changes            ‚îÇ
        ‚îÇ                          ‚îÇ
        ‚îÇ PushChanges              ‚îÇ
        ‚îÇ (local changes)          ‚îÇ
        ‚îú‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê>‚îÇ
        ‚îÇ                          ‚îÇ
        ‚îÇ Ack + new last_sync      ‚îÇ
        ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
        ‚îÇ                          ‚îÇ
        ‚îÇ ‚úÖ Fully synced          ‚îÇ
```

### Example 4: Intermittent Network Connection (Ubuntu Laptop Scenario)

**Real-world use case**: Ubuntu laptop works in isolated network for hours, then reconnects to home LAN.

```
Timeline: Saturday 9:00 AM - 3:00 PM
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

09:00 - Ubuntu Laptop (Before Disconnect)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     Home LAN      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Ubuntu Laptop   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Desktop Linux   ‚îÇ
‚îÇ enx.db: 1000    ‚îÇ    Connected     ‚îÇ enx.db: 1000    ‚îÇ
‚îÇ Last sync: 9:00 ‚îÇ                  ‚îÇ Last sync: 9:00 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚úÖ Both nodes synchronized at 1000 words

09:15 - Ubuntu Disconnects (Enter Isolated Network)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Ubuntu Laptop   ‚îÇ    ‚ï≥‚ï≥‚ï≥‚ï≥‚ï≥‚ï≥‚ï≥‚ï≥     ‚îÇ Desktop Linux   ‚îÇ
‚îÇ (OFFLINE)       ‚îÇ    Disconnected  ‚îÇ (online)        ‚îÇ
‚îÇ                 ‚îÇ                  ‚îÇ                 ‚îÇ
‚îÇ Working locally ‚îÇ                  ‚îÇ Working locally ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

09:30 - 13:00 (Both nodes work independently)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Ubuntu (offline):                Desktop (online):
  ‚îú‚îÄ 09:45: Add "algorithm" (1)     ‚îú‚îÄ 10:00: Add "database" (1)
  ‚îú‚îÄ 10:30: Add "network" (2)       ‚îú‚îÄ 10:30: Add "server" (2)
  ‚îú‚îÄ 11:00: Add "protocol" (3)      ‚îú‚îÄ 11:30: Mark 10 words learned
  ‚îú‚îÄ 11:45: Add "security" (4)      ‚îú‚îÄ 12:00: Add "cluster" (3)
  ‚îî‚îÄ 12:30: Add "encryption" (5)    ‚îî‚îÄ 13:00: Add "replica" (4)

Ubuntu: 1005 words (5 new)          Desktop: 1004 words (4 new)
All changes stored locally ‚úÖ        All changes stored locally ‚úÖ

15:00 - Ubuntu Reconnects to Home LAN
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Ubuntu Laptop   ‚îÇ                  ‚îÇ Desktop Linux   ‚îÇ
‚îÇ enx.db: 1005    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄConnected‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ enx.db: 1004    ‚îÇ
‚îÇ Last sync: 9:00 ‚îÇ    (Home LAN)    ‚îÇ Last sync: 9:00 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                                    ‚îÇ
         ‚îÇ 1. Detect network available        ‚îÇ
         ‚îÇ 2. Query: "Any peers online?"      ‚îÇ
         ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ
         ‚îÇ                                    ‚îÇ
         ‚îÇ 3. Response: "Desktop at 192.168.1.10:8091"
         ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
         ‚îÇ                                    ‚îÇ
         ‚îÇ 4. Request changes since 9:00 AM   ‚îÇ
         ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ
         ‚îÇ                                    ‚îÇ
         ‚îÇ 5. Desktop sends 4 words + metadata‚îÇ
         ‚îÇ    (database, server, cluster,     ‚îÇ
         ‚îÇ     replica + learning progress)   ‚îÇ
         ‚îÇ<‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚îÇ
         ‚îÇ                                    ‚îÇ
         ‚îÇ 6. Ubuntu applies changes          ‚îÇ
         ‚îÇ    Merge check: timestamps OK ‚úÖ   ‚îÇ
         ‚îÇ    Ubuntu now: 1009 words          ‚îÇ
         ‚îÇ                                    ‚îÇ
         ‚îÇ 7. Push local changes to Desktop   ‚îÇ
         ‚îÇ    (algorithm, network, protocol,  ‚îÇ
         ‚îÇ     security, encryption)          ‚îÇ
         ‚îÇ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê>‚îÇ
         ‚îÇ                                    ‚îÇ
         ‚îÇ                                    ‚îÇ 8. Desktop applies changes
         ‚îÇ                                    ‚îÇ    Desktop now: 1009 words
         ‚îÇ                                    ‚îÇ
         ‚îÇ 9. Sync complete acknowledgment    ‚îÇ
         ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
         ‚îÇ                                    ‚îÇ
         ‚îÇ ‚úÖ Ubuntu: 1009 words, synced      ‚îÇ
         ‚îÇ ‚úÖ Desktop: 1009 words, synced     ‚îÇ
         ‚îÇ Last sync: 15:00 (both nodes)      ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Result: Full synchronization achieved
  ‚Ä¢ Ubuntu got: 4 words from Desktop
  ‚Ä¢ Desktop got: 5 words from Ubuntu
  ‚Ä¢ Total: 1009 words on both nodes
  ‚Ä¢ Conflict resolution: Timestamp-based (all timestamps different)
  ‚Ä¢ Offline duration: 6 hours (no problem!)
```

**Key Features Demonstrated:**

```
1. Local Persistence ‚úÖ
   - Both nodes work independently offline
   - All changes saved to local SQLite
   - No data loss during offline period

2. Automatic Reconnection ‚úÖ
   - Ubuntu detects network availability
   - Automatically discovers peers on LAN
   - Initiates sync without user intervention

3. Bidirectional Sync ‚úÖ
   - Ubuntu pulls changes from Desktop
   - Ubuntu pushes changes to Desktop
   - Both nodes reach same state

4. Timestamp-based Merge ‚úÖ
   - Each change has update_datetime
   - Compare timestamps to resolve conflicts
   - No manual conflict resolution needed

5. Offline Duration Tolerance ‚úÖ
   - 6 hours offline: No problem
   - Could be days/weeks: Still works
   - Only limitation: Storage space for changes
```

**Sync Configuration for Intermittent Nodes:**

```yaml
# sync-config.yaml (Ubuntu laptop)
node:
  id: "ubuntu-laptop"
  name: "Ubuntu Work Laptop"
  
network:
  mode: "opportunistic"        # Sync when network available
  reconnect_interval: "30s"    # Check for network every 30 seconds
  sync_on_reconnect: true      # Auto-sync when reconnected
  
peers:
  - address: "192.168.1.10:8091"  # Desktop Linux (home LAN)
    name: "desktop"
    auto_discover: true            # Auto-find on LAN
  
  - address: "192.168.1.20:8091"  # MacBook (if online)
    name: "macbook"
    auto_discover: true
    
sync:
  interval: "5m"               # Sync every 5 min when connected
  retry_on_failure: true
  max_offline_changes: 10000   # Store up to 10k changes offline
  
  # Clock synchronization (Phase 1: Manual NTP configuration)
  # Note: Ensure NTP is enabled on all nodes before running sync
  # Automatic clock checking deferred to Phase 2
  
storage:
  path: "./enx.db"
  wal_enabled: true            # Enable WAL for concurrent access
```

**Implementation Notes:**

1. **Clock Sync** (Phase 1): 
   - ‚úÖ User manually configures NTP on all nodes
   - ‚úÖ README documents verification steps
   - ‚è≥ Automatic checking deferred to Phase 2
2. **Network Detection**: Service periodically checks network availability
3. **Peer Discovery**: mDNS or broadcast to find peers on LAN
4. **Change Tracking**: All operations record `update_datetime` using system clock
5. **Conflict Resolution**: Last-Write-Wins (LWW) strategy - latest timestamp always wins
6. **Efficient Transfer**: Only send changes since `last_sync_time`

### Concurrent Write Conflict Resolution

**Strategy: Last-Write-Wins (LWW)** ‚≠ê

**Scenario**: Two nodes modify the same record while disconnected

```
Node A (Desktop):
  10:00:00 AM - Update word "hello" ‚Üí translation="‰Ω†Â•Ω", timestamp: 10:00:00
  
Node B (MacBook, offline):
  10:00:05 AM - Update word "hello" ‚Üí translation="ÂìàÂñΩ", timestamp: 10:00:05
  
Sync happens at 10:30 AM:
  Compare timestamps: 10:00:05 > 10:00:00
  ‚Üí Node B's update wins ‚úÖ
  ‚Üí Node A's update is overwritten (lost) ‚ö†Ô∏è
```

**LWW Properties**:
- ‚úÖ **Simple**: Just compare timestamps, no complex merge logic
- ‚úÖ **Fast**: O(1) comparison, no computation overhead
- ‚úÖ **Predictable**: Always the latest write wins (based on system clock)
- ‚ö†Ô∏è **Data Loss Possible**: Earlier writes are discarded without warning
- ‚ö†Ô∏è **Requires Clock Sync**: Inaccurate if clocks are skewed

**When LWW Works Well**:
- ‚úÖ Single user accessing different devices (unlikely to edit same record simultaneously)
- ‚úÖ Infrequent concurrent edits (rare conflicts)
- ‚úÖ Simple data (text, numbers) where last version is acceptable

**When LWW Fails** (Not Your Scenario):
- ‚ùå Multiple users editing same record simultaneously (would need CRDT)
- ‚ùå Complex data structures needing field-level merge (e.g., nested JSON)
- ‚ùå High-value data where no loss is acceptable

**‚úÖ ENX Decision: LWW is Perfect**
- ‚úÖ Single user (developer only)
- ‚úÖ Development-phase sync only
- ‚úÖ Low conflict probability
- ‚úÖ Simple and maintainable
- ‚ùå **CRDT NOT needed** - too complex for this use case

### Soft Delete and Sync Strategy (Simplified Approach)

**üéØ Design Philosophy: Timestamp-first, handle complexity later**

#### Phase 1: Timestamp-Based Undelete (MVP) ‚≠ê **CHOSEN for Initial Version**

**Core Principle**: Delete and Update operations both use timestamp comparison - if an update happens after a delete, it effectively "undeletes" the record.

```
Scenario: Delete followed by Update
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Node A:
  10:00 AM - Delete record (id="abc-123") ‚Üí deleted_at: 10:00, update_datetime: 10:00
  
Node B (offline):
  10:05 AM - Update record (id="abc-123", content="new") ‚Üí update_datetime: 10:05
  
Sync happens at 10:10 AM:
  Node A receives update with timestamp 10:05
  10:05 > 10:00 ‚Üí Node B's update wins
  
Result: Record is "undeleted" with new content ‚úÖ
         (Equivalent to: delete, then recreate with same ID)
```

**Why This Works**:
1. ‚úÖ **Simple**: No special delete handling, just timestamp comparison
2. ‚úÖ **Intuitive**: Later operations override earlier ones (consistent with update behavior)
3. ‚úÖ **No extra complexity**: Reuses existing conflict resolution logic
4. ‚úÖ **Practical**: If user wants to update after deleting, they probably want to restore it

**Schema Requirements**:
```sql
CREATE TABLE example_table (
    id TEXT PRIMARY KEY,           -- UUID
    content TEXT,
    deleted_at INTEGER,             -- Unix timestamp (milliseconds), NULL if not deleted
    update_datetime INTEGER NOT NULL,  -- Unix timestamp (milliseconds)
    -- Other fields...
);

-- Soft delete: Set deleted_at and update_datetime
UPDATE example_table 
SET deleted_at = strftime('%s', 'now') * 1000,
    update_datetime = strftime('%s', 'now') * 1000
WHERE id = 'abc-123';

-- Update after delete: Sets deleted_at = NULL, updates content
UPDATE example_table 
SET content = 'new content',
    deleted_at = NULL,                    -- Clear deletion flag
    update_datetime = strftime('%s', 'now') * 1000
WHERE id = 'abc-123';
```

**Sync Logic**:
```go
// Merge logic applies to both updates and deletes
func mergeRecord(local, remote Record) Record {
    // Timestamp comparison (works for both delete and update)
    if remote.UpdateDateTime > local.UpdateDateTime {
        return remote  // Remote wins (could be delete or update)
    }
    return local      // Local wins
}

// No special handling needed - deleted_at is just another field
```

**Edge Cases Handled**:

| Scenario | Node A Action | Node B Action | Merge Result | Explanation |
|----------|---------------|---------------|--------------|-------------|
| **Delete ‚Üí Update** | 10:00 Delete | 10:05 Update | **Undeleted** (B wins) | Update timestamp > Delete timestamp |
| **Update ‚Üí Delete** | 10:00 Update | 10:05 Delete | **Deleted** (B wins) | Delete timestamp > Update timestamp |
| **Concurrent Delete** | 10:00 Delete | 10:00:05 Delete | **Deleted** (B wins) | Both deleted, timestamps close (5ms skew) |
| **Delete ‚Üí Delete** | 10:00 Delete | 10:05 Delete | **Deleted** (B wins) | Later delete wins (both result in deleted state) |

**What We're NOT Handling (Intentionally Deferred)**:

‚ùå **Concurrent Writes (Same Record, Same Timestamp)**
  - **Real-world scenario**: Two nodes modify the same word within 1 second
  - **Why not handling**: **User never uses multiple devices simultaneously**
    - Only one device active at a time (Desktop OR MacBook OR Ubuntu laptop)
    - Sync happens when switching devices, not during active use
    - Probability of sub-second conflict: < 0.01% (once per 10,000 operations)
  - **If it happens**: Simple last-write-wins based on timestamp comparison
  - **When to add**: If concurrent usage becomes common (multi-user scenario)

‚ùå **Tie-breaker for Identical Timestamps**
  - **Scenario**: Two nodes modify at exactly the same millisecond (extremely rare)
  - **Why not handling**: With NTP sync, sub-millisecond conflicts are virtually impossible
    - Clock skew detection prevents significant differences
    - Single-user usage pattern makes this theoretical
  - **Fallback**: Natural ordering based on node comparison (deterministic but arbitrary)
  - **When to add**: If clock skew issues are reported in production

‚ùå **Field-Level Conflict Detection**
  - **Scenario**: Node A updates field1, Node B updates field2 on same record
  - **Why not handling**: Adds significant complexity, unclear benefit for word learning app
    - Words are small records (english, chinese, pronunciation)
    - Partial updates don't make sense (if you update a word, update the whole thing)
  - **When to add**: If users request granular merge control (probably never)

‚ùå **Tombstone Expiration**
  - **Scenario**: Deleted records accumulate over years
  - **Why not handling**: Storage is cheap, premature optimization
    - 10,000 deleted words ‚âà 1MB (negligible)
    - No retention policy defined yet
  - **When to add**: If database size becomes a problem (> 100MB from tombstones)

‚ùå **Hard Delete**
  - **Scenario**: Permanently remove records (GDPR compliance)
  - **Why not handling**: Personal project, no legal requirements
    - Soft delete sufficient for "hide from UI" use case
  - **When to add**: If compliance requirements emerge

**Summary: Keep It Simple**

```
Current Design Philosophy:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚úÖ Handle Common Cases:
   ‚Ä¢ Sequential updates (different times) ‚Üí Simple timestamp comparison
   ‚Ä¢ Delete followed by update ‚Üí Timestamp-based undelete
   ‚Ä¢ Offline sync ‚Üí Change tracking with timestamps

‚è≥ Defer Rare Cases Until Proven Necessary:
   ‚Ä¢ Concurrent writes ‚Üí User doesn't use multiple devices simultaneously
   ‚Ä¢ Identical timestamps ‚Üí Virtually impossible with NTP + single-user
   ‚Ä¢ Field-level merge ‚Üí No clear use case for word learning app
   ‚Ä¢ Tombstone cleanup ‚Üí Storage not a concern yet

Philosophy: Add complexity only when real-world usage proves it's needed.
            Don't solve theoretical problems.
```

**Decision Rationale**:

1. **Single-user usage pattern**: User works on ONE device at a time, switches rarely (daily/weekly)
2. **NTP synchronization**: Clock skew < 5 seconds makes sub-second conflicts extremely rare
3. **Small data size**: Words are tiny records, storage overhead is negligible
4. **Development velocity**: Simple design allows faster iteration and fewer bugs

**When to revisit**:
- User reports unexpected data loss or merge issues
- Multi-user support required (family sharing, team collaboration)
- Storage becomes a bottleneck (> 100MB database)
- Clock skew frequently exceeds 5 seconds (NTP failures)
  - **When to add**: If users complain about unexpected undeletes

‚ùå **Multi-field Conflict Detection**: No field-level merge (e.g., merge content but keep delete flag)
  - **Why defer**: Adds significant complexity with unclear benefit
  - **When to add**: If users need fine-grained control (probably never for this project)

#### Phase 2: Tombstone Management (If Storage Becomes a Problem)

**Trigger**: Database grows too large from accumulated deleted records

**Approach**: Add tombstone expiration policy

```go
// Periodically clean up old tombstones
func cleanupTombstones(db *sql.DB, retentionDays int) {
    cutoff := time.Now().AddDate(0, 0, -retentionDays).UnixMilli()
    
    _, err := db.Exec(`
        DELETE FROM example_table 
        WHERE deleted_at IS NOT NULL 
        AND deleted_at < ?
    `, cutoff)
    
    // Log cleanup results...
}
```

**Decision**: Only implement if database size becomes a real problem

#### Phase 3: Advanced Conflict Strategies (If Users Complain)

**Trigger**: Users report unexpected behavior from delete/update conflicts

**Possible Approaches**:
1. **Last-Writer-Wins with Delete Priority**: Deletes always win over updates (regardless of timestamp)
2. **User Confirmation**: Prompt user when update conflicts with delete
3. **Field-Level Merge**: Merge content changes but preserve delete flag

**Decision**: Defer until specific use cases emerge from real-world usage

#### Recommended Decision Tree

```
Does storage grow too large from deleted records?
‚îú‚îÄ No ‚Üí Keep Phase 1 (timestamp-based) ‚úÖ
‚îÇ       95% of use cases
‚îÇ
‚îî‚îÄ Yes ‚Üí Implement tombstone cleanup (Phase 2) üßπ
   
Are users confused by undelete behavior?
‚îú‚îÄ No ‚Üí Keep Phase 1 ‚úÖ
‚îÇ
‚îî‚îÄ Yes ‚Üí Consider Phase 3 (advanced conflict rules) ü§î
         (only if proven necessary through user feedback)
```

**Summary**: Start with simple timestamp comparison, treat delete as just another update. Add complexity only when proven necessary. üöÄ

**Clock Sync Verification (Startup Check):**

```go
// Check clock synchronization at service startup
func verifyClockSync() error {
    // Method 1: Check NTP sync status (Linux)
    cmd := exec.Command("timedatectl", "status")
    output, err := cmd.Output()
    if err != nil {
        return fmt.Errorf("failed to check NTP status: %w", err)
    }
    
    if !strings.Contains(string(output), "NTP service: active") {
        return fmt.Errorf("‚ö†Ô∏è  NTP is not active. Please enable with: sudo timedatectl set-ntp true")
    }
    
    log.Println("‚úÖ NTP synchronization active")
    return nil
}

// Check clock skew between peers during sync
func (s *SyncService) checkClockSkew(peer Peer) error {
    // Get peer's current time
    peerTime, err := s.getPeerTime(peer)
    if err != nil {
        return err
    }
    
    localTime := time.Now()
    skew := localTime.Sub(peerTime).Abs()
    
    maxSkew := s.config.MaxClockSkew // e.g., 5 seconds
    
    if skew > maxSkew {
        log.Warnf("‚ö†Ô∏è  Clock skew with %s: %v (local: %v, peer: %v)", 
            peer.Name, skew, localTime, peerTime)
        
        if skew > 1*time.Minute {
            return fmt.Errorf("clock skew too large (%v), sync aborted. Please sync system clocks", skew)
        }
    }
    
    return nil
}

// Service initialization with clock check
func main() {
    log.Println("üöÄ Starting enx-data-service...")
    
    // STEP 1: Verify clock synchronization
    if err := verifyClockSync(); err != nil {
        log.Fatalf("‚ùå Clock sync check failed: %v", err)
        log.Fatal("   Please enable NTP: sudo timedatectl set-ntp true")
    }
    
    // STEP 2: Initialize database
    db := initDatabase("./enx.db")
    
    // STEP 3: Start sync service
    syncService := NewSyncService(db, loadConfig())
    
    // ... rest of initialization
}
```

## Error Handling

### Connection Errors

```go
// Retry with exponential backoff
func (c *DataClient) GetWordWithRetry(word string) (*Word, error) {
    backoff := time.Second
    maxRetries := 3

    for i := 0; i < maxRetries; i++ {
        word, err := c.GetWord(word)
        if err == nil {
            return word, nil
        }

        if isNetworkError(err) {
            time.Sleep(backoff)
            backoff *= 2
            continue
        }

        return nil, err
    }

    return nil, ErrMaxRetriesExceeded
}
```

### Timeout Handling

```go
// Set appropriate timeouts
ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
defer cancel()

word, err := client.GetWord(ctx, req)
if err == context.DeadlineExceeded {
    // Handle timeout
    log.Warn("Request timeout, using cached data")
    return getCachedWord(req.English)
}
```

### Circuit Breaker Pattern

```go
// Prevent cascading failures
type CircuitBreaker struct {
    maxFailures int
    timeout     time.Duration
    failures    int
    lastFail    time.Time
    state       State  // Closed, Open, HalfOpen
}

func (cb *CircuitBreaker) Call(fn func() error) error {
    if cb.state == Open {
        if time.Since(cb.lastFail) > cb.timeout {
            cb.state = HalfOpen
        } else {
            return ErrCircuitOpen
        }
    }

    err := fn()
    if err != nil {
        cb.failures++
        cb.lastFail = time.Now()
        if cb.failures >= cb.maxFailures {
            cb.state = Open
        }
        return err
    }

    cb.failures = 0
    cb.state = Closed
    return nil
}
```

### Network Detection and Auto-Sync (for Intermittent Connectivity)

**Implementation for Ubuntu laptop scenario: Detect when network becomes available and trigger sync**

```go
package sync

import (
    "context"
    "log"
    "net"
    "time"
)

// NetworkMonitor detects network availability changes
type NetworkMonitor struct {
    isOnline     bool
    lastCheck    time.Time
    checkInterval time.Duration
    syncService  *SyncService
}

// NewNetworkMonitor creates a network monitor for intermittent connectivity
func NewNetworkMonitor(syncService *SyncService, checkInterval time.Duration) *NetworkMonitor {
    return &NetworkMonitor{
        isOnline:      false,
        checkInterval: checkInterval,
        syncService:   syncService,
    }
}

// Start begins monitoring network availability
func (nm *NetworkMonitor) Start(ctx context.Context) {
    log.Println("üîç Starting network monitor for opportunistic sync...")
    
    ticker := time.NewTicker(nm.checkInterval)
    defer ticker.Stop()

    for {
        select {
        case <-ctx.Done():
            log.Println("Network monitor stopped")
            return
            
        case <-ticker.C:
            nm.checkAndSync()
        }
    }
}

// checkAndSync checks network and triggers sync if newly online
func (nm *NetworkMonitor) checkAndSync() {
    online := nm.isNetworkAvailable()
    
    // Detect state transition: offline ‚Üí online
    if online && !nm.isOnline {
        log.Println("üåê Network detected! Starting opportunistic sync...")
        nm.onNetworkReconnect()
    } else if !online && nm.isOnline {
        log.Println("üì° Network lost. Working offline...")
    }
    
    nm.isOnline = online
    nm.lastCheck = time.Now()
}

// isNetworkAvailable checks if network is reachable
func (nm *NetworkMonitor) isNetworkAvailable() bool {
    // Method 1: Try to resolve a known host
    _, err := net.LookupHost("google.com")
    if err == nil {
        return true
    }
    
    // Method 2: Check if we can reach peers on LAN
    peers := nm.syncService.GetConfiguredPeers()
    for _, peer := range peers {
        if nm.canReachPeer(peer.Address) {
            return true
        }
    }
    
    return false
}

// canReachPeer checks if a peer is reachable on LAN
func (nm *NetworkMonitor) canReachPeer(address string) bool {
    conn, err := net.DialTimeout("tcp", address, 2*time.Second)
    if err != nil {
        return false
    }
    conn.Close()
    return true
}

// onNetworkReconnect handles network reconnection event
func (nm *NetworkMonitor) onNetworkReconnect() {
    ctx := context.Background()
    
    log.Println("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    log.Println("üîÑ Network Reconnected - Starting Sync")
    log.Println("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    
    // Discover peers on LAN
    peers, err := nm.syncService.DiscoverPeers(ctx)
    if err != nil {
        log.Printf("‚ö†Ô∏è  Peer discovery failed: %v", err)
        return
    }
    
    if len(peers) == 0 {
        log.Println("‚ÑπÔ∏è  No peers found on network")
        return
    }
    
    log.Printf("‚úÖ Found %d peer(s): %v", len(peers), peers)
    
    // Sync with each discovered peer
    for _, peer := range peers {
        log.Printf("üîÑ Syncing with %s...", peer.Name)
        
        if err := nm.syncService.SyncWithPeer(ctx, peer); err != nil {
            log.Printf("‚ùå Sync with %s failed: %v", peer.Name, err)
            continue
        }
        
        log.Printf("‚úÖ Sync with %s completed", peer.Name)
    }
    
    log.Println("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    log.Println("‚úÖ Opportunistic sync completed")
    log.Println("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
}

// SyncService methods for peer discovery and sync
func (s *SyncService) DiscoverPeers(ctx context.Context) ([]Peer, error) {
    var peers []Peer
    
    // Try configured peers first
    for _, peerAddr := range s.config.Peers {
        if s.isPeerReachable(peerAddr) {
            peer := Peer{
                Address: peerAddr,
                Name:    s.getPeerName(peerAddr),
            }
            peers = append(peers, peer)
        }
    }
    
    // Optional: mDNS discovery for auto-discovery on LAN
    if s.config.AutoDiscover {
        discovered := s.discoverViaMDNS(ctx)
        peers = append(peers, discovered...)
    }
    
    return peers, nil
}

func (s *SyncService) SyncWithPeer(ctx context.Context, peer Peer) error {
    // 1. Get changes from peer since last sync
    lastSyncTime := s.getLastSyncTime(peer.Address)
    
    remoteChanges, err := s.fetchChangesFromPeer(ctx, peer, lastSyncTime)
    if err != nil {
        return fmt.Errorf("failed to fetch changes: %w", err)
    }
    
    log.Printf("üì• Received %d changes from %s", len(remoteChanges), peer.Name)
    
    // 2. Apply remote changes locally
    if err := s.applyChanges(ctx, remoteChanges); err != nil {
        return fmt.Errorf("failed to apply changes: %w", err)
    }
    
    // 3. Push local changes to peer
    localChanges := s.getLocalChanges(lastSyncTime)
    
    log.Printf("üì§ Sending %d changes to %s", len(localChanges), peer.Name)
    
    if err := s.pushChangesToPeer(ctx, peer, localChanges); err != nil {
        return fmt.Errorf("failed to push changes: %w", err)
    }
    
    // 4. Update last sync time
    s.updateLastSyncTime(peer.Address, time.Now())
    
    return nil
}

// Main service initialization
func main() {
    // Initialize data service
    db := initDatabase("./enx.db")
    syncService := NewSyncService(db, loadConfig())
    
    // Start network monitor for opportunistic sync
    networkMonitor := NewNetworkMonitor(syncService, 30*time.Second)
    
    ctx, cancel := context.WithCancel(context.Background())
    defer cancel()
    
    // Start monitoring in background
    go networkMonitor.Start(ctx)
    
    // Also run periodic sync when online (every 5 minutes)
    go func() {
        ticker := time.NewTicker(5 * time.Minute)
        defer ticker.Stop()
        
        for range ticker.C {
            if networkMonitor.isOnline {
                log.Println("‚è∞ Periodic sync triggered")
                networkMonitor.onNetworkReconnect()
            }
        }
    }()
    
    // Start gRPC server
    startGRPCServer(syncService)
}
```

**Configuration for Intermittent Connectivity:**

```yaml
# config.yaml (Ubuntu laptop)
network:
  # Check network every 30 seconds
  monitor_interval: "30s"
  
  # Auto-sync when network detected
  sync_on_reconnect: true
  
  # Also do periodic sync when online
  periodic_sync_interval: "5m"
  
peers:
  # Home LAN peers (auto-discover)
  - address: "192.168.1.10:8091"
    name: "desktop"
    
  - address: "192.168.1.20:8091"
    name: "macbook"
    
auto_discover:
  enabled: true              # Use mDNS to find peers on LAN
  service_name: "_enx-sync._tcp"
  
sync:
  # Store changes offline
  offline_buffer_size: 10000
  
  # Retry failed syncs
  retry_on_failure: true
  max_retries: 3
  retry_backoff: "1m"
```

**Log Output Example (Ubuntu reconnecting):**

```
2025-12-13 15:00:01 üîç Starting network monitor for opportunistic sync...
2025-12-13 15:00:31 üì° Network lost. Working offline...
2025-12-13 15:05:01 üì° Working offline... (check interval)
2025-12-13 15:10:01 üì° Working offline... (check interval)
2025-12-13 15:15:01 üåê Network detected! Starting opportunistic sync...
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üîÑ Network Reconnected - Starting Sync
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚úÖ Found 2 peer(s): [desktop macbook]
üîÑ Syncing with desktop...
üì• Received 4 changes from desktop
üì§ Sending 5 changes to desktop
‚úÖ Sync with desktop completed
üîÑ Syncing with macbook...
üì• Received 2 changes from macbook
üì§ Sending 5 changes to macbook
‚úÖ Sync with macbook completed
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚úÖ Opportunistic sync completed
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
```

**Key Benefits:**

1. ‚úÖ **Automatic**: No manual intervention required
2. ‚úÖ **Resilient**: Works offline, syncs when possible
3. ‚úÖ **Efficient**: Only checks network periodically (low CPU)
4. ‚úÖ **Robust**: Handles peer discovery and connection failures
5. ‚úÖ **Flexible**: Configurable check interval and retry logic

## Performance Considerations

### Connection Pooling

```go
// Reuse gRPC connections
var (
    connPool = make(map[string]*grpc.ClientConn)
    poolMux  sync.RWMutex
)

func GetConnection(addr string) (*grpc.ClientConn, error) {
    poolMux.RLock()
    if conn, exists := connPool[addr]; exists {
        poolMux.RUnlock()
        return conn, nil
    }
    poolMux.RUnlock()

    poolMux.Lock()
    defer poolMux.Unlock()

    conn, err := grpc.Dial(addr, grpc.WithInsecure())
    if err != nil {
        return nil, err
    }

    connPool[addr] = conn
    return conn, nil
}
```

### Request Batching

```go
// Batch multiple operations
type BatchRequest struct {
    Operations []Operation
}

func (c *DataClient) BatchExecute(ops []Operation) error {
    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
    defer cancel()

    stream, err := c.client.BatchExecute(ctx)
    if err != nil {
        return err
    }

    for _, op := range ops {
        if err := stream.Send(op); err != nil {
            return err
        }
    }

    resp, err := stream.CloseAndRecv()
    return err
}
```

### Caching Strategy

```go
// Cache frequently accessed data (generic approach)
type CachedDataClient struct {
    client GenericDataServiceClient
    cache  *lru.Cache
    ttl    time.Duration
}

func (c *CachedDataClient) FindWithCache(table, filter string) (*pb.FindResponse, error) {
    cacheKey := fmt.Sprintf("%s:%s", table, filter)
    
    // Check cache first
    if val, ok := c.cache.Get(cacheKey); ok {
        if entry := val.(*CacheEntry); time.Since(entry.Time) < c.ttl {
            return entry.Response, nil
        }
    }

    // Cache miss, fetch from service
    result, err := c.client.Find(context.Background(), &pb.FindRequest{
        Table:  table,
        Filter: filter,
    })
    if err != nil {
        return nil, err
    }

    // Update cache
    c.cache.Add(cacheKey, &CacheEntry{
        Response: result,
        Time:     time.Now(),
    })

    return result, nil
}

// Example: ENX-specific wrapper for word lookup
func (c *CachedDataClient) GetWord(word string) (*Word, error) {
    resp, err := c.FindWithCache("words", fmt.Sprintf(`{"english": "%s"}`, word))
    if err != nil {
        return nil, err
    }
    // Parse response to Word struct (ENX-specific business logic)
    return parseWord(resp.Rows[0]), nil
}
```

## Clock Synchronization Strategy

### Why Clock Sync Matters

Timestamp-based conflict resolution requires consistent time across all nodes:

```
‚ùå Without Clock Sync:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Desktop (clock fast +10 min):
  10:10 AM - Update word "hello" ‚Üí timestamp: 10:10
  
Ubuntu (clock correct):
  10:05 AM - Update word "hello" ‚Üí timestamp: 10:05
  
Sync happens:
  Desktop's 10:10 > Ubuntu's 10:05 ‚Üí Desktop wins ‚úÖ
  BUT Desktop's change actually happened BEFORE Ubuntu's! ‚ùå
  
Result: Wrong merge order, data loss


‚úÖ With Clock Sync (NTP enabled):
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Desktop (NTP synced):
  10:05:30 AM - Update word "hello" ‚Üí timestamp: 10:05:30
  
Ubuntu (NTP synced):
  10:06:15 AM - Update word "hello" ‚Üí timestamp: 10:06:15
  
Sync happens:
  Ubuntu's 10:06:15 > Desktop's 10:05:30 ‚Üí Ubuntu wins ‚úÖ
  
Result: Correct merge order, no data loss
```

### Implementation Strategy (Phased Approach)

**üéØ Design Philosophy: Start simple, add complexity only when needed**

#### Phase 1: System Clock (MVP) ‚≠ê **CHOSEN for Initial Version**

**Approach**: Require nodes to sync system clocks before using P2P sync

**Prerequisites**:
```bash
# Enable NTP on all nodes (one-time setup)

# Linux (Ubuntu/Debian)
sudo timedatectl set-ntp true
timedatectl status  # Verify: "NTP service: active"

# macOS (usually auto-enabled)
sudo systemsetup -setusingnetworktime on
systemsetup -getusingnetworktime

# Verify time sync
date  # Check if times match across nodes
```

**Pros**:
- ‚úÖ **Simple**: No extra code, uses OS features
- ‚úÖ **Reliable**: NTP is battle-tested (accuracy: ~10-50ms)
- ‚úÖ **No overhead**: Zero performance cost
- ‚úÖ **Standard practice**: Most servers already use NTP

**Cons**:
- ‚ö†Ô∏è **Requires NTP**: Needs internet connection for NTP servers
- ‚ö†Ô∏è **Manual setup**: Users must enable NTP on all nodes
- ‚ö†Ô∏è **Trust OS**: Assumes OS time management works correctly

**When this works**:
- ‚úÖ Development environment (can ensure NTP is enabled)
- ‚úÖ Home LAN with internet access
- ‚úÖ Single user controlling all nodes
- ‚úÖ Infrequent clock adjustments

**When this fails**:
- ‚ùå VMs with time drift (paused/resumed VMs)
- ‚ùå Containers with isolated clocks
- ‚ùå Networks without NTP access
- ‚ùå Frequently hibernated laptops

**Implementation**: Add startup check

```go
func main() {
    // Fail fast if NTP not enabled
    if err := verifyNTPEnabled(); err != nil {
        log.Fatal("Clock sync required. Enable NTP: sudo timedatectl set-ntp true")
    }
    
    // Continue with normal startup
    startService()
}
```

#### Phase 2: Hybrid Logical Clock (HLC) - If Clock Skew Becomes a Problem

**Trigger**: If users report incorrect merge order due to clock skew

**Approach**: Implement HLC (combines physical time + logical counter)

```go
type HLC struct {
    PhysicalTime int64  // Milliseconds since epoch
    Logical      int64  // Counter for same physical time
}

// Update HLC when receiving remote event
func (h *HLC) Update(remoteHLC HLC) {
    h.PhysicalTime = max(h.PhysicalTime, remoteHLC.PhysicalTime, time.Now().UnixMilli())
    if h.PhysicalTime == remoteHLC.PhysicalTime {
        h.Logical = max(h.Logical, remoteHLC.Logical) + 1
    } else {
        h.Logical = 0
    }
}
```

**Pros**:
- ‚úÖ Tolerates clock skew up to ~10 minutes
- ‚úÖ Maintains causality (happened-before relationships)
- ‚úÖ Gradually corrects clock differences

**Cons**:
- ‚ö†Ô∏è More complex implementation
- ‚ö†Ô∏è Need to store HLC in every record (extra storage)
- ‚ö†Ô∏è Requires schema migration

**Decision point**: Only implement if Phase 1 proves insufficient in real-world usage

#### Phase 3: Vector Clocks - Only if Causality Tracking Required

**Trigger**: If HLC still insufficient (very unlikely for this project)

**Approach**: Full vector clock per node

```go
type VectorClock map[string]int64  // node_id -> counter

func (vc VectorClock) Merge(other VectorClock) {
    for node, count := range other {
        vc[node] = max(vc[node], count)
    }
}
```

**Pros**:
- ‚úÖ Perfect causality tracking
- ‚úÖ Works with arbitrary clock differences

**Cons**:
- ‚ùå Much more complex
- ‚ùå O(N) storage per record (N = number of nodes)
- ‚ùå Difficult conflict detection

**Decision**: Defer until absolutely necessary (likely never for 3-node setup)

### Recommended Decision Tree

```
Is NTP available on all nodes?
‚îú‚îÄ Yes ‚Üí Use Phase 1 (System Clock) ‚úÖ
‚îÇ        95% of use cases
‚îÇ
‚îî‚îÄ No ‚Üí Can you enable NTP?
   ‚îú‚îÄ Yes ‚Üí Enable NTP, use Phase 1 ‚úÖ
   ‚îÇ
   ‚îî‚îÄ No ‚Üí Is clock skew causing problems?
      ‚îú‚îÄ No ‚Üí Keep using Phase 1, document requirement üìù
      ‚îÇ
      ‚îî‚îÄ Yes ‚Üí Implement Phase 2 (HLC) üîß
                (only if proven necessary through bug reports)
```

### Deployment Checklist

**Before enabling P2P sync, verify:**

```bash
# 1. Check NTP status on each node
timedatectl status

# Expected output:
#   System clock synchronized: yes
#   NTP service: active

# 2. Compare times across nodes (should differ by < 1 second)
# Run on each node:
date +"%Y-%m-%d %H:%M:%S.%3N"

# Example output:
# Desktop: 2025-12-13 15:30:45.123
# MacBook: 2025-12-13 15:30:45.234  (diff: 111ms ‚úÖ)
# Ubuntu:  2025-12-13 15:30:45.089  (diff: 34ms ‚úÖ)

# 3. If times differ by > 5 seconds, force sync:
sudo systemctl restart systemd-timesyncd  # Linux
sudo sntp -sS time.apple.com             # macOS
```

### Future Considerations

**When to upgrade from Phase 1**:
- ‚ö†Ô∏è Users report data loss from incorrect merge order
- ‚ö†Ô∏è Clock skew > 5 seconds frequently occurs
- ‚ö†Ô∏è Nodes often run in VMs that pause/resume
- ‚ö†Ô∏è Network doesn't have NTP access

**Until then**: Keep it simple, use system clock with NTP ‚úÖ

## Security Considerations

### Peer-to-Peer Sync Authentication

**‚ö†Ô∏è IMPORTANT: This section discusses authentication BETWEEN sync nodes (peer-to-peer), not end-user authentication**

**Context**: When Node A (Desktop) connects to Node B (MacBook) for P2P sync, we need to ensure:
- ‚úÖ Only trusted nodes can connect (not any random device on LAN)
- ‚úÖ Prevent unauthorized access to database sync API
- ‚úÖ Ensure data is not exposed to untrusted peers

**End-user authentication** (user login to enx-api) is a separate concern handled by the application layer.

### Recommended Approach: Pre-Shared Key (Environment Variable) ‚≠ê

**For initial implementation, we use a simple pre-shared key approach:**

**Why this approach:**
- ‚úÖ **Simple**: No complex JWT logic, no certificate management
- ‚úÖ **Sufficient**: All nodes are trusted (your own devices)
- ‚úÖ **Low maintenance**: Single shared secret across all nodes
- ‚úÖ **Easy deployment**: Configure once via environment variable
- ‚ö†Ô∏è **Security trade-off**: Key must be kept secret, suitable for home LAN

**When to upgrade**: If you need multi-tenant support or expose sync service to untrusted networks, consider JWT or mTLS.

#### Implementation: Simple API Key Authentication

**1. Server Side (Validates incoming requests)**

```go
// ==================== Server Side ====================

package main

import (
    "context"
    "os"
    "google.golang.org/grpc"
    "google.golang.org/grpc/codes"
    "google.golang.org/grpc/metadata"
    "google.golang.org/grpc/status"
)

// Simple API Key Interceptor
type APIKeyInterceptor struct {
    validAPIKey string  // Pre-shared key from environment variable
}

// Unary interceptor for API key validation
func (a *APIKeyInterceptor) Unary() grpc.UnaryServerInterceptor {
    return func(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo,
                handler grpc.UnaryHandler) (interface{}, error) {

        // Skip authentication for health check
        if info.FullMethod == "/enx.data.DataService/HealthCheck" {
            return handler(ctx, req)
        }

        // Extract metadata
        md, ok := metadata.FromIncomingContext(ctx)
        if !ok {
            return nil, status.Error(codes.Unauthenticated, "missing metadata")
        }

        // Get API key from metadata
        keys := md.Get("x-api-key")
        if len(keys) == 0 {
            return nil, status.Error(codes.Unauthenticated, "missing API key")
        }

        // Validate API key
        if keys[0] != a.validAPIKey {
            return nil, status.Error(codes.Unauthenticated, "invalid API key")
        }

        // Authentication successful
        return handler(ctx, req)
    }
}

// Create gRPC server with API key authentication
func NewAuthenticatedServer() *grpc.Server {
    // Load API key from environment variable
    apiKey := os.Getenv("ENX_SYNC_API_KEY")
    if apiKey == "" {
        log.Fatal("‚ùå ENX_SYNC_API_KEY environment variable not set")
    }
    
    if len(apiKey) < 32 {
        log.Fatal("‚ùå ENX_SYNC_API_KEY must be at least 32 characters")
    }

    authInterceptor := &APIKeyInterceptor{
        validAPIKey: apiKey,
    }

    server := grpc.NewServer(
        grpc.UnaryInterceptor(authInterceptor.Unary()),
    )

    log.Printf("‚úÖ API key authentication enabled (key length: %d)", len(apiKey))
    return server
}
```

**2. Client Side (Adds API key to requests)**

```go
// ==================== Client Side ====================

package main

import (
    "context"
    "os"
    "google.golang.org/grpc"
    "google.golang.org/grpc/metadata"
)

// Client interceptor to add API key to requests
func apiKeyInterceptor(apiKey string) grpc.UnaryClientInterceptor {
    return func(ctx context.Context, method string, req, reply interface{},
                cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption) error {

        // Add API key to metadata
        ctx = metadata.AppendToOutgoingContext(ctx, "x-api-key", apiKey)

        return invoker(ctx, method, req, reply, cc, opts...)
    }
}

// Create authenticated client
func NewAuthClient(addr string) (*grpc.ClientConn, error) {
    // Load API key from environment variable
    apiKey := os.Getenv("ENX_SYNC_API_KEY")
    if apiKey == "" {
        return nil, fmt.Errorf("ENX_SYNC_API_KEY environment variable not set")
    }

    conn, err := grpc.Dial(addr,
        grpc.WithInsecure(),
        grpc.WithUnaryInterceptor(apiKeyInterceptor(apiKey)),
    )
    if err != nil {
        return nil, err
    }

    log.Printf("‚úÖ Connected to %s with API key authentication", addr)
    return conn, nil
}
```

**3. Configuration (Environment Variable Setup)**

```bash
# Generate a secure random API key (do this ONCE)
openssl rand -hex 32
# Output example: a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0u1v2w3x4y5z6a7b8c9d0e1f2

# Set environment variable on ALL nodes (use the SAME key)
export ENX_SYNC_API_KEY="a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0u1v2w3x4y5z6a7b8c9d0e1f2"

# Add to shell profile to persist across reboots
echo 'export ENX_SYNC_API_KEY="your-key-here"' >> ~/.bashrc
source ~/.bashrc

# Verify on each node
echo $ENX_SYNC_API_KEY  # Should output your key
```

**4. Deployment Configuration**

```yaml
# config.yaml (per node)
node:
  id: "desktop-linux-001"
  name: "Desktop Linux"
  listen_addr: "0.0.0.0:8091"

peers:
  - id: "macbook-001"
    name: "MacBook"
    addr: "192.168.1.10:8091"
    # API key loaded from ENX_SYNC_API_KEY env var
  - id: "ubuntu-laptop-001"
    name: "Ubuntu Laptop"
    addr: "192.168.1.20:8091"
    # Same API key on all nodes

security:
  api_key_env: "ENX_SYNC_API_KEY"  # Environment variable name
  min_key_length: 32               # Minimum key length requirement
```

**Security Considerations:**

‚úÖ **Sufficient for home LAN:**
- All nodes are your own devices (trusted)
- Network is physically secured (home router)
- No external access to sync service

‚ö†Ô∏è **Limitations:**
- Key is shared across all nodes (can't revoke per-node)
- If one node is compromised, all nodes are at risk
- No key rotation mechanism (need to update all nodes)

**When to upgrade to JWT/mTLS:**
- Multiple users need different access levels
- Exposing sync service over internet
- Need per-node access revocation
- Compliance requirements (enterprise)

**For now: Pre-shared key is simple, secure enough for home use, and easy to maintain.**

---

### Alternative Authentication Methods (Future Reference)

gRPC supports more complex authentication methods if needed in the future:

#### JWT Token-Based Authentication (For Future Reference)

<details>
<summary>Click to expand: JWT implementation details (not used in initial version)</summary>

```go
// Example JWT implementation (for future reference)
type Claims struct {
    UserID   int64  `json:"user_id"`
    Username string `json:"username"`
    jwt.StandardClaims
}

// Unary interceptor for JWT validation
func (a *AuthInterceptor) Unary() grpc.UnaryServerInterceptor {
    return func(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo,
                handler grpc.UnaryHandler) (interface{}, error) {

        // Skip authentication for public endpoints
        if isPublicMethod(info.FullMethod) {
            return handler(ctx, req)
        }

        // Extract metadata
        md, ok := metadata.FromIncomingContext(ctx)
        if !ok {
            return nil, status.Error(codes.Unauthenticated, "missing metadata")
        }

        // Get authorization token
        tokens := md.Get("authorization")
        if len(tokens) == 0 {
            return nil, status.Error(codes.Unauthenticated, "missing authorization token")
        }

        // Validate token
        claims, err := a.validateToken(tokens[0])
        if err != nil {
            return nil, status.Error(codes.Unauthenticated, fmt.Sprintf("invalid token: %v", err))
        }

        // Add user info to context
        ctx = context.WithValue(ctx, "user_id", claims.UserID)
        ctx = context.WithValue(ctx, "username", claims.Username)

        return handler(ctx, req)
    }
}

// Validate JWT token
func (a *AuthInterceptor) validateToken(tokenString string) (*Claims, error) {
    token, err := jwt.ParseWithClaims(tokenString, &Claims{}, func(token *jwt.Token) (interface{}, error) {
        if _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok {
            return nil, fmt.Errorf("unexpected signing method: %v", token.Header["alg"])
        }
        return a.jwtSecret, nil
    })

    if err != nil {
        return nil, err
    }

    if claims, ok := token.Claims.(*Claims); ok && token.Valid {
        return claims, nil
    }

    return nil, fmt.Errorf("invalid token")
}

// Public methods that don't require authentication
func isPublicMethod(method string) bool {
    publicMethods := map[string]bool{
        "/enx.data.DataService/HealthCheck": true,
        "/enx.data.AuthService/Login":       true,
        "/enx.data.AuthService/Register":    true,
    }
    return publicMethods[method]
}

// Create gRPC server with authentication
func NewAuthenticatedServer(jwtSecret string) *grpc.Server {
    authInterceptor := &AuthInterceptor{
        jwtSecret: []byte(jwtSecret),
    }

    server := grpc.NewServer(
        grpc.UnaryInterceptor(authInterceptor.Unary()),
    )

    return server
}

// ==================== Client Side ====================

// Client with JWT token
type AuthClient struct {
    conn  *grpc.ClientConn
    token string
}

// Create authenticated client
func NewAuthClient(addr, token string) (*AuthClient, error) {
    conn, err := grpc.Dial(addr,
        grpc.WithInsecure(),
        grpc.WithUnaryInterceptor(tokenInterceptor(token)),
    )
    if err != nil {
        return nil, err
    }

    return &AuthClient{
        conn:  conn,
        token: token,
    }, nil
}

// Client interceptor to add token to requests
func tokenInterceptor(token string) grpc.UnaryClientInterceptor {
    return func(ctx context.Context, method string, req, reply interface{},
                cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption) error {

        // Add token to metadata
        ctx = metadata.AppendToOutgoingContext(ctx, "authorization", token)

        return invoker(ctx, method, req, reply, cc, opts...)
    }
}

// Example: Login to get token
func Login(client AuthServiceClient, username, password string) (string, error) {
    ctx := context.Background()

    resp, err := client.Login(ctx, &LoginRequest{
        Username: username,
        Password: password,
    })
    if err != nil {
        return "", err
    }

    return resp.Token, nil
}

// Generate JWT token (server-side)
func GenerateToken(userID int64, username string, secret []byte, duration time.Duration) (string, error) {
    claims := &Claims{
        UserID:   userID,
        Username: username,
        StandardClaims: jwt.StandardClaims{
            ExpiresAt: time.Now().Add(duration).Unix(),
            IssuedAt:  time.Now().Unix(),
        },
    }

    token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)
    return token.SignedString(secret)
}
```

#### 2. **Basic Authentication (Username/Password)**

**Simple username/password authentication**

```go
// ==================== Server Side ====================

type BasicAuthInterceptor struct {
    users map[string]string // username -> password hash
}

func (b *BasicAuthInterceptor) Unary() grpc.UnaryServerInterceptor {
    return func(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo,
                handler grpc.UnaryHandler) (interface{}, error) {

        md, ok := metadata.FromIncomingContext(ctx)
        if !ok {
            return nil, status.Error(codes.Unauthenticated, "missing credentials")
        }

        // Get username and password
        usernames := md.Get("username")
        passwords := md.Get("password")

        if len(usernames) == 0 || len(passwords) == 0 {
            return nil, status.Error(codes.Unauthenticated, "missing username or password")
        }

        // Validate credentials
        if !b.validateCredentials(usernames[0], passwords[0]) {
            return nil, status.Error(codes.Unauthenticated, "invalid credentials")
        }

        ctx = context.WithValue(ctx, "username", usernames[0])
        return handler(ctx, req)
    }
}

func (b *BasicAuthInterceptor) validateCredentials(username, password string) bool {
    expectedHash, exists := b.users[username]
    if !exists {
        return false
    }

    // Compare password hash (use bcrypt in production)
    return comparePasswordHash(password, expectedHash)
}

// ==================== Client Side ====================

type BasicAuthClient struct {
    conn     *grpc.ClientConn
    username string
    password string
}

func basicAuthInterceptor(username, password string) grpc.UnaryClientInterceptor {
    return func(ctx context.Context, method string, req, reply interface{},
                cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption) error {

        // Add credentials to metadata
        ctx = metadata.AppendToOutgoingContext(ctx,
            "username", username,
            "password", password,
        )

        return invoker(ctx, method, req, reply, cc, opts...)
    }
}

func NewBasicAuthClient(addr, username, password string) (*BasicAuthClient, error) {
    conn, err := grpc.Dial(addr,
        grpc.WithInsecure(),
        grpc.WithUnaryInterceptor(basicAuthInterceptor(username, password)),
    )
    if err != nil {
        return nil, err
    }

    return &BasicAuthClient{
        conn:     conn,
        username: username,
        password: password,
    }, nil
}
```

#### 3. **API Key Authentication**

**Authentication using API Keys**

```go
// ==================== Server Side ====================

type APIKeyInterceptor struct {
    validKeys map[string]string // apiKey -> userID
}

func (a *APIKeyInterceptor) Unary() grpc.UnaryServerInterceptor {
    return func(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo,
                handler grpc.UnaryHandler) (interface{}, error) {

        md, ok := metadata.FromIncomingContext(ctx)
        if !ok {
            return nil, status.Error(codes.Unauthenticated, "missing metadata")
        }

        // Get API key
        keys := md.Get("x-api-key")
        if len(keys) == 0 {
            return nil, status.Error(codes.Unauthenticated, "missing API key")
        }

        // Validate API key
        userID, valid := a.validKeys[keys[0]]
        if !valid {
            return nil, status.Error(codes.Unauthenticated, "invalid API key")
        }

        ctx = context.WithValue(ctx, "user_id", userID)
        return handler(ctx, req)
    }
}

// ==================== Client Side ====================

func apiKeyInterceptor(apiKey string) grpc.UnaryClientInterceptor {
    return func(ctx context.Context, method string, req, reply interface{},
                cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption) error {

        ctx = metadata.AppendToOutgoingContext(ctx, "x-api-key", apiKey)
        return invoker(ctx, method, req, reply, cc, opts...)
    }
}
```

#### 4. **OAuth2 / OpenID Connect**

**Enterprise-grade authentication solution**

```go
import (
    "golang.org/x/oauth2"
    "google.golang.org/grpc/credentials/oauth"
)

// Client with OAuth2
func NewOAuth2Client(addr, accessToken string) (*grpc.ClientConn, error) {
    perRPC := oauth.NewOauthAccess(&oauth2.Token{
        AccessToken: accessToken,
    })

    return grpc.Dial(addr,
        grpc.WithPerRPCCredentials(perRPC),
        grpc.WithTransportCredentials(insecure.NewCredentials()),
    )
}
```

#### 5. **Mutual TLS (mTLS)**

**Bidirectional certificate authentication between client and server**

```go
// ==================== Server Side ====================

import (
    "crypto/tls"
    "crypto/x509"
    "io/ioutil"

    "google.golang.org/grpc"
    "google.golang.org/grpc/credentials"
)

func NewMTLSServer(certFile, keyFile, caFile string) (*grpc.Server, error) {
    // Load server certificate
    cert, err := tls.LoadX509KeyPair(certFile, keyFile)
    if err != nil {
        return nil, err
    }

    // Load CA certificate
    caCert, err := ioutil.ReadFile(caFile)
    if err != nil {
        return nil, err
    }

    caPool := x509.NewCertPool()
    caPool.AppendCertsFromPEM(caCert)

    // Configure TLS
    tlsConfig := &tls.Config{
        Certificates: []tls.Certificate{cert},
        ClientAuth:   tls.RequireAndVerifyClientCert,
        ClientCAs:    caPool,
    }

    creds := credentials.NewTLS(tlsConfig)
    server := grpc.NewServer(grpc.Creds(creds))

    return server, nil
}

// ==================== Client Side ====================

func NewMTLSClient(addr, certFile, keyFile, caFile string) (*grpc.ClientConn, error) {
    // Load client certificate
    cert, err := tls.LoadX509KeyPair(certFile, keyFile)
    if err != nil {
        return nil, err
    }

    // Load CA certificate
    caCert, err := ioutil.ReadFile(caFile)
    if err != nil {
        return nil, err
    }

    caPool := x509.NewCertPool()
    caPool.AppendCertsFromPEM(caCert)

    tlsConfig := &tls.Config{
        Certificates: []tls.Certificate{cert},
        RootCAs:      caPool,
    }

    creds := credentials.NewTLS(tlsConfig)
    return grpc.Dial(addr, grpc.WithTransportCredentials(creds))
}
```

</details>

---

## Monitoring and Observability

### Metrics

```go
// Prometheus metrics
var (
    requestDuration = promauto.NewHistogramVec(
        prometheus.HistogramOpts{
            Name: "enx_data_service_request_duration_seconds",
            Help: "Request duration in seconds",
        },
        []string{"method", "status"},
    )

    syncOperations = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Name: "enx_data_service_sync_operations_total",
            Help: "Total number of sync operations",
        },
        []string{"type", "node"},
    )
)

// Instrument RPC calls
func instrumentedHandler(ctx context.Context, req interface{}) (interface{}, error) {
    start := time.Now()
    resp, err := originalHandler(ctx, req)
    duration := time.Since(start).Seconds()

    status := "success"
    if err != nil {
        status = "error"
    }

    requestDuration.WithLabelValues(method, status).Observe(duration)
    return resp, err
}
```

### Logging

```go
// Structured logging
func LoggingInterceptor() grpc.UnaryServerInterceptor {
    return func(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo,
                handler grpc.UnaryHandler) (interface{}, error) {
        start := time.Now()

        log.WithFields(log.Fields{
            "method": info.FullMethod,
            "request": req,
        }).Info("RPC started")

        resp, err := handler(ctx, req)

        log.WithFields(log.Fields{
            "method": info.FullMethod,
            "duration": time.Since(start),
            "error": err,
        }).Info("RPC completed")

        return resp, err
    }
}
```

### Tracing

```go
// OpenTelemetry tracing
import "go.opentelemetry.io/otel"

func TracingInterceptor() grpc.UnaryServerInterceptor {
    return func(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo,
                handler grpc.UnaryHandler) (interface{}, error) {
        tracer := otel.Tracer("enx-data-service")
        ctx, span := tracer.Start(ctx, info.FullMethod)
        defer span.End()

        resp, err := handler(ctx, req)
        if err != nil {
            span.RecordError(err)
        }

        return resp, err
    }
}
```

## SQLite WAL (Write-Ahead Logging) - Performance Optimization

**‚ö†Ô∏è IMPORTANT: WAL is a SQLite configuration for better performance, NOT a requirement for P2P sync**

**Clarification**:
- ‚ùå WAL does NOT affect sync logic (sync is based on timestamps, not WAL)
- ‚úÖ WAL improves concurrent read/write performance on local SQLite
- ‚úÖ WAL is optional - sync works with or without WAL mode
- üí° Recommendation: Enable WAL for better local performance, but it's independent of sync

### What is WAL?

**WAL (Write-Ahead Logging)** is an alternative journaling mode in SQLite that provides better concurrency and performance compared to the traditional rollback journal.

**Official Documentation**: https://www.sqlite.org/wal.html

### Core Concept

In WAL mode, changes are written to a separate **WAL file** before being applied to the main database:

```
Traditional Mode (Rollback Journal):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ enx.db   ‚îÇ ‚Üê Direct write (locks entire file)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ Rollback ‚îÇ ‚Üê Backup for crash recovery
‚îÇ Journal  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

WAL Mode:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ enx.db   ‚îÇ ‚Üê Main database (readers read here)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ enx.db-wal‚îÇ ‚Üê Write-Ahead Log (writers write here first)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ enx.db-shm‚îÇ ‚Üê Shared memory (coordination)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### How WAL Works

#### 1. **Write Flow**

```
Step 1: Write to WAL file
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Client: INSERT INTO words VALUES ('hello', '‰Ω†Â•Ω')
SQLite: Appends change to enx.db-wal
        (Main database enx.db NOT modified yet)

Step 2: Transaction commit
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Client: COMMIT
SQLite: Marks transaction complete in WAL
        WAL file now contains: [INSERT hello ‰Ω†Â•Ω] [COMMIT]

Step 3: Checkpoint (periodic)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
SQLite: Copies changes from WAL to main database
        enx.db-wal ‚Üí enx.db
        Truncates or resets WAL file
```

#### 2. **Read Flow**

```
Traditional Mode:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Reader: SELECT * FROM words
        Must wait if writer is active ‚ùå

WAL Mode:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Reader: SELECT * FROM words
        Reads from enx.db + uncommitted changes in WAL
        Can read even while writer is writing ‚úÖ
```

#### 3. **Read with Uncommitted WAL (Your Question!)**

**‚úÖ Readers can see data in WAL even before checkpoint**

```
Timeline: UPDATE in WAL, Query before Checkpoint
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

10:00:00 - Initial State
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
enx.db:
  id | english | chinese
  ---|---------|--------
  1  | hello   | ‰Ω†Â•Ω

enx.db-wal: (empty)

10:00:01 - Writer: UPDATE chinese
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
UPDATE words SET chinese = 'ÊÇ®Â•Ω' WHERE english = 'hello';
COMMIT;

enx.db: (unchanged)
  id | english | chinese
  ---|---------|--------
  1  | hello   | ‰Ω†Â•Ω     ‚Üê Still old value

enx.db-wal: (has new data)
  Frame 1: [UPDATE words id=1 chinese='ÊÇ®Â•Ω']
  Frame 2: [COMMIT]

10:00:02 - Reader: Query BEFORE Checkpoint
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
SELECT * FROM words WHERE english = 'hello';

SQLite reads:
  Step 1: Check enx.db-shm (shared memory index)
          ‚Üí Finds: "WAL has data for page containing id=1"

  Step 2: Read from WAL (priority over main DB)
          ‚Üí Gets: chinese = 'ÊÇ®Â•Ω' (from WAL)

  Step 3: Returns merged result
          ‚Üí Result: (1, 'hello', 'ÊÇ®Â•Ω') ‚úÖ NEW DATA!

Reader sees updated data ‚úÖ even though:
  - Data still only in WAL
  - enx.db not updated yet
  - Checkpoint hasn't happened yet

10:05:00 - Later: Checkpoint happens
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
PRAGMA wal_checkpoint(PASSIVE);

enx.db: (now updated)
  id | english | chinese
  ---|---------|--------
  1  | hello   | ÊÇ®Â•Ω     ‚Üê Updated from WAL

enx.db-wal: (reset/truncated)
```

**How SQLite merges reads from enx.db + WAL**:

```
Reader query process in WAL mode:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

1. Query starts: SELECT * FROM words WHERE id = 1

2. SQLite checks WAL index (in enx.db-shm):
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ WAL Index (Shared Memory)       ‚îÇ
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ Page 1: Frame 5 (in WAL)        ‚îÇ ‚Üê Page 1 has newer version in WAL
   ‚îÇ Page 2: Not in WAL              ‚îÇ
   ‚îÇ Page 3: Frame 7 (in WAL)        ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

3. For page containing id=1:
   if (page in WAL) {
       Read from WAL  ‚úÖ (newer version)
   } else {
       Read from enx.db (no WAL changes)
   }

4. Result: Merged view
   - Some pages from enx.db (no changes)
   - Some pages from WAL (has changes)
   - Reader sees consistent snapshot
```

**Detailed example with multiple records**:

```
Scenario: Partial data in WAL
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

enx.db (main database):
  id | english | chinese | update_time
  ---|---------|---------|------------
  1  | hello   | ‰Ω†Â•Ω    | 10:00
  2  | world   | ‰∏ñÁïå    | 10:00
  3  | bye     | ÂÜçËßÅ    | 10:00

enx.db-wal (has updates for id=1 and id=3):
  Frame 1: UPDATE words SET chinese='ÊÇ®Â•Ω' WHERE id=1
  Frame 2: UPDATE words SET chinese='ÊãúÊãú' WHERE id=3
  (id=2 not updated, not in WAL)

Query: SELECT * FROM words ORDER BY id;

SQLite reads:
  Row 1 (id=1): Check WAL ‚Üí Found Frame 1 ‚Üí Return 'ÊÇ®Â•Ω' (from WAL) ‚úÖ
  Row 2 (id=2): Check WAL ‚Üí Not found ‚Üí Return '‰∏ñÁïå' (from enx.db) ‚úÖ
  Row 3 (id=3): Check WAL ‚Üí Found Frame 2 ‚Üí Return 'ÊãúÊãú' (from WAL) ‚úÖ

Result:
  id | english | chinese | Source
  ---|---------|---------|--------
  1  | hello   | ÊÇ®Â•Ω    | WAL ‚úÖ
  2  | world   | ‰∏ñÁïå    | enx.db
  3  | bye     | ÊãúÊãú    | WAL ‚úÖ

Reader sees a consistent, merged view!
```

**Consistency guarantees**:

```
Transaction consistency in WAL mode:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Scenario: Writer in middle of transaction
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
10:00:00 - BEGIN TRANSACTION
10:00:01 - UPDATE words SET chinese='ÊÇ®Â•Ω' WHERE id=1  ‚Üê In WAL, uncommitted
10:00:02 - Reader: SELECT ... WHERE id=1
           ‚Üí Sees OLD data ('‰Ω†Â•Ω') ‚úÖ Correct!
           ‚Üí Uncommitted changes not visible

10:00:03 - UPDATE words SET chinese='ÊÇ®Â•Ω' WHERE id=2  ‚Üê In WAL, uncommitted
10:00:04 - COMMIT ‚úÖ
10:00:05 - Reader: SELECT ... WHERE id=1
           ‚Üí Sees NEW data ('ÊÇ®Â•Ω') ‚úÖ Correct!
           ‚Üí Committed changes now visible

Key point: Readers see consistent snapshots
  - Before COMMIT: Old data
  - After COMMIT: New data (from WAL)
  - No partial/inconsistent reads
```

**Performance implications**:

```
Why reading from WAL is fast:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

1. WAL Index in Shared Memory (enx.db-shm):
   - Hash table: page_number ‚Üí frame_number
   - O(1) lookup: "Is this page in WAL?"
   - Very fast (in RAM, not disk)

2. Sequential reads:
   If page in WAL: Read WAL frame (sequential)
   Else:           Read enx.db page (random, but cached)

3. Most pages NOT in WAL:
   - Only recently changed pages in WAL
   - Most data still from enx.db (fast)
   - WAL overhead minimal (~5-10% slower)

Example:
  Database: 1000 pages
  Recent changes: 10 pages in WAL

  Query touching 50 pages:
    40 pages from enx.db (fast, no WAL overhead)
    10 pages from WAL (check index + read WAL)

  Total overhead: 10/50 = 20% of reads check WAL
  Performance impact: ~2% slower (negligible)
```

### File Structure

#### **enx.db** (Main Database)
- Contains "checkpoint" state of data
- Updated periodically by checkpoint process
- Readers primarily read from here

#### **enx.db-wal** (Write-Ahead Log)
- Append-only file containing recent transactions
- Format: [page_number | page_data | frame_header] repeated
- Grows until checkpoint occurs

#### **enx.db-shm** (Shared Memory)
- Coordination between readers and writers
- Contains WAL index for fast lookups
- Automatically managed by SQLite

```
$ ls -lh enx.db*
-rw-r--r--  1 user  staff  500K Nov 12 10:00 enx.db
-rw-r--r--  1 user  staff   64K Nov 12 10:05 enx.db-wal    ‚Üê Recent changes
-rw-r--r--  1 user  staff   32K Nov 12 10:05 enx.db-shm    ‚Üê Coordination
```

### Key Advantages

#### 1. **Concurrent Readers and Writers**

```
Traditional Mode:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Time    Reader          Writer          Result
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
10:00   SELECT ...      -               ‚úÖ Read
10:01   SELECT ...      INSERT ...      ‚ùå Blocked
10:02   -               INSERT ...      ‚úÖ Write
10:03   SELECT ...      INSERT ...      ‚ùå Blocked

Problem: Readers block writers, writers block readers
```

```
WAL Mode:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Time    Reader          Writer          Result
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
10:00   SELECT ...      -               ‚úÖ Read
10:01   SELECT ...      INSERT ...      ‚úÖ Both work
10:02   SELECT ...      INSERT ...      ‚úÖ Both work
10:03   SELECT ...      INSERT ...      ‚úÖ Both work

Benefit: Readers and writers don't block each other
```

#### 2. **Better Write Performance**

```
Traditional Mode:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
INSERT: Write to rollback journal ‚Üí Write to database
        2 writes, synchronous, slower

WAL Mode:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
INSERT: Append to WAL file
        1 write, sequential append, much faster
```

**Benchmark**:
```
Operation: Insert 1000 rows

Traditional Mode: 5.2 seconds
WAL Mode:         1.1 seconds  (5x faster!)
```

#### 3. **Crash Recovery**

```
Traditional Mode:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Crash during write ‚Üí Rollback journal replays changes
                   ‚Üí Database restored to previous state

WAL Mode:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Crash during write ‚Üí WAL file contains complete transactions
                   ‚Üí Committed transactions in WAL applied
                   ‚Üí Uncommitted transactions ignored
                   ‚Üí Database in consistent state
```

### Checkpoint Process

**Checkpoint** moves changes from WAL to main database:

```
Before Checkpoint:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ enx.db   ‚îÇ       ‚îÇ enx.db-wal‚îÇ
‚îÇ 1000 rows‚îÇ       ‚îÇ +50 rows  ‚îÇ ‚Üê Recent inserts
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

After Checkpoint:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ enx.db   ‚îÇ       ‚îÇ enx.db-wal‚îÇ
‚îÇ 1050 rows‚îÇ ‚úÖ    ‚îÇ (empty)   ‚îÇ ‚Üê Truncated
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Checkpoint Modes

```sql
-- PASSIVE: Don't block, checkpoint what's possible
PRAGMA wal_checkpoint(PASSIVE);

-- FULL: Wait for readers, checkpoint everything
PRAGMA wal_checkpoint(FULL);

-- RESTART: FULL + start new WAL file
PRAGMA wal_checkpoint(RESTART);

-- TRUNCATE: RESTART + truncate WAL to 0 bytes
PRAGMA wal_checkpoint(TRUNCATE);
```

### Manual Checkpoint APIs

**‚úÖ Yes! SQLite provides multiple APIs to manually trigger WAL checkpoint**

#### 1. SQL API (PRAGMA)

**ÊúÄÁÆÄÂçïÁöÑÊñπÂºèÔºö‰ΩøÁî® PRAGMA ËØ≠Âè•**

```sql
-- PASSIVE checkpoint (Êé®ËçêÁî®‰∫éÂêéÂè∞‰ªªÂä°)
PRAGMA wal_checkpoint(PASSIVE);
-- ËøîÂõûÔºö(busy, log, checkpointed)
-- ‰æãÂ¶ÇÔºö0|100|100
-- ÊÑèÊÄùÔºö0=ÊàêÂäü, 100È°µÂú®Êó•Âøó‰∏≠, 100È°µÂ∑≤checkpoint

-- FULL checkpoint (Á≠âÂæÖËØªËÄÖÂÆåÊàê)
PRAGMA wal_checkpoint(FULL);

-- RESTART checkpoint (FULL + ÈáçÁΩÆWAL)
PRAGMA wal_checkpoint(RESTART);

-- TRUNCATE checkpoint (RESTART + Êà™Êñ≠WALÂà∞0Â≠óËäÇ)
PRAGMA wal_checkpoint(TRUNCATE);

-- ‰∏çÊåáÂÆöÊ®°ÂºèÔºàÈªòËÆ§ PASSIVEÔºâ
PRAGMA wal_checkpoint;
```

**ËøîÂõûÂÄºËß£Èáä**Ôºö

```
PRAGMA wal_checkpoint(mode) ËøîÂõûÔºö(busy, log_pages, checkpointed_pages)

busy:
  0 = ÊàêÂäü checkpoint ÊâÄÊúâÈ°µ
  1 = ÈÉ®ÂàÜÈ°µÂõ†‰∏∫ÊúâÊ¥ªË∑ÉËØªËÄÖËÄåÊó†Ê≥ï checkpoint

log_pages:
  WAL Êñá‰ª∂‰∏≠ÁöÑÊÄªÈ°µÊï∞

checkpointed_pages:
  ÊàêÂäü checkpoint ÁöÑÈ°µÊï∞

Á§∫‰æãÔºö
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
PRAGMA wal_checkpoint(PASSIVE);
‚Üí 0|500|500  ‚úÖ ÊàêÂäü checkpoint 500È°µ
‚Üí 1|500|300  ‚ö†Ô∏è Âè™ checkpoint ‰∫Ü300È°µÔºà200È°µÊúâËØªËÄÖÂú®Áî®Ôºâ
‚Üí 0|0|0      ‚úÖ WAL ‰∏∫Á©∫ÔºåÊó†ÈúÄ checkpoint
```

#### 2. Go API (database/sql)

**Âú® Go ‰∏≠Ëß¶Âèë checkpoint**

```go
package main

import (
    "database/sql"
    "fmt"
    "log"

    _ "github.com/mattn/go-sqlite3"
)

// Simple checkpoint (PASSIVE mode)
func checkpointSimple(db *sql.DB) error {
    _, err := db.Exec("PRAGMA wal_checkpoint(PASSIVE)")
    return err
}

// Checkpoint with result
func checkpointWithResult(db *sql.DB, mode string) (busy, logPages, checkpointed int, err error) {
    query := fmt.Sprintf("PRAGMA wal_checkpoint(%s)", mode)
    err = db.QueryRow(query).Scan(&busy, &logPages, &checkpointed)
    return
}

// Example usage
func main() {
    db, _ := sql.Open("sqlite3", "enx.db")
    defer db.Close()

    // Enable WAL
    db.Exec("PRAGMA journal_mode=WAL")

    // ... perform writes ...
    db.Exec("INSERT INTO words VALUES (...)")
    db.Exec("UPDATE user_dicts SET ...")

    // Manual checkpoint
    busy, log, ckpt, err := checkpointWithResult(db, "PASSIVE")
    if err != nil {
        log.Fatal(err)
    }

    fmt.Printf("Checkpoint result:\n")
    fmt.Printf("  Busy: %d\n", busy)
    fmt.Printf("  WAL pages: %d\n", log)
    fmt.Printf("  Checkpointed: %d\n", ckpt)

    if busy == 1 {
        fmt.Println("‚ö†Ô∏è Some pages couldn't be checkpointed (active readers)")
    } else {
        fmt.Println("‚úÖ All pages checkpointed successfully")
    }
}

// Checkpoint with retry
func checkpointWithRetry(db *sql.DB, maxRetries int) error {
    for i := 0; i < maxRetries; i++ {
        busy, _, _, err := checkpointWithResult(db, "PASSIVE")
        if err != nil {
            return err
        }

        if busy == 0 {
            log.Printf("‚úÖ Checkpoint successful on attempt %d", i+1)
            return nil
        }

        log.Printf("‚ö†Ô∏è Checkpoint busy, retrying (%d/%d)", i+1, maxRetries)
        time.Sleep(100 * time.Millisecond)
    }

    return fmt.Errorf("checkpoint failed after %d retries", maxRetries)
}

// Background checkpoint worker
func startCheckpointWorker(db *sql.DB, interval time.Duration) {
    ticker := time.NewTicker(interval)
    defer ticker.Stop()

    for range ticker.C {
        busy, log, ckpt, err := checkpointWithResult(db, "PASSIVE")
        if err != nil {
            log.Printf("‚ùå Checkpoint error: %v", err)
            continue
        }

        walSizeMB := float64(log * 4096) / 1024 / 1024
        log.Printf("üìä Checkpoint: WAL %.2fMB (%d pages), checkpointed %d pages, busy: %d",
            walSizeMB, log, ckpt, busy)

        // Force RESTART if WAL too large and not busy
        if walSizeMB > 50 && busy == 0 {
            log.Println("‚ö†Ô∏è WAL > 50MB, forcing RESTART checkpoint")
            db.Exec("PRAGMA wal_checkpoint(RESTART)")
        }
    }
}

// Checkpoint on shutdown
func gracefulShutdown(db *sql.DB) {
    log.Println("üõë Shutting down, performing final checkpoint...")

    // Use TRUNCATE to clean up WAL file
    busy, log, ckpt, err := checkpointWithResult(db, "TRUNCATE")
    if err != nil {
        log.Printf("‚ùå Final checkpoint error: %v", err)
    } else {
        log.Printf("‚úÖ Final checkpoint: %d/%d pages, WAL truncated", ckpt, log)
    }

    db.Close()
    log.Println("‚úÖ Database closed cleanly")
}
```

#### 3. C API (for advanced use)

**SQLite C API**

```c
#include <sqlite3.h>

// Basic checkpoint
int checkpoint_basic(sqlite3 *db) {
    return sqlite3_wal_checkpoint(db, NULL);  // NULL = all databases
}

// Checkpoint with mode (v2 API, recommended)
int checkpoint_with_mode(sqlite3 *db, int mode) {
    int nLog, nCkpt;  // Output parameters

    int rc = sqlite3_wal_checkpoint_v2(
        db,              // Database connection
        NULL,            // Database name (NULL = all attached DBs)
        mode,            // SQLITE_CHECKPOINT_PASSIVE/FULL/RESTART/TRUNCATE
        &nLog,           // OUT: Pages in WAL
        &nCkpt           // OUT: Pages checkpointed
    );

    printf("Checkpoint: WAL=%d pages, checkpointed=%d pages\n", nLog, nCkpt);

    return rc;
}

// Checkpoint modes
#define SQLITE_CHECKPOINT_PASSIVE  0
#define SQLITE_CHECKPOINT_FULL     1
#define SQLITE_CHECKPOINT_RESTART  2
#define SQLITE_CHECKPOINT_TRUNCATE 3

// Example usage
void example() {
    sqlite3 *db;
    sqlite3_open("enx.db", &db);

    // Enable WAL
    sqlite3_exec(db, "PRAGMA journal_mode=WAL", NULL, NULL, NULL);

    // ... perform operations ...

    // Checkpoint
    int nLog, nCkpt;
    int rc = sqlite3_wal_checkpoint_v2(
        db, NULL, SQLITE_CHECKPOINT_PASSIVE, &nLog, &nCkpt
    );

    if (rc == SQLITE_OK) {
        printf("‚úÖ Checkpoint successful: %d/%d pages\n", nCkpt, nLog);
    } else {
        printf("‚ùå Checkpoint failed: %s\n", sqlite3_errmsg(db));
    }

    sqlite3_close(db);
}
```

#### 4. Checkpoint Hooks (C API)

**Ê≥®ÂÜå checkpoint ÂõûË∞É**

```c
// Checkpoint hook function
int checkpoint_hook(void *pArg, sqlite3 *db, const char *zDb, int nFrame) {
    printf("Checkpoint hook: %d frames to checkpoint\n", nFrame);
    return SQLITE_OK;  // Allow checkpoint
}

// Register hook
void setup_checkpoint_hook(sqlite3 *db) {
    sqlite3_wal_hook(db, checkpoint_hook, NULL);
}

// This hook is called BEFORE auto-checkpoint
// Can be used to:
// - Log checkpoint events
// - Delay checkpoint if needed
// - Perform cleanup before checkpoint
```

#### 5. When to Trigger Manual Checkpoint

**‰ΩøÁî®Âú∫ÊôØ**

```
Scenario 1: Application shutdown
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
func main() {
    db, _ := sql.Open("sqlite3", "enx.db")
    defer func() {
        // Clean shutdown with TRUNCATE
        db.Exec("PRAGMA wal_checkpoint(TRUNCATE)")
        db.Close()
    }()

    // ... application logic ...
}

Why: Ensures WAL is merged and cleaned up before exit


Scenario 2: Before backup
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
func backupDatabase(db *sql.DB) error {
    // Checkpoint to merge WAL into main DB
    _, err := db.Exec("PRAGMA wal_checkpoint(TRUNCATE)")
    if err != nil {
        return err
    }

    // Now safe to copy enx.db (contains all data)
    return copyFile("enx.db", "backup/enx.db")
}

Why: Ensures backup includes all data (not just main DB)


Scenario 3: Low disk space
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
func monitorDiskSpace(db *sql.DB) {
    ticker := time.NewTicker(1 * time.Minute)
    for range ticker.C {
        diskFree := getDiskFreeSpace()
        if diskFree < 100*1024*1024 {  // < 100MB
            log.Warn("Low disk space, checkpointing WAL")
            db.Exec("PRAGMA wal_checkpoint(RESTART)")
        }
    }
}

Why: Merge WAL to free up disk space


Scenario 4: Performance maintenance
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
func performanceMaintenance(db *sql.DB) {
    var logPages int
    db.QueryRow("PRAGMA wal_checkpoint(PASSIVE)").Scan(&_, &logPages, &_)

    walSizeMB := float64(logPages * 4096) / 1024 / 1024
    if walSizeMB > 50 {
        log.Warn("WAL > 50MB, forcing checkpoint")
        db.Exec("PRAGMA wal_checkpoint(RESTART)")
    }
}

Why: Keep WAL size manageable for good read performance


Scenario 5: Scheduled maintenance
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
func scheduledMaintenance(db *sql.DB) {
    // Run every night at 2 AM
    schedule := cron.New()
    schedule.AddFunc("0 2 * * *", func() {
        log.Info("Running scheduled checkpoint")
        db.Exec("PRAGMA wal_checkpoint(TRUNCATE)")
        db.Exec("PRAGMA optimize")  // Also optimize DB
        log.Info("Maintenance complete")
    })
    schedule.Start()
}

Why: Periodic cleanup during low-activity hours
```

#### 6. Best Practices

**ÊâãÂä® checkpoint ÊúÄ‰Ω≥ÂÆûË∑µ**

```
‚úÖ DO:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

1. Use PASSIVE for background tasks
   - Non-blocking, safe during normal operations
   - Good for periodic maintenance

2. Use TRUNCATE on shutdown
   - Cleans up WAL file completely
   - Leaves database in clean state

3. Use RESTART when WAL > 50MB
   - Keeps WAL size manageable
   - Improves read performance

4. Checkpoint before backup
   - Ensures backup is complete
   - Simpler backup process (one file)

5. Handle busy return value
   - Retry if busy=1 (readers active)
   - Don't force FULL/RESTART if busy


‚ùå DON'T:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

1. Don't use FULL in hot path
   - Blocks on active readers
   - Can cause latency spikes

2. Don't checkpoint too frequently
   - Wastes CPU, defeats WAL purpose
   - Let auto-checkpoint handle it

3. Don't ignore errors
   - Check return value
   - Log failures for debugging

4. Don't checkpoint in transactions
   - Can interfere with transaction
   - Checkpoint outside transaction

5. Don't assume TRUNCATE always truncates
   - May not truncate if readers active
   - Check return value
```

#### 7. Complete Example for Your Project

**ENX È°πÁõÆÂÆåÊï¥Á§∫‰æã**

```go
package main

import (
    "database/sql"
    "log"
    "os"
    "os/signal"
    "syscall"
    "time"

    _ "github.com/mattn/go-sqlite3"
)

type Database struct {
    db *sql.DB
}

func NewDatabase(path string) (*Database, error) {
    db, err := sql.Open("sqlite3", path)
    if err != nil {
        return nil, err
    }

    // Enable WAL
    if _, err := db.Exec("PRAGMA journal_mode=WAL"); err != nil {
        return nil, err
    }

    // Configure auto-checkpoint
    if _, err := db.Exec("PRAGMA wal_autocheckpoint=1000"); err != nil {
        return nil, err
    }

    d := &Database{db: db}

    // Start background checkpoint worker
    go d.checkpointWorker()

    // Setup graceful shutdown
    d.setupGracefulShutdown()

    return d, nil
}

// Background checkpoint worker
func (d *Database) checkpointWorker() {
    ticker := time.NewTicker(5 * time.Minute)
    defer ticker.Stop()

    for range ticker.C {
        d.periodicCheckpoint()
    }
}

// Periodic checkpoint
func (d *Database) periodicCheckpoint() {
    var busy, logPages, checkpointed int
    err := d.db.QueryRow("PRAGMA wal_checkpoint(PASSIVE)").Scan(
        &busy, &logPages, &checkpointed,
    )
    if err != nil {
        log.Printf("‚ùå Checkpoint error: %v", err)
        return
    }

    walSizeMB := float64(logPages * 4096) / 1024 / 1024
    log.Printf("üìä WAL: %.2fMB (%d pages), checkpointed: %d, busy: %d",
        walSizeMB, logPages, checkpointed, busy)

    // Force RESTART if WAL > 50MB and not busy
    if walSizeMB > 50 && busy == 0 {
        log.Println("‚ö†Ô∏è WAL > 50MB, forcing RESTART checkpoint")
        d.db.Exec("PRAGMA wal_checkpoint(RESTART)")
    }
}

// Manual checkpoint (for backup, etc.)
func (d *Database) Checkpoint() error {
    log.Println("üîÑ Manual checkpoint requested")

    var busy, logPages, checkpointed int
    err := d.db.QueryRow("PRAGMA wal_checkpoint(TRUNCATE)").Scan(
        &busy, &logPages, &checkpointed,
    )
    if err != nil {
        return err
    }

    if busy == 1 {
        log.Printf("‚ö†Ô∏è Partial checkpoint: %d/%d pages (readers active)",
            checkpointed, logPages)
    } else {
        log.Printf("‚úÖ Checkpoint complete: %d pages, WAL truncated", logPages)
    }

    return nil
}

// Graceful shutdown
func (d *Database) setupGracefulShutdown() {
    sigChan := make(chan os.Signal, 1)
    signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)

    go func() {
        <-sigChan
        log.Println("üõë Shutdown signal received")
        d.Close()
        os.Exit(0)
    }()
}

// Close with final checkpoint
func (d *Database) Close() error {
    log.Println("üîÑ Performing final checkpoint...")

    // TRUNCATE checkpoint on shutdown
    var busy, logPages, checkpointed int
    err := d.db.QueryRow("PRAGMA wal_checkpoint(TRUNCATE)").Scan(
        &busy, &logPages, &checkpointed,
    )
    if err != nil {
        log.Printf("‚ö†Ô∏è Final checkpoint error: %v", err)
    } else {
        log.Printf("‚úÖ Final checkpoint: %d/%d pages", checkpointed, logPages)
    }

    // Close database
    if err := d.db.Close(); err != nil {
        return err
    }

    log.Println("‚úÖ Database closed cleanly")
    return nil
}

func main() {
    db, err := NewDatabase("enx.db")
    if err != nil {
        log.Fatal(err)
    }
    defer db.Close()

    // Your application logic...

    // Manual checkpoint when needed
    db.Checkpoint()
}
```

### Summary: Checkpoint APIs

| API Type | Command | Use Case |
|----------|---------|----------|
| **SQL** | `PRAGMA wal_checkpoint(mode)` | ÊúÄÁÆÄÂçïÔºåÊâÄÊúâËØ≠Ë®ÄÈÄöÁî® |
| **Go** | `db.Exec("PRAGMA wal_checkpoint(...)")` | Go Â∫îÁî®Êé®Ëçê |
| **C** | `sqlite3_wal_checkpoint_v2()` | È´òÁ∫ßÁî®Êà∑ÔºåC Êâ©Â±ï |
| **Hook** | `sqlite3_wal_hook()` | ÁõëÊéß checkpoint ‰∫ã‰ª∂ |

#### Auto-Checkpoint

```sql
-- Checkpoint automatically when WAL reaches 1000 pages (~4MB)
PRAGMA wal_autocheckpoint=1000;

-- Disable auto-checkpoint (manual control)
PRAGMA wal_autocheckpoint=0;
```

### SQLite Automatic Checkpoint Mechanisms

**‚úÖ Yes! SQLite has multiple automatic checkpoint mechanisms**

#### 1. Auto-Checkpoint (Most Common)

**Âü∫‰∫éÂÜôÂÖ•È°µÊï∞ÁöÑËá™Âä®Ëß¶Âèë**

```
Trigger Condition:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
WAL Êñá‰ª∂ËææÂà∞ÈÖçÁΩÆÁöÑÈ°µÊï∞ÈòàÂÄºÊó∂Ôºå‰∏ã‰∏ÄÊ¨°ÂÜôÊìç‰Ωú‰ºöËß¶Âèë checkpoint

Default: 1000 pages (4MB)
Mode: PASSIVE (non-blocking)

Timing:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Write 1 ‚Üí Write 2 ‚Üí ... ‚Üí Write 999 ‚Üí Write 1000        ‚îÇ
‚îÇ                                         ‚Üì                ‚îÇ
‚îÇ                               Checkpoint triggered!      ‚îÇ
‚îÇ                               (on next write commit)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Important: Checkpoint happens AFTER the threshold is reached,
           on the NEXT write transaction commit
```

**Example**

```go
// Configure auto-checkpoint threshold
db.Exec("PRAGMA wal_autocheckpoint=1000")

// These writes accumulate in WAL
for i := 0; i < 1100; i++ {
    db.Exec("INSERT INTO words VALUES (...)")
}
// After ~1000 pages written, next transaction triggers checkpoint

// Check status
var pages int
db.QueryRow("PRAGMA wal_checkpoint(PASSIVE)").Scan(&_, &pages, &_)
fmt.Printf("WAL pages: %d\n", pages)  // Should be < 100 after checkpoint
```

#### 2. Last Connection Close

**ÊúÄÂêé‰∏Ä‰∏™ËøûÊé•ÂÖ≥Èó≠Êó∂Ëß¶Âèë**

```
Trigger Condition:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
ÂΩìÊúÄÂêé‰∏Ä‰∏™Êï∞ÊçÆÂ∫ìËøûÊé•ÂÖ≥Èó≠Êó∂ÔºåSQLite ‰ºöÂ∞ùËØïÊâßË°å checkpoint

Mode: PASSIVE (default) or TRUNCATE (configurable)

Scenario 1: Single connection
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ App starts ‚Üí Opens DB ‚Üí ... ‚Üí Closes DB                 ‚îÇ
‚îÇ                                    ‚Üì                     ‚îÇ
‚îÇ                          Checkpoint triggered!           ‚îÇ
‚îÇ                          WAL merged to main DB           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Scenario 2: Multiple connections
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Conn 1: Open ‚Üí ... ‚Üí Close                              ‚îÇ
‚îÇ Conn 2: Open ‚Üí ... ‚Üí Close                              ‚îÇ
‚îÇ Conn 3: Open ‚Üí ... ‚Üí Close (last one!)                  ‚îÇ
‚îÇ                         ‚Üì                                ‚îÇ
‚îÇ               Checkpoint triggered!                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Why: Clean up WAL file when database is no longer in use
```

**Example**

```go
func main() {
    // Open database
    db, _ := sql.Open("sqlite3", "enx.db")
    db.Exec("PRAGMA journal_mode=WAL")

    // Perform writes
    db.Exec("INSERT INTO words VALUES (...)")
    db.Exec("UPDATE user_dicts SET ...")

    // Close database (last connection)
    db.Close()  // ‚Üê Triggers checkpoint here!

    // After close:
    // - WAL merged to enx.db
    // - WAL file may be truncated (depending on config)
}

// Check on disk
$ ls -lh enx.db*
-rw-r--r-- 1 user user 500K enx.db        # All data here
-rw-r--r-- 1 user user  32K enx.db-wal    # Small residual (or truncated)
-rw-r--r-- 1 user user  32K enx.db-shm    # Shared memory
```

#### 3. Checkpoint on Commit (Conditional)

**Êèê‰∫§‰∫ãÂä°Êó∂ÁöÑÊù°‰ª∂Ëß¶Âèë**

```
Trigger Condition:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
ÂÜô‰∫ãÂä° COMMIT Êó∂ÔºåÂ¶ÇÊûúÊª°Ë∂≥‰ª•‰∏ãÊù°‰ª∂‰πã‰∏ÄÔºö

1. WAL ËææÂà∞ auto-checkpoint ÈòàÂÄº
2. Á≥ªÁªüËµÑÊ∫êÂéãÂäõÔºàÂÜÖÂ≠ò/Á£ÅÁõòÔºâ
3. ÈïøÊó∂Èó¥Êú™ checkpointÔºàÊüê‰∫õÂÆûÁé∞Ôºâ

Timing:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ BEGIN TRANSACTION                                        ‚îÇ
‚îÇ   INSERT INTO words VALUES (...)                         ‚îÇ
‚îÇ   UPDATE user_dicts SET ...                              ‚îÇ
‚îÇ COMMIT  ‚Üê Check if checkpoint needed                    ‚îÇ
‚îÇ         ‚Üì                                                ‚îÇ
‚îÇ   If threshold reached ‚Üí Trigger checkpoint              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Note: This is implementation-dependent behavior
      Primary mechanism is still auto-checkpoint threshold
```

#### 4. Forced Checkpoint (Programmatic)

**Á®ãÂ∫è‰∏ªÂä®Ëß¶Âèë**

```
Trigger Condition:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Â∫îÁî®Á®ãÂ∫èÊòæÂºèË∞ÉÁî® checkpoint API

Methods:
- PRAGMA wal_checkpoint(mode)
- sqlite3_wal_checkpoint_v2()
- Background worker
- Scheduled maintenance

Timing: Anytime, controlled by application

Use Cases:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚úÖ Before backup                                         ‚îÇ
‚îÇ ‚úÖ Before shutdown                                       ‚îÇ
‚îÇ ‚úÖ Low disk space                                        ‚îÇ
‚îÇ ‚úÖ WAL too large (>50MB)                                 ‚îÇ
‚îÇ ‚úÖ Scheduled maintenance (2 AM)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### 5. System Events (Advanced)

**Á≥ªÁªüÁ∫ßËß¶ÂèëÊù°‰ª∂**

```
Trigger Conditions:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

1. Memory Pressure
   - System running low on memory
   - SQLite may checkpoint to free cache

2. Disk Sync Events
   - fsync() or similar system calls
   - Database pages being flushed to disk

3. Lock Contention
   - Many readers waiting
   - Checkpoint to improve read performance

4. Process Termination
   - SIGTERM, SIGINT signals
   - Graceful shutdown attempts checkpoint

Note: These are less common, implementation-specific
      Primary mechanism is still auto-checkpoint threshold
```

#### Complete Auto-Checkpoint Workflow

**ÂÆåÊï¥ÁöÑËá™Âä® checkpoint ÊµÅÁ®ã**

```
Timeline:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

T0: App starts
    ‚îú‚îÄ Open database
    ‚îú‚îÄ Enable WAL: PRAGMA journal_mode=WAL
    ‚îî‚îÄ Set threshold: PRAGMA wal_autocheckpoint=1000

T1-T999: Normal writes (WAL grows)
    ‚îú‚îÄ Write 1: INSERT INTO words ... ‚Üí WAL: 1 page
    ‚îú‚îÄ Write 2: UPDATE user_dicts ... ‚Üí WAL: 2 pages
    ‚îú‚îÄ ...
    ‚îî‚îÄ Write 999: INSERT INTO words ... ‚Üí WAL: 999 pages

T1000: Threshold reached
    ‚îú‚îÄ Write 1000: INSERT INTO words ... ‚Üí WAL: 1000 pages
    ‚îî‚îÄ ‚ö†Ô∏è Threshold reached! Checkpoint scheduled

T1001: Next write commits
    ‚îú‚îÄ BEGIN TRANSACTION
    ‚îú‚îÄ INSERT INTO words ...
    ‚îú‚îÄ COMMIT
    ‚îÇ   ‚îú‚îÄ üîÑ Auto-checkpoint triggered (PASSIVE mode)
    ‚îÇ   ‚îú‚îÄ Merge WAL ‚Üí enx.db (non-blocking)
    ‚îÇ   ‚îî‚îÄ Reset WAL write position
    ‚îî‚îÄ ‚úÖ Transaction committed, checkpoint complete

T1002+: Continue normally
    ‚îú‚îÄ WAL restarted from beginning
    ‚îî‚îÄ Cycle repeats

T_end: App exits
    ‚îú‚îÄ db.Close() called
    ‚îú‚îÄ üîÑ Final checkpoint (last connection)
    ‚îî‚îÄ ‚úÖ Clean shutdown


Monitoring:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Check WAL status anytime:
PRAGMA wal_checkpoint(PASSIVE);
‚Üí Returns: (busy, log_pages, checkpointed_pages)

Example outputs:
0|0|0       ‚úÖ WAL empty, no checkpoint needed
0|500|500   ‚úÖ Checkpoint successful, 500 pages merged
1|1000|800  ‚ö†Ô∏è Partial checkpoint, 200 pages still have readers
```

#### Configuration Examples

**ÈíàÂØπ‰∏çÂêåÂú∫ÊôØÁöÑÈÖçÁΩÆ**

```go
// Scenario 1: Low-frequency writes (your case)
// Default is perfect - checkpoint every 4MB
db.Exec("PRAGMA wal_autocheckpoint=1000")  // 4MB

// Scenario 2: High-frequency writes
// Increase threshold to reduce checkpoint frequency
db.Exec("PRAGMA wal_autocheckpoint=5000")  // 20MB

// Scenario 3: Manual control (advanced)
// Disable auto-checkpoint, manual checkpoints only
db.Exec("PRAGMA wal_autocheckpoint=0")
// Then manually checkpoint in background worker
go func() {
    ticker := time.NewTicker(5 * time.Minute)
    for range ticker.C {
        db.Exec("PRAGMA wal_checkpoint(PASSIVE)")
    }
}()

// Scenario 4: Aggressive checkpointing
// Checkpoint very frequently (for small databases)
db.Exec("PRAGMA wal_autocheckpoint=100")  // 400KB
```

#### Summary: When Does Checkpoint Happen?

| Trigger Mechanism | Frequency | Mode | Automatic? |
|-------------------|-----------|------|------------|
| **Auto-checkpoint** | Every N pages (default: 1000) | PASSIVE | ‚úÖ Yes |
| **Last connection close** | On app exit | PASSIVE/TRUNCATE | ‚úÖ Yes |
| **Commit checkpoint** | On threshold + commit | PASSIVE | ‚úÖ Yes |
| **Manual PRAGMA** | Anytime (app-controlled) | Configurable | ‚ùå No |
| **Background worker** | Scheduled (e.g., 5 min) | Configurable | ‚úÖ Yes (if implemented) |
| **System events** | Rare (memory pressure, etc.) | PASSIVE | ‚úÖ Yes |

**Key Takeaway:**

```
For your ENX project:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚úÖ Auto-checkpoint (default 1000 pages = 4MB) handles most cases
‚úÖ Last connection close cleans up on app exit
‚úÖ No manual intervention needed for normal usage

Optional enhancements:
- Background worker for proactive checkpointing
- Shutdown checkpoint (TRUNCATE) for clean exit
- Monitoring/logging for WAL size tracking
```

### WAL Size Limits and Capacity

#### Default Configuration

**ÈªòËÆ§ Auto-Checkpoint ÈòàÂÄºÔºö1000 È°µ**

```
Page size:       4096 bytes (4KB, SQLite default)
Checkpoint at:   1000 pages
Maximum WAL:     1000 √ó 4KB = 4MB (before auto-checkpoint)

ËÆ°ÁÆóÔºö
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Default: PRAGMA wal_autocheckpoint=1000
         PRAGMA page_size=4096

WAL capacity = 1000 pages √ó 4KB/page = 4MB

ËøôÊÑèÂë≥ÁùÄÔºöWAL ÊúÄÂ§öÁ¥ØÁßØ 4MB Êï∞ÊçÆÂêéËá™Âä®Ëß¶Âèë checkpoint
```

#### Theoretical Maximum (No Limit!)

**WAL ÁêÜËÆ∫‰∏äÊ≤°ÊúâÁ°¨ÊÄßÂ§ßÂ∞èÈôêÂà∂**

```
ÈáçË¶ÅÔºöWAL Êñá‰ª∂Êú¨Ë∫´Ê≤°ÊúâÊúÄÂ§ßÂ§ßÂ∞èÈôêÂà∂ÔºÅ
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Â¶ÇÊûúÁ¶ÅÁî® auto-checkpointÔºö
PRAGMA wal_autocheckpoint=0;

WAL ÂèØ‰ª•Êó†ÈôêÂ¢ûÈïøÔºö
- 10MB, 100MB, 1GB, 10GB... ÁêÜËÆ∫‰∏äÂèØ‰ª•Êó†ÈôêÂ§ß
- Âè™ÂèóÈôê‰∫éÁ£ÅÁõòÁ©∫Èó¥
- ‰ΩÜ‰ºöÂØºËá¥‰∏•ÈáçÊÄßËÉΩÈóÆÈ¢òÔºàËßÅ‰∏ãÊñáÔºâ
```

#### Practical Limits

**ÂÆûÈôÖ‰ΩøÁî®‰∏≠ÁöÑÊé®ËçêÈôêÂà∂**

```
Êé®ËçêÈÖçÁΩÆÔºö
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Small database (<10MB):
  PRAGMA wal_autocheckpoint=1000;    // 4MB WAL

Medium database (10MB-100MB):
  PRAGMA wal_autocheckpoint=2000;    // 8MB WAL

Large database (>100MB):
  PRAGMA wal_autocheckpoint=5000;    // 20MB WAL

Your case (enx.db ~500KB):
  PRAGMA wal_autocheckpoint=1000;    // 4MB WAL ‚úÖ Perfect!

Reason: 4MB WAL = 8x your entire database
        More than enough for typical usage
```

#### How Much Data Can WAL Hold?

**WAL ÂÆπÈáè = È°µÊï∞ √ó È°µÂ§ßÂ∞è**

```
Examples with default 4KB pages:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

wal_autocheckpoint | Max WAL Size | Approximate Data
-------------------|--------------|------------------
100                | 400KB        | ~500 rows (200 bytes each)
1000 (default)     | 4MB          | ~5,000 rows
2000               | 8MB          | ~10,000 rows
5000               | 20MB         | ~25,000 rows
10000              | 40MB         | ~50,000 rows
0 (disabled)       | Unlimited    | Limited only by disk space

Note: "Approximate Data" assumes typical word records
      Actual capacity depends on record size and update patterns
```

#### Calculate Capacity for Your Use Case

**‰ª•‰Ω†ÁöÑ words Ë°®‰∏∫‰æã**

```
Your table structure:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CREATE TABLE words (
    id INTEGER PRIMARY KEY,        -- 8 bytes
    english TEXT NOT NULL,         -- ~10 bytes avg
    chinese TEXT,                  -- ~10 bytes avg
    pronunciation TEXT,            -- ~20 bytes avg
    update_datetime TEXT,          -- 20 bytes
    load_count INTEGER             -- 8 bytes
);
-- Total per row: ~76 bytes (data)
-- Plus SQLite overhead: ~100-150 bytes per row

Page size: 4KB = 4096 bytes
Rows per page: 4096 / 150 = ~27 rows per page

With default wal_autocheckpoint=1000:
  1000 pages √ó 27 rows/page = ~27,000 word records

Your current database: ~1000 words
WAL capacity: 27x your entire database ‚úÖ More than enough!

Example scenario:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
You add 50 words:        50 rows = ~2 pages
You update 100 words:    100 rows = ~4 pages
You mark 200 learned:    200 rows = ~8 pages

Total: 14 pages in WAL (out of 1000 capacity)
Checkpoint will NOT trigger yet (only 1.4% full)

You would need to modify ~27,000 words to trigger checkpoint!
```

#### WAL Growth Patterns

**‰∏çÂêåÊìç‰ΩúÂØπ WAL Â§ßÂ∞èÁöÑÂΩ±Âìç**

```
Operation patterns and WAL growth:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

1. INSERT (Êñ∞Â¢û)
   Each INSERT: 1 page per ~27 rows

   Example: INSERT 1000 new words
   WAL size: 1000 / 27 = ~37 pages (~150KB)

2. UPDATE (Êõ¥Êñ∞)
   Each UPDATE: 1 page per modified row

   Example: UPDATE 1000 existing words
   WAL size: ~37 pages (~150KB)

3. DELETE (Âà†Èô§)
   Each DELETE: 1 page per deleted row

   Example: DELETE 1000 words
   WAL size: ~37 pages (~150KB)

4. Transaction size impact:
   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   Small transactions (< 100 rows):
     WAL grows slowly, checkpoint rarely needed

   Large transactions (> 10,000 rows):
     WAL grows fast, checkpoint frequently

   Your typical usage (add 10-50 words/day):
     WAL: < 2 pages/day
     Checkpoint: Every few months at current pace
```

#### When WAL Gets Too Large

**WAL ËøáÂ§ßÁöÑÂΩ±ÂìçÂíåÂ§ÑÁêÜ**

```
Performance degradation with large WAL:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

WAL Size        | Read Performance | Checkpoint Time | Status
----------------|------------------|-----------------|--------
< 10MB          | Excellent        | < 1 second      | ‚úÖ Optimal
10-50MB         | Good             | 1-5 seconds     | ‚úÖ OK
50-100MB        | Acceptable       | 5-10 seconds    | ‚ö†Ô∏è Consider checkpoint
100-500MB       | Slow             | 10-60 seconds   | ‚ö†Ô∏è Should checkpoint
> 500MB         | Very slow        | > 60 seconds    | ‚ùå Must checkpoint

Why performance degrades:
1. Larger WAL index to search
2. More frames to check per read
3. Checkpoint takes longer (blocks some operations)

Solution: Adjust wal_autocheckpoint to trigger earlier
```

#### Monitoring WAL Size

**Â¶Ç‰ΩïÊ£ÄÊü•ÂíåÁõëÊéß WAL Â§ßÂ∞è**

```sql
-- Check WAL file size in pages
PRAGMA wal_checkpoint(PASSIVE);
-- Returns: (busy, log_pages, checkpointed_pages)

-- Example output:
-- 0|237|237
-- Meaning: 237 pages in WAL, all checkpointed
-- WAL size: 237 √ó 4KB = 948KB

-- Get detailed WAL stats
SELECT
    page_count,
    page_size,
    page_count * page_size / 1024 / 1024 as wal_size_mb
FROM pragma_page_count('wal');
```

```go
// Monitor WAL size in Go
func monitorWALSize(db *sql.DB) {
    var busy, logPages, checkpointedPages int
    err := db.QueryRow("PRAGMA wal_checkpoint(PASSIVE)").Scan(
        &busy, &logPages, &checkpointedPages,
    )

    walSizeMB := float64(logPages * 4096) / 1024 / 1024

    log.Printf("WAL: %.2f MB (%d pages)", walSizeMB, logPages)

    if walSizeMB > 10 {
        log.Warn("WAL size exceeds 10MB, consider checkpoint")
        db.Exec("PRAGMA wal_checkpoint(RESTART)")
    }
}
```

```bash
# Check WAL size on disk
$ ls -lh enx.db-wal
-rw-r--r-- 1 user staff 2.3M Nov 12 10:00 enx.db-wal

# Calculate pages (assuming 4KB page size)
$ echo "scale=2; $(stat -f%z enx.db-wal) / 4096" | bc
573.00  # 573 pages in WAL
```

#### Configuring WAL Limits

**Ë∞ÉÊï¥ WAL ÈòàÂÄº**

```sql
-- Small WAL (checkpoint frequently)
PRAGMA wal_autocheckpoint=500;    -- 2MB

-- Default (balanced)
PRAGMA wal_autocheckpoint=1000;   -- 4MB

-- Large WAL (checkpoint less frequently, better write performance)
PRAGMA wal_autocheckpoint=5000;   -- 20MB

-- Disable auto-checkpoint (manual control)
PRAGMA wal_autocheckpoint=0;      -- Unlimited

-- Important: This setting persists in the database!
-- Set once, applies to all future connections
```

#### Best Practices for Your Project

**ÈíàÂØπ ENX È°πÁõÆÁöÑÂª∫ËÆÆ**

```
Your situation:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Database size:      ~500KB (1000 words)
Daily changes:      10-50 words
Change frequency:   Low (human usage)
WAL capacity:       4MB (27,000 words)

Recommendation:
  PRAGMA wal_autocheckpoint=1000;  ‚úÖ Default is perfect!

Reasons:
1. 4MB WAL = 50x your typical daily changes
2. Auto-checkpoint rarely triggers (low frequency)
3. When it does, < 1 second (not noticeable)
4. No manual management needed

Alternative scenarios:

If you do bulk imports (1000+ words at once):
  PRAGMA wal_autocheckpoint=2000;  // 8MB, more headroom

If you want checkpoint more often:
  PRAGMA wal_autocheckpoint=500;   // 2MB, checkpoint sooner

If using on low-storage device:
  PRAGMA wal_autocheckpoint=250;   // 1MB, save disk space
```

### Summary: WAL Size Limits

| Metric | Value |
|--------|-------|
| **Theoretical max** | Unlimited (disk space) |
| **Default threshold** | 1000 pages (4MB) |
| **Recommended max** | 10-50MB (performance) |
| **Your project needs** | 4MB (default) is perfect |
| **Typical WAL size** | < 1MB (with 1000 words) |
| **Capacity** | ~27,000 words before checkpoint |

### WAL File Lifecycle (enx.db-wal)

#### When Data is Written to WAL

**ÊØèÊ¨°ÂÜôÂÖ•Êìç‰ΩúÈÉΩ‰ºöÂÜôÂÖ• WAL**Ôºö

```
Timeline of WAL writes:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
10:00:00 - db.Exec("INSERT INTO words VALUES ('hello', '‰Ω†Â•Ω')")
           ‚Üí enx.db-wal: Append [Frame 1: INSERT hello]
           ‚Üí enx.db: NOT modified yet

10:00:01 - db.Exec("UPDATE user_dicts SET learned = 1 WHERE word_id = 5")
           ‚Üí enx.db-wal: Append [Frame 2: UPDATE user_dicts]
           ‚Üí enx.db: Still NOT modified

10:00:02 - tx.Commit()
           ‚Üí enx.db-wal: Append [Commit marker]
           ‚Üí enx.db: STILL NOT modified (changes only in WAL)
           ‚Üí Transaction is DURABLE (even though not in main DB)

10:00:03 - db.Exec("INSERT INTO words VALUES ('world', '‰∏ñÁïå')")
           ‚Üí enx.db-wal: Append [Frame 3: INSERT world]

Current state:
enx.db-wal: [Frame 1, Frame 2, Commit, Frame 3] (growing)
enx.db:     Old state (no new changes yet)
```

#### When WAL is Checkpointed (Merged to Main DB)

**‚úÖ Correct understanding: WAL is periodically reclaimed by SQLite**

```
Checkpoint triggers (WAL ‚Üí enx.db):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

1. Auto-checkpoint (most common)
   Trigger: WAL reaches 1000 pages (~4MB by default)

   10:05:00 - WAL size: 900 pages
   10:05:30 - WAL size: 1000 pages (threshold reached)
              ‚Üí SQLite: AUTO CHECKPOINT
              ‚Üí Copy frames from enx.db-wal to enx.db
              ‚Üí enx.db-wal: Reset to beginning (reused)
              ‚Üí File still exists, but content overwritten

2. Manual checkpoint
   db.Exec("PRAGMA wal_checkpoint(PASSIVE)")

3. Database close
   db.Close()
   ‚Üí Final checkpoint before closing

4. Read after long time
   If no recent checkpoint and readers need consistent view
```

#### WAL Content Management

**ÈáçË¶ÅÔºöWAL Êñá‰ª∂‰∏çÊòØ"Âà†Èô§"ÔºåËÄåÊòØ"ÈáçÁî®"**

```
WAL file behavior:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Phase 1: WAL file created (first write after enabling WAL)
  $ ls -lh enx.db*
  enx.db          500K
  enx.db-wal        0K  ‚Üê Created but empty
  enx.db-shm       32K

Phase 2: Writes accumulate in WAL
  10:00 - INSERT hello     ‚Üí enx.db-wal: 4K
  10:01 - INSERT world     ‚Üí enx.db-wal: 8K
  10:02 - INSERT goodbye   ‚Üí enx.db-wal: 12K
  ...
  10:30 - Many inserts     ‚Üí enx.db-wal: 4MB (1000 pages)

Phase 3: Auto-checkpoint triggers
  10:30:01 - Checkpoint starts
             1. Copy 4MB from enx.db-wal to enx.db
             2. Mark frames as "checkpointed" in WAL
             3. Reset WAL write position to beginning

  $ ls -lh enx.db*
  enx.db          504K  ‚Üê Grew by 4MB of data
  enx.db-wal      4MB   ‚Üê File still exists (not deleted)

Phase 4: WAL reused (not deleted)
  10:31 - INSERT new       ‚Üí enx.db-wal: Overwrites from beginning
  10:32 - INSERT another   ‚Üí enx.db-wal: Continues overwriting

  File size may stay same, but content is reused
```

#### Detailed Checkpoint Behavior

```
PASSIVE Checkpoint (default auto-checkpoint):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Before:
  enx.db-wal: [1000 frames, 4MB]
  Active readers: 2 connections reading frames 1-500

Checkpoint process:
  1. Check for active readers on each frame
  2. Frame 1-500:   Skip (readers still using) ‚úã
  3. Frame 501-1000: Copy to enx.db ‚úÖ
  4. Reset write position to frame 501

After:
  enx.db-wal: [Frames 1-500 still present, new writes at 501]
  File not truncated, partially checkpointed

Next writes:
  New frames written starting at position 501
  WAL grows: 501, 502, 503...

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
FULL Checkpoint:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Before:
  enx.db-wal: [1000 frames, 4MB]
  Active readers: 2 connections

Checkpoint process:
  1. WAIT for all readers to finish ‚è≥
  2. Once no readers, copy ALL frames to enx.db ‚úÖ
  3. Reset write position to 0

After:
  enx.db-wal: [File size 4MB, but all frames checkpointed]
  Write position at 0, ready for reuse

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TRUNCATE Checkpoint:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Before:
  enx.db-wal: [1000 frames, 4MB]

Checkpoint process:
  1. WAIT for all readers to finish
  2. Copy ALL frames to enx.db
  3. ftruncate(wal_fd, 0) ‚Üí Physically shrink file to 0 bytes ‚úÇÔ∏è

After:
  enx.db-wal: [File size 0 bytes] ‚Üê Actually deleted content

This is the ONLY mode that "deletes" WAL content
```

#### When WAL File is Actually Deleted

```
WAL file deletion scenarios:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

‚ùå NOT deleted on checkpoint:
   PASSIVE/FULL/RESTART: File remains, content reused

‚úÖ Deleted when:
   1. Switch journal mode:
      PRAGMA journal_mode=DELETE;  ‚Üí enx.db-wal deleted

   2. Last connection closes + no writes:
      All connections: db.Close()
      If WAL is empty ‚Üí enx.db-wal may be removed

   3. Manual deletion (dangerous!):
      rm enx.db-wal  ‚Üê DON'T do this while DB is open!
```

#### Practical Example

```go
// Real-world WAL lifecycle
func demonstrateWALLifecycle() {
    db, _ := sql.Open("sqlite3", "enx.db")
    db.Exec("PRAGMA journal_mode=WAL")
    db.Exec("PRAGMA wal_autocheckpoint=1000")

    // Phase 1: Initial writes
    for i := 0; i < 500; i++ {
        db.Exec("INSERT INTO words ...")
        // WAL size: ~2MB
    }
    fmt.Println("WAL size: ~2MB, NOT checkpointed yet")

    // Phase 2: Trigger auto-checkpoint
    for i := 0; i < 600; i++ {
        db.Exec("INSERT INTO words ...")
        // After insert 500: WAL reaches 1000 pages ‚Üí AUTO CHECKPOINT
    }
    fmt.Println("WAL auto-checkpointed, content merged to enx.db")
    fmt.Println("WAL file still exists, but write position reset")

    // Phase 3: More writes (reuses WAL)
    for i := 0; i < 100; i++ {
        db.Exec("INSERT INTO words ...")
        // Writes to WAL starting from position 0 (reused)
    }
    fmt.Println("WAL reused, size: ~400KB")

    // Phase 4: Manual checkpoint with truncate
    db.Exec("PRAGMA wal_checkpoint(TRUNCATE)")
    fmt.Println("WAL truncated to 0 bytes")

    // Phase 5: Close database
    db.Close()
    // WAL file may be deleted if empty, or kept for next open
}
```

#### Summary: WAL File Lifecycle

| Stage | enx.db-wal State | Action |
|-------|------------------|--------|
| **1. First write** | Created, empty | File created |
| **2. Accumulating writes** | Growing (0 ‚Üí 4MB) | Append frames |
| **3. Reach threshold** | 4MB (1000 pages) | Auto-checkpoint triggered |
| **4. After checkpoint** | File exists, reused | Content overwritten from start |
| **5. More writes** | Growing again | Reuses same file |
| **6. Database close** | May be kept/deleted | Depends on content |

**Key Points**:

‚úÖ **ÂÜôÂÖ•Êó∂Êú∫**: ÊØèÊ¨° INSERT/UPDATE/DELETE ÈÉΩÁ´ãÂç≥ÂÜôÂÖ• WAL
‚úÖ **ÂõûÊî∂Êó∂Êú∫**: WAL ËææÂà∞ÈòàÂÄºÊó∂Ëá™Âä® checkpointÔºàÂÆöÊúüÂõûÊî∂Ôºâ
‚úÖ **Êñá‰ª∂Â§ÑÁêÜ**: WAL Êñá‰ª∂‰∏çÂà†Èô§ÔºåËÄåÊòØ**ÈáçÁî®**ÔºàË¶ÜÁõñÂÜôÔºâ
‚úÖ **ÁúüÊ≠£Âà†Èô§**: Âè™Êúâ TRUNCATE checkpoint ÊàñÂàáÊç¢ journal mode ÊâçÁâ©ÁêÜÂà†Èô§ÂÜÖÂÆπ

### Configuration Options

```sql
-- Enable WAL mode (one-time, persists in database)
PRAGMA journal_mode=WAL;

-- Synchronous mode (durability vs performance)
PRAGMA synchronous=FULL;      -- Maximum durability, slower
PRAGMA synchronous=NORMAL;    -- Good balance (recommended)
PRAGMA synchronous=OFF;       -- Fastest, risk data loss

-- Busy timeout (wait for locks)
PRAGMA busy_timeout=5000;     -- Wait 5 seconds

-- Page size (affects performance)
PRAGMA page_size=4096;        -- Default, good for most cases

-- Cache size (memory for pages)
PRAGMA cache_size=-2000;      -- 2MB cache
```

### Performance Characteristics

| Operation | Traditional | WAL | Improvement |
|-----------|-------------|-----|-------------|
| **Small writes** | 10-20/sec | 50-100/sec | **5x faster** |
| **Large writes** | 100 MB/sec | 150 MB/sec | **1.5x faster** |
| **Concurrent reads** | Blocked | Never blocked | **‚àû** |
| **Read latency** | Low | Slightly higher* | 5-10% slower |
| **Storage** | 1x | 1.3x** | +30% during peak |

*Readers must check both database and WAL
**WAL file size before checkpoint

### Limitations and Considerations

#### ‚ùå 1. Not Recommended for Network File Systems

```
Problem: WAL requires POSIX advisory locking
         Network file systems (NFS, SMB) may not support properly

Solution: Use local disk for database files
          (Your P2P design already does this ‚úÖ)
```

#### ‚ùå 2. All Connections Must Use WAL

```
Problem: If one connection uses WAL, all must use WAL
         Mixed mode not supported

Solution: Enable WAL once, all connections inherit
          (Not an issue in your design ‚úÖ)
```

#### ‚ö†Ô∏è 3. Checkpoint Blocking

```
Scenario: Large WAL file + many readers
          Checkpoint must wait for all readers to finish

Mitigation: Use PASSIVE checkpoint (doesn't block)
            Acceptable WAL size (a few MB is fine)
```

#### ‚ö†Ô∏è 4. Read Performance

```
Readers must check: enx.db + enx.db-wal
                    Slightly slower than single file

Impact: ~5-10% slower reads (negligible for your use case)
```

### Best Practices

#### 1. **Always Enable WAL for Modern Apps**

```go
db, _ := sql.Open("sqlite3", "enx.db")
db.Exec("PRAGMA journal_mode=WAL")
db.Exec("PRAGMA synchronous=NORMAL")
db.Exec("PRAGMA busy_timeout=5000")
```

#### 2. **Monitor WAL Size**

```go
func checkWALSize(db *sql.DB) {
    var walPages int
    db.QueryRow("PRAGMA wal_checkpoint(PASSIVE)").Scan(&walPages)

    if walPages > 10000 {  // > 40MB
        log.Warn("WAL file too large, forcing checkpoint")
        db.Exec("PRAGMA wal_checkpoint(RESTART)")
    }
}
```

#### 3. **Backup Strategy**

```bash
# ‚ùå Wrong: Copy database while WAL active
cp enx.db backup.db  # Missing changes in WAL!

# ‚úÖ Correct: Checkpoint first
sqlite3 enx.db "PRAGMA wal_checkpoint(TRUNCATE)"
cp enx.db backup.db
cp enx.db-wal backup.db-wal  # Optional, usually empty after TRUNCATE
```

#### 4. **Checkpoint Timing**

```go
// Checkpoint during low activity
ticker := time.NewTicker(5 * time.Minute)
go func() {
    for range ticker.C {
        if isLowActivity() {
            db.Exec("PRAGMA wal_checkpoint(PASSIVE)")
        }
    }
}()
```

### WAL Integration for P2P Sync

### ‚ö†Ô∏è CRITICAL: Why NOT Use WAL File Directly for Sync

**‚ùå WAL Êñá‰ª∂Êú¨Ë∫´‰∏çÈÄÇÂêà‰Ωú‰∏∫ÂêåÊ≠•Êú∫Âà∂**

Before discussing how to **leverage** WAL mode for performance, it's crucial to understand why you should **NOT** use the WAL file itself as a sync mechanism:

#### Problem 1: Unpredictable Checkpoint Timing

**WAL ÂêàÂπ∂Êó∂Êú∫‰∏çÂèØÊéß**

```
WAL checkpoint can happen at ANY time:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚úÖ Automatic checkpoints:
   - Every 1000 pages (4MB) by default
   - On last connection close
   - On transaction commit (if threshold reached)
   - On system events (memory pressure, etc.)

‚ùå You cannot reliably predict WHEN checkpoint happens:

Timeline:
T0:   Write change A ‚Üí WAL contains [A]
T1:   Write change B ‚Üí WAL contains [A, B]
T2:   üí• Auto-checkpoint! ‚Üí WAL cleared, changes merged to DB
T3:   Try to sync WAL ‚Üí ‚ùå Changes A, B already gone!

Host A: Writes 10 changes ‚Üí Auto-checkpoint ‚Üí WAL cleared
Host B: Tries to read WAL from cloud ‚Üí ‚ùå Missing changes 1-10
Host C: Has different checkpoint timing ‚Üí Inconsistent state
```

**Example of the Problem**

```go
// Host A (Desktop)
for i := 0; i < 100; i++ {
    db.Exec("INSERT INTO words VALUES (...)")
}
// After ~1000 pages ‚Üí Auto-checkpoint triggered
// WAL file cleared, changes merged to enx.db

// Host B (MacBook) tries to sync
file := downloadFromCloud("enx.db-wal")
// ‚ùå WAL file is empty or has different content!
// ‚ùå Missing changes from Host A
// ‚ùå Sync fails or gets partial data
```

#### Problem 2: No Historical Record

**WAL Êñá‰ª∂‰∏ç‰øùÁïôÂéÜÂè≤**

```
WAL is a TEMPORARY buffer, not a change log:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Traditional change log (what you need for sync):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Change 1: 2025-11-01 10:00 - Insert word "hello"      ‚îÇ
‚îÇ Change 2: 2025-11-01 10:05 - Update word "world"      ‚îÇ
‚îÇ Change 3: 2025-11-02 14:30 - Insert word "goodbye"    ‚îÇ
‚îÇ ...                                                    ‚îÇ
‚îÇ Change 100: 2025-11-10 09:15 - Delete word "test"     ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ ‚úÖ All changes preserved                                ‚îÇ
‚îÇ ‚úÖ Can replay any subset                                ‚îÇ
‚îÇ ‚úÖ Can sync incrementally                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

WAL file (NOT a change log):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Page 1: [current data]                                 ‚îÇ
‚îÇ Page 2: [current data]                                 ‚îÇ
‚îÇ Page 3: [current data]                                 ‚îÇ
‚îÇ ...                                                     ‚îÇ
‚îÇ üí• Checkpoint ‚Üí All cleared                             ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ ‚ùå No historical changes                                ‚îÇ
‚îÇ ‚ùå Cannot replay past changes                           ‚îÇ
‚îÇ ‚ùå Cannot do incremental sync                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Problem:
- WAL only contains UNCOMMITTED or RECENT changes
- After checkpoint, old changes are GONE FOREVER
- No way to retrieve "changes since last sync"
```

#### Problem 3: No Access API

**Êó†Ê≥ïÂèØÈù†ËØªÂèñ WAL ÂÜÖÂÆπ**

```
SQLite does NOT provide API to read WAL content:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚ùå No API to:
   - List changes in WAL
   - Extract change records
   - Replay WAL to another database
   - Query WAL content

‚ùå WAL file format is internal:
   - Binary format (not human-readable)
   - Page-level data (not record-level)
   - No guarantees across SQLite versions
   - Not designed for external consumption

‚ùå Even if you parse WAL manually:
   - Complex binary format
   - Requires deep SQLite internals knowledge
   - Breaks on SQLite updates
   - No official documentation
```

#### Problem 4: Offline Sync Impossible

**Á¶ªÁ∫øÂêåÊ≠•Êó†Ê≥ïÂÆûÁé∞**

```
Your P2P scenario requires offline capability:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Reality with WAL-based sync:

Host A (Desktop):
  Day 1: Add 50 words ‚Üí WAL grows ‚Üí Auto-checkpoint ‚Üí WAL cleared
  Day 2: Add 30 words ‚Üí WAL grows ‚Üí Auto-checkpoint ‚Üí WAL cleared
  Day 3: Upload WAL to cloud ‚Üí ‚ùå Only contains Day 3 changes!
                              ‚Üí ‚ùå Day 1-2 changes lost!

Host B (MacBook, offline for 2 days):
  Day 1-2: No network, can't sync
  Day 3: Come online, download WAL ‚Üí ‚ùå Missing 50 + 30 words from Day 1-2

Host C (Ubuntu, isolated network):
  Week 1: Work offline entirely
  Week 2: Connect to sync ‚Üí ‚ùå WAL checkpointed many times
                           ‚Üí ‚ùå All changes lost except recent ones
```

#### Problem 5: Conflict Resolution Impossible

**Êó†Ê≥ïËß£ÂÜ≥ÂÜ≤Á™Å**

```
Conflict resolution requires change tracking:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

What you need:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Host A: Update word "hello" at 2025-11-01 10:00       ‚îÇ
‚îÇ Host B: Update word "hello" at 2025-11-01 10:05       ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ Conflict: Same word updated on both hosts              ‚îÇ
‚îÇ Resolution: Keep newer timestamp (Host B)              ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ ‚úÖ Requires: Timestamp metadata for each change        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

What WAL provides:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Page 42: [raw binary data for word "hello"]           ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ ‚ùå No timestamp                                         ‚îÇ
‚îÇ ‚ùå No change metadata                                   ‚îÇ
‚îÇ ‚ùå Cannot determine which change is newer              ‚îÇ
‚îÇ ‚ùå Cannot resolve conflicts                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### The Right Approach: Timestamp-Based Sync

**‚úÖ Correct approach: Timestamp fields + WAL mode**

```
Separate concerns:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

1. WAL Mode ‚Üí For performance (fast writes, no blocking)
   ‚îú‚îÄ Enable: PRAGMA journal_mode=WAL
   ‚îú‚îÄ Benefit: 5x faster writes, concurrent reads
   ‚îî‚îÄ Purpose: Improve app performance

2. Timestamp Fields ‚Üí For sync mechanism
   ‚îú‚îÄ Schema: update_datetime, update_time in every table
   ‚îú‚îÄ Benefit: Reliable change tracking, conflict resolution
   ‚îî‚îÄ Purpose: Track changes for P2P sync

Best of both worlds:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚úÖ WAL enabled ‚Üí Fast, concurrent operations           ‚îÇ
‚îÇ ‚úÖ Timestamp tracking ‚Üí Reliable sync mechanism        ‚îÇ
‚îÇ ‚úÖ No dependency on WAL file ‚Üí Predictable behavior    ‚îÇ
‚îÇ ‚úÖ Offline support ‚Üí Changes tracked even when offline ‚îÇ
‚îÇ ‚úÖ Conflict resolution ‚Üí Timestamp comparison          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Comparison: WAL-Based vs Timestamp-Based Sync

| Aspect | WAL File as Sync | Timestamp Fields | Winner |
|--------|------------------|------------------|---------|
| **Checkpoint timing** | ‚ùå Unpredictable, data loss | ‚úÖ Always preserved | ‚úÖ Timestamp |
| **Historical record** | ‚ùå Cleared after checkpoint | ‚úÖ Permanent record | ‚úÖ Timestamp |
| **Offline support** | ‚ùå Loses old changes | ‚úÖ All changes tracked | ‚úÖ Timestamp |
| **Conflict resolution** | ‚ùå No metadata | ‚úÖ Timestamp comparison | ‚úÖ Timestamp |
| **API availability** | ‚ùå No public API | ‚úÖ Standard SQL | ‚úÖ Timestamp |
| **Reliability** | ‚ùå Fragile, timing-dependent | ‚úÖ Solid, predictable | ‚úÖ Timestamp |
| **Performance boost** | N/A | ‚úÖ WAL mode enabled | ‚úÖ Both |
| **Implementation** | ‚ùå Complex, hacky | ‚úÖ Simple, standard | ‚úÖ Timestamp |

#### Summary: Use WAL Mode, Don't Sync WAL File

```
The winning strategy:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚úÖ DO:
   - Enable WAL mode for performance
   - Use timestamp fields for sync mechanism
   - Sync main DB file (enx.db) to cloud
   - Compare timestamps to detect changes
   - Merge changes based on timestamps

‚ùå DON'T:
   - Try to sync WAL file
   - Depend on WAL file for change tracking
   - Parse WAL file manually
   - Use WAL as a "change log"
   - Expect WAL file to contain historical changes

Key insight:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
WAL is a PERFORMANCE feature, not a SYNC mechanism.
Use it for speed, not for tracking changes.
```

---

### Why Use WAL Mode?

SQLite's Write-Ahead Logging (WAL) mode is **highly beneficial** for this P2P sync architecture:

#### 1. Concurrent Read-Write Operations

**Traditional Mode (DELETE journal)**:
```
Sync process reading changes ‚Üí ‚ùå BLOCKS user operations
User writing new word       ‚Üí ‚ùå BLOCKS sync reading
Result: Poor performance, frequent locks
```

**WAL Mode**:
```
Sync process reading changes ‚Üí ‚úÖ Continues
User writing new word       ‚Üí ‚úÖ Continues simultaneously
Background sync             ‚Üí ‚úÖ No interruption to users
Result: Smooth operation, no blocking
```

#### 2. Atomic Batch Sync

WAL ensures atomic commits for sync operations:

```go
// Sync a batch of 100 changes atomically
tx, err := db.Begin()
defer tx.Rollback()

for _, change := range changes {
    // Apply change
    if err := applyChange(tx, change); err != nil {
        // ‚úÖ Entire batch rolls back automatically
        // No partial sync, no inconsistent state
        return err
    }
}

tx.Commit()  // ‚úÖ All 100 changes commit atomically
```

#### 3. Performance Benefits

| Operation | Traditional Mode | WAL Mode | Improvement |
|-----------|------------------|----------|-------------|
| Small writes | ~10 writes/sec | ~50 writes/sec | **5x faster** |
| Concurrent reads | Blocked during write | Never blocked | **‚àû improvement** |
| Sync throughput | 100 records/sec | 500 records/sec | **5x faster** |
| Checkpoint | N/A | Auto background | No manual work |

### Implementation

#### 1. Enable WAL Mode

```go
// enx-data-service database initialization
func InitDatabase(dbPath string) (*sql.DB, error) {
    db, err := sql.Open("sqlite3", dbPath)
    if err != nil {
        return nil, err
    }

    // ‚úÖ Enable WAL mode
    _, err = db.Exec("PRAGMA journal_mode=WAL")
    if err != nil {
        return nil, fmt.Errorf("failed to enable WAL: %w", err)
    }

    // ‚úÖ Configure WAL checkpoint interval
    // Auto-checkpoint when WAL reaches 1000 pages (~4MB)
    _, err = db.Exec("PRAGMA wal_autocheckpoint=1000")
    if err != nil {
        return nil, err
    }

    // ‚úÖ Set synchronous mode for durability/performance balance
    // NORMAL mode: Fast, safe for most use cases
    // FULL mode: Slower but maximum durability
    _, err = db.Exec("PRAGMA synchronous=NORMAL")
    if err != nil {
        return nil, err
    }

    // ‚úÖ Set busy timeout for better concurrency handling
    _, err = db.Exec("PRAGMA busy_timeout=5000")  // 5 seconds
    if err != nil {
        return nil, err
    }

    log.Info("SQLite WAL mode enabled successfully")
    return db, nil
}
```

#### 2. WAL-Aware Sync Algorithm

```go
// Leverage WAL for efficient sync operations
func (s *SyncService) ApplyChanges(changes []Change) error {
    // ‚úÖ Start transaction (uses WAL)
    tx, err := s.db.Begin()
    if err != nil {
        return err
    }
    defer tx.Rollback()

    // ‚úÖ Batch apply changes
    for _, change := range changes {
        switch change.Action {
        case "insert":
            if err := s.insertRecord(tx, change); err != nil {
                // WAL ensures entire batch rolls back
                return err
            }
        case "update":
            if err := s.updateRecord(tx, change); err != nil {
                return err
            }
        case "delete":
            if err := s.deleteRecord(tx, change); err != nil {
                return err
            }
        }
    }

    // ‚úÖ Atomic commit (all or nothing)
    if err := tx.Commit(); err != nil {
        return err
    }

    log.Infof("Applied %d changes atomically", len(changes))
    return nil
}
```

#### 3. WAL Checkpoint Management

```go
// Manual checkpoint control (optional, usually automatic)
func (s *SyncService) CheckpointWAL() error {
    // Checkpoint types:
    // PASSIVE: Don't block, checkpoint what's possible
    // FULL: Wait for readers to finish, checkpoint everything
    // RESTART: FULL + start new WAL file
    // TRUNCATE: RESTART + truncate old WAL file to 0 bytes

    _, err := s.db.Exec("PRAGMA wal_checkpoint(PASSIVE)")
    if err != nil {
        return fmt.Errorf("WAL checkpoint failed: %w", err)
    }

    log.Info("WAL checkpoint completed")
    return nil
}

// Periodic checkpoint (runs in background)
func (s *SyncService) StartWALCheckpointWorker() {
    ticker := time.NewTicker(5 * time.Minute)
    defer ticker.Stop()

    for range ticker.C {
        if err := s.CheckpointWAL(); err != nil {
            log.Warnf("Background checkpoint failed: %v", err)
        }
    }
}
```

### WAL File Management

#### File Structure

```
/data/
‚îú‚îÄ‚îÄ enx.db           # Main database file
‚îú‚îÄ‚îÄ enx.db-wal       # Write-Ahead Log (transactions)
‚îî‚îÄ‚îÄ enx.db-shm       # Shared memory (coordination)
```

#### Important Considerations

**1. Backup Strategy**:
```bash
# ‚ùå Wrong: Copy only enx.db
cp enx.db enx-backup.db  # Missing uncommitted changes in WAL!

# ‚úÖ Correct: Checkpoint first, then copy
sqlite3 enx.db "PRAGMA wal_checkpoint(TRUNCATE)"
cp enx.db enx-backup.db
```

**2. Sync Strategy (for P2P)**:
```go
// Option A: Sync database file + WAL file together
func (s *SyncService) PrepareForSync() error {
    // Checkpoint to merge WAL into main database
    _, err := s.db.Exec("PRAGMA wal_checkpoint(RESTART)")
    return err
}

// Option B: Use enx-data-service API (recommended)
// Don't sync files directly, sync through gRPC API
// enx-data-service handles WAL internally
```

**3. Multi-Process Access**:
```
‚úÖ Safe: Multiple read-only processes
‚úÖ Safe: One writer + multiple readers
‚ùå Unsafe: Multiple writers (not needed in P2P design)
```

### Integration with P2P Sync

#### Sync Process with WAL

```
Step 1: Node A requests changes from Node B
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Node A ‚îÇ‚îÄ‚îÄ‚îÄ GetChanges(since) ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ Node B ‚îÇ
‚îÇ        ‚îÇ                            ‚îÇ        ‚îÇ
‚îÇ        ‚îÇ                            ‚îÇ (WAL)  ‚îÇ
‚îÇ        ‚îÇ                            ‚îÇ Read   ‚îÇ
‚îÇ        ‚îÇ                            ‚îÇ changes‚îÇ
‚îÇ        ‚îÇ‚Üê‚îÄ‚îÄ‚îÄ Stream: 100 changes ‚îÄ‚îÄ‚îÇ ‚úÖ No  ‚îÇ
‚îÇ        ‚îÇ                            ‚îÇ block  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Step 2: Node A applies changes locally
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Node A ‚îÇ
‚îÇ        ‚îÇ tx.Begin()              ‚Üê WAL transaction
‚îÇ        ‚îÇ Apply 100 changes       ‚Üê In-memory
‚îÇ        ‚îÇ tx.Commit()             ‚Üê WAL atomic write
‚îÇ        ‚îÇ ‚úÖ Success or rollback
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Step 3: Background checkpoint (automatic)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Node A ‚îÇ
‚îÇ        ‚îÇ [5 minutes later]
‚îÇ        ‚îÇ Checkpoint WAL          ‚Üê Merge to main DB
‚îÇ        ‚îÇ ‚úÖ No interruption to users
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Why WAL is Perfect for P2P Sync

1. **Non-blocking sync**: Sync operations don't block user operations
2. **Atomic batches**: Entire sync batch commits or rolls back together
3. **Better performance**: Faster writes, no lock contention
4. **Crash recovery**: WAL provides automatic recovery
5. **No manual intervention**: Auto-checkpoint handles WAL size

### Configuration Best Practices

```yaml
# Environment variables for enx-data-service
environment:
  # Database
  - DB_PATH=/data/enx.db
  - DB_WAL_MODE=true                    # Enable WAL
  - DB_WAL_AUTOCHECKPOINT=1000          # Pages before auto-checkpoint
  - DB_SYNCHRONOUS=NORMAL               # Balance durability/performance
  - DB_BUSY_TIMEOUT=5000                # Wait 5s on busy

  # Sync
  - SYNC_INTERVAL=300                   # 5 minutes
  - SYNC_BATCH_SIZE=100                 # Records per transaction

  # Checkpoint
  - WAL_CHECKPOINT_INTERVAL=300         # 5 minutes
  - WAL_CHECKPOINT_TYPE=PASSIVE         # Non-blocking
```

### Monitoring WAL Health

```go
// Check WAL statistics
func (s *SyncService) GetWALStats() (*WALStats, error) {
    var stats WALStats

    // Get WAL file size
    row := s.db.QueryRow(`
        SELECT
            page_count * page_size / 1024 / 1024 as wal_size_mb,
            (SELECT page_count FROM pragma_page_count()) as db_pages
        FROM pragma_wal_checkpoint('PASSIVE')
    `)

    err := row.Scan(&stats.WALSizeMB, &stats.DBPages)
    if err != nil {
        return nil, err
    }

    return &stats, nil
}

// Alert if WAL grows too large (indicates checkpoint issues)
func (s *SyncService) MonitorWALSize() {
    ticker := time.NewTicker(1 * time.Minute)
    defer ticker.Stop()

    for range ticker.C {
        stats, err := s.GetWALStats()
        if err != nil {
            log.Errorf("Failed to get WAL stats: %v", err)
            continue
        }

        // Alert if WAL exceeds 100MB (adjust based on workload)
        if stats.WALSizeMB > 100 {
            log.Warnf("WAL file too large: %.2f MB", stats.WALSizeMB)
            // Trigger manual checkpoint
            s.CheckpointWAL()
        }
    }
}
```

### Advantages Summary

| Feature | Benefit for P2P Sync |
|---------|----------------------|
| **Concurrent access** | Sync doesn't block user operations |
| **Atomic transactions** | Entire sync batch commits or fails |
| **Better performance** | 5x faster writes during sync |
| **Auto-checkpoint** | No manual WAL management needed |
| **Crash recovery** | Automatic recovery from interrupted sync |
| **Read consistency** | Readers see consistent snapshot |

### Potential Issues & Solutions

#### Issue 1: WAL File Growth

**Problem**: WAL file grows indefinitely if checkpoint fails
**Solution**:
```go
// Monitor and force checkpoint if needed
if walSize > 100*1024*1024 {  // 100MB
    db.Exec("PRAGMA wal_checkpoint(RESTART)")
}
```

#### Issue 2: NFS/Network Drives

**Problem**: WAL mode not recommended on NFS
**Solution**:
- Use local disk for each node's enx.db
- Sync via gRPC API, not file copying
- This design already does this ‚úÖ

#### Issue 3: Checkpoint Blocking

**Problem**: FULL/RESTART checkpoints wait for readers
**Solution**:
- Use PASSIVE checkpoint (default)
- Checkpoint during low-activity periods
- Acceptable WAL size is fine (auto-managed)

## SQLite Session Extension Alternative

### What is Session Extension?

SQLite Session Extension is an **official SQLite module** for capturing and replaying database changes. It's designed specifically for **replication and synchronization** scenarios.

**Official Documentation**: https://www.sqlite.org/sessionintro.html

### Core Concepts

#### 1. **Session Object**

Tracks all changes to specified tables during a session:

```c
sqlite3_session *pSession;

// Create session
sqlite3_session_create(db, "main", &pSession);

// Attach tables to track
sqlite3_session_attach(pSession, "words");
sqlite3_session_attach(pSession, "user_dicts");

// ... perform INSERT/UPDATE/DELETE operations ...

// Extract changeset
int nChangeset;
void *pChangeset;
sqlite3_session_changeset(pSession, &nChangeset, &pChangeset);
```

#### 2. **Changeset**

Binary format containing all changes:

```
Changeset contains:
- Table name
- Action (INSERT, UPDATE, DELETE)
- Old values (for UPDATE/DELETE)
- New values (for INSERT/UPDATE)
- Primary key information
```

#### 3. **Apply Changeset**

Replay changes on another database:

```c
// Apply changeset to another database
sqlite3_changeset_apply(
    db2,                    // Target database
    nChangeset,             // Changeset size
    pChangeset,             // Changeset data
    filter_callback,        // Optional filter
    conflict_callback,      // Conflict handler
    NULL                    // User data
);
```

### How It Works

```
Node A                          Node B
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. Create      ‚îÇ            ‚îÇ                 ‚îÇ
‚îÇ     Session     ‚îÇ            ‚îÇ                 ‚îÇ
‚îÇ                 ‚îÇ            ‚îÇ                 ‚îÇ
‚îÇ  2. Track       ‚îÇ            ‚îÇ                 ‚îÇ
‚îÇ     Changes     ‚îÇ            ‚îÇ                 ‚îÇ
‚îÇ     - INSERT    ‚îÇ            ‚îÇ                 ‚îÇ
‚îÇ     - UPDATE    ‚îÇ            ‚îÇ                 ‚îÇ
‚îÇ     - DELETE    ‚îÇ            ‚îÇ                 ‚îÇ
‚îÇ                 ‚îÇ            ‚îÇ                 ‚îÇ
‚îÇ  3. Generate    ‚îÇ            ‚îÇ                 ‚îÇ
‚îÇ     Changeset   ‚îÇ            ‚îÇ                 ‚îÇ
‚îÇ     (binary)    ‚îÇ            ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                              ‚îÇ
         ‚îÇ  4. Transfer Changeset       ‚îÇ
         ‚îÇ     (gRPC/HTTP/File)         ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ
                                        ‚îÇ
                                 5. Apply
                                    Changeset
                                        ‚îÇ
                                 6. Resolve
                                    Conflicts
```

### Go Implementation (with CGo)

#### Installation

```bash
# Install go-sqlite3 with session extension
go get github.com/mattn/go-sqlite3

# Need to enable CGO
export CGO_ENABLED=1
```

#### Code Example

```go
package main

import (
    "database/sql"
    "unsafe"

    "github.com/mattn/go-sqlite3"
)

/*
#cgo LDFLAGS: -lsqlite3
#include <sqlite3.h>
#include <stdlib.h>
*/
import "C"

type SessionManager struct {
    db      *sql.DB
    session *C.sqlite3_session
}

// Create session and attach tables
func NewSession(db *sql.DB) (*SessionManager, error) {
    // Get raw SQLite connection
    conn, err := db.Conn(context.Background())
    if err != nil {
        return nil, err
    }

    var session *C.sqlite3_session
    var sqliteDB *C.sqlite3

    // Extract native handle (using go-sqlite3 internal API)
    conn.Raw(func(driverConn interface{}) error {
        sqliteConn := driverConn.(*sqlite3.SQLiteConn)
        // ... extract native handle ...
        return nil
    })

    // Create session
    rc := C.sqlite3_session_create(sqliteDB, C.CString("main"), &session)
    if rc != C.SQLITE_OK {
        return nil, fmt.Errorf("failed to create session: %d", rc)
    }

    // Attach tables
    C.sqlite3_session_attach(session, C.CString("words"))
    C.sqlite3_session_attach(session, C.CString("user_dicts"))
    C.sqlite3_session_attach(session, C.CString("users"))

    return &SessionManager{db: db, session: session}, nil
}

// Capture changes
func (sm *SessionManager) GetChangeset() ([]byte, error) {
    var nChangeset C.int
    var pChangeset *C.void

    rc := C.sqlite3_session_changeset(sm.session, &nChangeset, &pChangeset)
    if rc != C.SQLITE_OK {
        return nil, fmt.Errorf("failed to get changeset: %d", rc)
    }
    defer C.sqlite3_free(pChangeset)

    // Convert to Go bytes
    changeset := C.GoBytes(pChangeset, nChangeset)
    return changeset, nil
}

// Apply changeset to database
func ApplyChangeset(db *sql.DB, changeset []byte) error {
    var sqliteDB *C.sqlite3
    // ... get native handle ...

    rc := C.sqlite3_changeset_apply(
        sqliteDB,
        C.int(len(changeset)),
        unsafe.Pointer(&changeset[0]),
        nil,  // filter
        (*[0]byte)(C.conflictCallback),  // conflict handler
        nil,  // user data
    )

    if rc != C.SQLITE_OK {
        return fmt.Errorf("failed to apply changeset: %d", rc)
    }

    return nil
}

// Conflict resolution callback
//export conflictCallback
func conflictCallback(pCtx unsafe.Pointer, eConflict C.int, pIter *C.sqlite3_changeset_iter) C.int {
    switch eConflict {
    case C.SQLITE_CHANGESET_DATA:
        // Data conflict: remote and local both modified
        // Strategy: Keep newer version based on timestamp
        return C.SQLITE_CHANGESET_REPLACE

    case C.SQLITE_CHANGESET_NOTFOUND:
        // Record not found: remote deleted, local modified
        return C.SQLITE_CHANGESET_OMIT

    case C.SQLITE_CHANGESET_CONFLICT:
        // Primary key conflict
        return C.SQLITE_CHANGESET_REPLACE

    default:
        return C.SQLITE_CHANGESET_ABORT
    }
}
```

### Integration with P2P Sync

#### Workflow

```go
// Node A: Capture changes
session, _ := NewSession(db)

// ... user performs operations ...
db.Exec("INSERT INTO words ...")
db.Exec("UPDATE user_dicts ...")

// Extract changeset
changeset, _ := session.GetChangeset()

// Send to Node B via gRPC
client.PushChangeset(context.Background(), &pb.Changeset{
    Data: changeset,
    Timestamp: time.Now().Format(time.RFC3339),
})

// Node B: Apply changeset
ApplyChangeset(db, changeset)
```

#### gRPC Service Definition

```protobuf
service SyncService {
  rpc PushChangeset(ChangesetRequest) returns (ChangesetResponse);
  rpc PullChangesets(PullRequest) returns (stream ChangesetRequest);
}

message ChangesetRequest {
  string node_id = 1;
  bytes data = 2;          // Binary changeset
  string timestamp = 3;
  int32 sequence = 4;      // Ordering
}

message ChangesetResponse {
  bool success = 1;
  string error = 2;
  int32 conflicts_resolved = 3;
}
```

### Advantages of Session Extension

| Feature | Benefit |
|---------|---------|
| **Official SQLite** | Maintained by SQLite team, stable API |
| **Efficient format** | Binary changeset, smaller than JSON |
| **Automatic tracking** | No manual change recording needed |
| **Conflict resolution** | Built-in callback mechanism |
| **Incremental** | Only captures changes, not full snapshot |
| **Type-safe** | Preserves data types correctly |

### Disadvantages

| Issue | Impact |
|-------|--------|
| **Requires CGo** | Complex build process, platform-dependent |
| **Session is in-memory** | Lost on process restart |
| **No persistence** | Need external storage for changesets |
| **Learning curve** | C API, not idiomatic Go |
| **No automatic replay** | Need to implement transport layer |

### Comparison: Session Extension vs Timestamp-based

#### Session Extension Approach

```go
‚úÖ Automatic change tracking
session.attach("words")  // Tracks all changes

‚úÖ Efficient binary format
changeset size: ~100 bytes per change

‚úÖ Built-in conflict resolution
Callback handles conflicts automatically

‚ùå Requires CGo
Complex build, platform issues

‚ùå Session doesn't persist
Restart = lose tracking state

‚ùå Need changeset storage
Must save changesets somewhere
```

#### Timestamp-based Approach (Current)

```go
‚úÖ Simple SQL queries
SELECT * FROM words WHERE update_datetime > ?

‚úÖ Pure Go
No CGo, cross-platform

‚úÖ Persistent tracking
Timestamps in database, survive restarts

‚úÖ Easy debugging
Direct SQL queries, human-readable

‚ùå Manual timestamp management
Need to add update_datetime to each table

‚ùå Larger transfer size
Full records vs binary diffs
```

### Critical Limitations for Your Use Case

#### ‚ö†Ô∏è Problem 1: Session Doesn't Survive Restarts

**Session Extension fatal flaw for development environments**:

```
Scenario: Development on Desktop
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
10:00 AM - Start enx-data-service
           session = sqlite3_session_create()  ‚Üê Session in memory

10:30 AM - Add 50 words
           session tracks changes              ‚Üê 50 changes in memory

11:00 AM - Reboot computer for kernel update
           ‚ùå Process killed
           ‚ùå Session destroyed
           ‚ùå 50 word changes LOST

11:30 AM - Start enx-data-service again
           session = sqlite3_session_create()  ‚Üê NEW empty session
           session has no history              ‚Üê Cannot get 10:30 changes

Result: MacBook will NEVER receive those 50 words!
```

**Root cause**:
- Session lives in **process memory**
- Not persisted to disk
- Restart = complete loss of tracking state

**Workaround complexity**:
```go
// Would need to persist changesets before every potential restart:
session.GetChangeset()           // Extract changes
db.Exec("INSERT INTO changesets ...") // Persist to disk

// On restart:
rows := db.Query("SELECT * FROM changesets WHERE not_synced = 1")
// Manually reconstruct and send changesets

// This defeats the purpose of using Session Extension!
// Might as well use timestamp-based approach directly.
```

#### ‚ö†Ô∏è Problem 2: Offline Sync Impossible

**Your scenario (Ubuntu isolated environment)**:

```
Friday - Ubuntu (offline):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Start enx-data-service ‚Üí session.create()
Add 30 words           ‚Üí session tracks (in memory)
                       ‚Üí ‚ùå Cannot sync (no network)
Shutdown for weekend   ‚Üí ‚ùå Session destroyed
                       ‚Üí ‚ùå 30 words tracking LOST

Monday - Ubuntu (back online):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Start enx-data-service ‚Üí NEW session.create()
Try to sync            ‚Üí ‚ùå No changeset available
                       ‚Üí ‚ùå 30 words never sync to Desktop/MacBook

Desktop/MacBook:
Never receive the 30 words from Ubuntu
```

**Why timestamp-based works**:
```sql
-- Timestamps are IN THE DATABASE (persistent)
Friday  - Ubuntu: INSERT INTO words (english, update_datetime) VALUES (...)
Weekend - Ubuntu: Offline (data safe in database)
Monday  - Ubuntu: SELECT * FROM words WHERE update_datetime > last_sync
                  ‚úÖ Gets all 30 words added on Friday
                  ‚úÖ Syncs to Desktop/MacBook successfully
```

### When to Use Session Extension?

**Good fit (NONE apply to your project)**:
- ‚úÖ High-frequency changes (thousands/sec)
  - Your case: Human usage, maybe 10 words/day ‚ùå
- ‚úÖ Large records (minimize transfer size)
  - Your case: Small records (~200 bytes each) ‚ùå
- ‚úÖ Complex conflict resolution needs
  - Your case: Simple timestamp comparison ‚ùå
- ‚úÖ Real-time replication required
  - Your case: 5-minute sync interval is fine ‚ùå
- ‚úÖ **Long-running process (no restarts)**
  - Your case: Development environment, frequent restarts ‚ùå
- ‚úÖ Team comfortable with CGo
  - Your case: Prefer pure Go ‚ùå

**Not a good fit (ALL apply to your project)**:
- ‚ùå **Development environment** ‚Üí Frequent restarts kill sessions
- ‚ùå **Offline usage** ‚Üí Session doesn't persist between online/offline cycles
- ‚ùå Simple schema (3 tables)
- ‚ùå Low change frequency (human usage)
- ‚ùå Need persistent change tracking
- ‚ùå Pure Go preference
- ‚ùå Simple timestamp-based solution already works perfectly

### Hybrid Approach: Session Extension + Storage

If you want to use Session Extension for your P2P sync:

```go
// 1. Capture changes with Session Extension
session := NewSession(db)
// ... perform operations ...
changeset, _ := session.GetChangeset()

// 2. Store changeset persistently
storeChangeset(changeset, sequence)

// 3. Sync with peers
for _, peer := range peers {
    changesets := getUnsentChangesets(peer)
    peer.PushChangesets(changesets)
}

// 4. Apply received changesets
for _, changeset := range receivedChangesets {
    ApplyChangeset(db, changeset)
}
```

**Storage table**:
```sql
CREATE TABLE changesets (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    node_id TEXT NOT NULL,
    sequence INTEGER NOT NULL,
    data BLOB NOT NULL,           -- Binary changeset
    timestamp TEXT NOT NULL,
    applied_to TEXT DEFAULT '',   -- CSV of peer IDs
    INDEX idx_sequence (sequence),
    INDEX idx_timestamp (timestamp)
);
```

### Recommendation for Your Project

**Stick with timestamp-based approach** because:

1. **Simplicity**: Pure Go, no CGo complexity
2. **Offline-friendly**: Timestamps persist in database
3. **Easy debugging**: SQL queries, human-readable
4. **Sufficient performance**: Your change frequency is low
5. **Already implemented**: Working solution exists
6. **Restart-safe**: Survives process restarts (Session Extension doesn't)
7. **Offline-safe**: Works across offline/online cycles (Session Extension doesn't)

Session Extension would add complexity without significant benefits for your use case.

## Litestream Analysis

### What is Litestream?

**Litestream** is a popular community tool for **streaming replication** of SQLite databases to cloud storage (S3, Azure Blob, GCS, etc.).

**Official Site**: https://litestream.io/

### How Litestream Works

```
Primary Node (Desktop)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ enx-api                 ‚îÇ
‚îÇ   ‚Üì                     ‚îÇ
‚îÇ enx.db (SQLite + WAL)   ‚îÇ
‚îÇ   ‚Üì                     ‚îÇ
‚îÇ Litestream (monitor)    ‚îÇ ‚Üê Monitors WAL file changes
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ Stream WAL frames
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   S3 / Cloud Storage    ‚îÇ
‚îÇ   - enx.db snapshot     ‚îÇ
‚îÇ   - WAL frames (000001) ‚îÇ
‚îÇ   - WAL frames (000002) ‚îÇ
‚îÇ   - ...                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ Restore command
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Standby Node (MacBook)  ‚îÇ
‚îÇ litestream restore      ‚îÇ
‚îÇ   ‚Üì                     ‚îÇ
‚îÇ enx.db (restored)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Critical Limitation: No Bidirectional Sync

#### ‚ùå Problem: Single Writer Only

**Litestream is designed for disaster recovery, NOT multi-node sync**:

```
‚úÖ Supported (Primary-Standby):
Primary (Desktop)  ‚Üí Litestream ‚Üí S3 ‚Üí Restore ‚Üí Standby (MacBook)
   [WRITE]                                          [READ-ONLY]

‚ùå NOT Supported (Multi-Master):
Desktop ‚Üê‚Üí S3 ‚Üê‚Üí MacBook
 [WRITE]        [WRITE]
   ‚Üì              ‚Üì
 Conflict!    Overwrite!
```

#### Your Use Case Analysis

**What you need**:
```
Monday - Desktop:
  - Add 50 words
  - Sync to MacBook ‚úÖ Want this

Friday - MacBook (traveling):
  - Add 20 words
  - Sync to Desktop ‚úÖ Want this

Weekend - Ubuntu laptop (isolated):
  - Add 30 words (offline)
  - Later sync to Desktop/MacBook ‚úÖ Want this
```

**What Litestream provides**:
```
Desktop ‚Üí S3 ‚Üí MacBook  ‚úÖ Desktop to MacBook works
MacBook ‚Üí S3 ‚Üí Desktop  ‚ùå OVERWRITES Desktop's data!

Problem: Last writer wins, no merge logic
```

**Detailed failure scenario**:
```
Step 1: Desktop adds 50 words
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Desktop: enx.db (1000 words + 50 new = 1050 words)
Litestream: Replicates to S3
S3: enx.db (1050 words)

Step 2: MacBook restores from S3
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
MacBook: litestream restore
MacBook: enx.db (1050 words) ‚úÖ Correct

Step 3: MacBook adds 20 words (offline)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
MacBook: enx.db (1050 + 20 = 1070 words)
Desktop: still has 1050 words

Step 4: MacBook comes online, replicates to S3
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
MacBook: litestream replicate ‚Üí S3
S3: enx.db (1070 words) ‚Üê Overwrites!

Step 5: Desktop restores from S3
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Desktop: litestream restore
Desktop: enx.db (1070 words)
Desktop: ‚ùå Lost local changes made after Step 1!

Result: Data loss if both nodes write
```

### Why Litestream Can't Do P2P Sync

**Architecture reasons**:

1. **No conflict resolution**:
   - Litestream just copies database files
   - No logic to merge changes from multiple sources
   - Last write wins = data loss

2. **Designed for failover, not collaboration**:
   ```
   Intended use case:
   - Primary fails ‚Üí Standby takes over
   - New primary replicates to S3
   - Old primary is DISCARDED

   Your use case:
   - All nodes are active writers
   - Need to merge changes from all nodes
   - No node is discarded
   ```

3. **One-way stream**:
   - Litestream only streams Primary ‚Üí Cloud
   - No built-in mechanism to pull and merge from cloud
   - Manual restore is full replacement, not incremental merge

### Could You Make Litestream Work?

**Theoretical workaround** (not recommended):

```bash
# Each node replicates to separate S3 paths
Desktop:  litestream replicate enx.db s3://bucket/desktop/enx.db
MacBook:  litestream replicate enx.db s3://bucket/macbook/enx.db
Ubuntu:   litestream replicate enx.db s3://bucket/ubuntu/enx.db

# Then manually merge (every sync):
litestream restore s3://bucket/macbook/enx.db -o /tmp/macbook.db
litestream restore s3://bucket/ubuntu/enx.db -o /tmp/ubuntu.db
# Manual merge of databases (requires custom merge tool)
litestream replicate enx.db s3://bucket/desktop/enx.db
```

**Why this is bad**:
- ‚ùå **Complexity**: Litestream + custom merge logic
- ‚ùå **Cost**: 3x storage (full database copy per node)
- ‚ùå **Bandwidth**: Downloading full databases every sync
- ‚ùå **Conflict window**: Race conditions between restore and replicate
- ‚ùå **No benefit**: Too complex for simple use case

### Comparison: Litestream vs Timestamp-based Sync

| Feature | Litestream | Timestamp-based Approach |
|---------|------------|-------------------------|
| **P2P sync** | ‚ùå Single-writer only | ‚úÖ Multi-node merge |
| **Bidirectional** | ‚ùå One-way only | ‚úÖ Bidirectional |
| **Conflict resolution** | ‚ùå None (last write wins) | ‚úÖ Timestamp-based merge |
| **Offline support** | ‚ö†Ô∏è Manual restore | ‚úÖ Automatic on reconnect |
| **Setup complexity** | ‚ö†Ô∏è S3 + credentials | ‚úÖ Minimal setup |
| **Cost** | üí∞ S3 storage + API calls | üí∞ No additional cost |
| **Data safety** | ‚ö†Ô∏è Data loss risk | ‚úÖ Merge preserves all data |

### When to Use Litestream

**Good fit**:
- ‚úÖ Single primary database with hot standby
- ‚úÖ Disaster recovery / backup
- ‚úÖ Point-in-time recovery
- ‚úÖ Production environment (stable, long-running)
- ‚úÖ One writer, multiple read replicas

**NOT a good fit (your case)**:
- ‚ùå Multiple active writers (Desktop, MacBook, Ubuntu)
- ‚ùå Bidirectional sync needed
- ‚ùå Offline-then-merge workflow
- ‚ùå Development environment (frequent restarts)
- ‚ùå Simple 3-node P2P sync

### Litestream Use Case Example (Not Your Scenario)

**Production blog site**:
```
Primary Server (us-east-1):
  - Handles all writes
  - Litestream ‚Üí S3

If primary fails:
  1. Standby (us-west-2): litestream restore
  2. Standby becomes new primary
  3. Update DNS to point to new primary
  4. New primary: litestream ‚Üí S3

Result: < 60 seconds downtime
```

This is **completely different** from your multi-node development scenario.

### Recommendation for Your Project

**Stick with timestamp-based approach** because:

1. **Simplicity**: Pure Go, no CGo complexity
2. **Offline-friendly**: Timestamps persist in database
3. **Easy debugging**: SQL queries, human-readable
4. **Sufficient performance**: Your change frequency is low
5. **Already implemented**: Working solution exists
6. **Restart-safe**: Survives process restarts (Session Extension doesn't)
7. **Offline-safe**: Works across offline/online cycles (Session Extension doesn't)
8. **True P2P**: All nodes can read and write (Litestream can't)
9. **No data loss**: Merge logic preserves all changes (Litestream overwrites)

Both Session Extension and Litestream would add complexity without benefits for your use case.

## cr-sqlite / CRDT Analysis - NOT USED ‚ùå

### ‚ö†Ô∏è Decision: ENX Does NOT Use CRDT

**CRDT is explicitly rejected for ENX** for the following reasons:

1. **‚ùå Too complex for development-phase sync**
   - ENX only needs sync during development
   - Not a production multi-user system
   - CRDT adds unnecessary complexity

2. **‚ùå Overkill for single-user scenario**
   - Only one person (developer) using the system
   - No concurrent writes from multiple users
   - Simple timestamp comparison is sufficient

3. **‚ùå Build complexity (CGo requirements)**
   - cr-sqlite requires C extension compilation
   - Cross-platform build issues
   - Pure Go solution preferred

4. **‚ùå Storage overhead (~30%)**
   - CRDT metadata increases database size
   - Wasted space for unused features
   - Simple timestamps have zero overhead

5. **‚úÖ Current solution is adequate**
   - Timestamp-based Last-Write-Wins works perfectly
   - Conflicts are rare (single user)
   - Simple and maintainable

### What is CRDT? (Background Only)

**CRDT (Conflict-free Replicated Data Type)** is a mathematical approach for multi-master replication:

```
Use case: Multiple users editing same document simultaneously
Example: Google Docs, Figma, Notion

ENX reality: Single developer, sequential access ‚ùå
```

### When CRDT Would Be Needed

**CRDT is designed for**:
- Real-time collaboration (5+ users typing simultaneously)
- Character-level conflict resolution
- Complex multi-master scenarios

**ENX scenario** (single user):
- Monday: Add words on Desktop
- Tuesday: Add words on MacBook
- Only ONE active device at a time

**Conclusion**: Timestamp comparison is sufficient, CRDT is not needed.

### cr-sqlite Reference (Not Implemented)

#### 1. **CRR Tables (Conflict-free Replicated Relations)**

Standard SQLite tables are converted to CRDT-enabled tables:

```sql
-- Create a CRDT table
CREATE TABLE words (
    id INTEGER PRIMARY KEY,
    english TEXT NOT NULL,
    chinese TEXT
);

-- Enable CRDT tracking
SELECT crsql_as_crr('words');

-- cr-sqlite automatically adds metadata:
-- - __crsql_db_version (global version)
-- - __crsql_col_version (per-column version)
-- - __crsql_site_id (node identifier)
```

#### 2. **Version Vectors**

Every change is tracked with version information:

```sql
-- After enabling CRDT, internal structure:
words:
  id | english | chinese | __crsql_col_version | __crsql_site_id
  ---|---------|---------|---------------------|----------------
  1  | hello   | ‰Ω†Â•Ω    | {A:5, B:3}         | A
  2  | world   | ‰∏ñÁïå    | {A:3, B:7}         | B
```

#### 3. **Change Tracking**

cr-sqlite tracks changes at **column level**:

```sql
-- Query changes since version 10
SELECT * FROM crsql_changes WHERE db_version > 10;

Result:
table   | pk  | cid      | val   | col_version | db_version | site_id
--------|-----|----------|-------|-------------|------------|--------
words   | 1   | english  | hello | 5           | 15         | A
words   | 1   | chinese  | ‰Ω†Â•Ω  | 5           | 15         | A
words   | 2   | chinese  | ‰∏ñÁïå  | 7           | 16         | B
```

#### 4. **Automatic Merge**

When syncing between nodes:

```sql
-- Node A receives changes from Node B
INSERT INTO crsql_changes VALUES
  ('words', 2, 'chinese', '‰∏ñÁïå', 7, 16, 'B');

-- cr-sqlite automatically:
-- 1. Compares version vectors
-- 2. Merges if remote version is newer
-- 3. Keeps local if local version is newer
-- 4. No manual conflict resolution needed
```

### Architecture with cr-sqlite

```
Node A (Desktop)                    Node B (MacBook)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ enx.db + cr-sqlite ‚îÇ             ‚îÇ enx.db + cr-sqlite ‚îÇ
‚îÇ                    ‚îÇ             ‚îÇ                    ‚îÇ
‚îÇ words (CRDT)       ‚îÇ             ‚îÇ words (CRDT)       ‚îÇ
‚îÇ - english: "hello" ‚îÇ             ‚îÇ - english: "world" ‚îÇ
‚îÇ - version: {A:5}   ‚îÇ             ‚îÇ - version: {B:7}   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                                  ‚îÇ
         ‚îÇ Pull changes (db_version > 5)   ‚îÇ
         ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
         ‚îÇ Push changes (db_version > 7)   ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
         ‚îÇ                                  ‚îÇ
         ‚îÇ After sync:                      ‚îÇ
         ‚îÇ version: {A:5, B:7}              ‚îÇ
         ‚îÇ Both have "hello" and "world"    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Code Example (Go with CGo)

```go
package main

import (
    "database/sql"
    "log"

    _ "github.com/mattn/go-sqlite3"
)

func main() {
    // Open database with cr-sqlite extension
    db, err := sql.Open("sqlite3", "enx.db?_extensions=crsqlite")
    if err != nil {
        log.Fatal(err)
    }
    defer db.Close()

    // Create table
    db.Exec(`CREATE TABLE IF NOT EXISTS words (
        id INTEGER PRIMARY KEY,
        english TEXT NOT NULL,
        chinese TEXT
    )`)

    // Enable CRDT tracking
    db.Exec(`SELECT crsql_as_crr('words')`)

    // Get current database version
    var dbVersion int64
    db.QueryRow(`SELECT crsql_db_version()`).Scan(&dbVersion)
    log.Printf("Current DB version: %d", dbVersion)

    // Insert data (automatically tracked)
    db.Exec(`INSERT INTO words (english, chinese) VALUES (?, ?)`,
        "hello", "‰Ω†Â•Ω")

    // Query changes since version 0 (all changes)
    rows, _ := db.Query(`SELECT * FROM crsql_changes WHERE db_version > 0`)
    defer rows.Close()

    for rows.Next() {
        var table, pk, cid, val string
        var colVer, dbVer int64
        var siteId string

        rows.Scan(&table, &pk, &cid, &val, &colVer, &dbVer, &siteId)
        log.Printf("Change: %s.%s = %s (version %d)", table, cid, val, dbVer)
    }
}

// Sync function: Pull changes from remote node
func syncFromRemote(db *sql.DB, remoteDB *sql.DB, lastSyncVersion int64) error {
    // Get changes from remote since last sync
    rows, err := remoteDB.Query(`
        SELECT "table", pk, cid, val, col_version, db_version, site_id
        FROM crsql_changes
        WHERE db_version > ?
    `, lastSyncVersion)
    if err != nil {
        return err
    }
    defer rows.Close()

    // Apply changes to local database
    stmt, _ := db.Prepare(`INSERT INTO crsql_changes VALUES (?, ?, ?, ?, ?, ?, ?)`)
    defer stmt.Close()

    for rows.Next() {
        var table, pk, cid, val string
        var colVer, dbVer int64
        var siteId string

        rows.Scan(&table, &pk, &cid, &val, &colVer, &dbVer, &siteId)

        // cr-sqlite automatically handles conflicts
        stmt.Exec(table, pk, cid, val, colVer, dbVer, siteId)
    }

    return nil
}
```

### Sync Protocol

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ P2P Sync with cr-sqlite                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Node A                           Node B
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                           ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
1. Get local version
   SELECT crsql_db_version()     ‚Üí version: 15

2. Request changes from Node B
   "Send me changes since version 15"
                                 3. Query changes
                                    SELECT * FROM crsql_changes
                                    WHERE db_version > 15

                                 4. Send changes
   ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  Changes: version 16-20

5. Apply changes
   INSERT INTO crsql_changes     ‚Üí Automatic merge

6. Push local changes to Node B
   SELECT * FROM crsql_changes
   WHERE db_version > last_sync  ‚Üí Changes: version 21-25
                                 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫

                                 7. Apply changes
                                    INSERT INTO crsql_changes
                                    ‚Üí Automatic merge

Result: Both nodes at version 25, all changes merged
```

### Advantages of cr-sqlite

| Feature | Benefit |
|---------|---------|
| **True multi-master** | All nodes can read and write simultaneously |
| **Automatic conflict resolution** | No manual timestamp comparison needed |
| **Column-level tracking** | More granular than row-level |
| **Mathematically correct** | CRDT guarantees eventual consistency |
| **Offline support** | Changes queue locally, sync when online |
| **No central server** | True P2P architecture |
| **Order independent** | Changes can arrive in any order |

### Disadvantages and Limitations

| Issue | Impact |
|-------|--------|
| **Requires CGo** | Complex build, platform dependencies |
| **Extension installation** | Need to compile and load cr-sqlite extension |
| **Storage overhead** | Metadata for version vectors (~30% overhead) |
| **Learning curve** | CRDT concepts are complex |
| **Maturity** | Relatively new project (2021) |
| **Limited ecosystem** | Fewer tools and examples |
| **Schema migrations** | More complex with CRDT tables |
| **No standard** | Proprietary to vlcn.io |

### Performance Comparison

```
Scenario: Sync 1000 word records between 3 nodes

Timestamp-based (current):
- Transfer size: ~200KB (full records)
- Conflicts: Manual comparison of 1000 timestamps
- Time: ~500ms
- Storage: No overhead

cr-sqlite:
- Transfer size: ~80KB (only changes with metadata)
- Conflicts: Automatic CRDT merge
- Time: ~200ms (faster merge)
- Storage: +30% (version vectors)
```

### Use Case Fit Analysis

#### ‚úÖ Great for cr-sqlite (NOT your case):

1. **High-frequency concurrent writes**
   ```
   Example: Collaborative document editing
   - 10 users typing simultaneously
   - Character-level CRDTs
   - Real-time sync

   Your case: Single user at a time ‚ùå
   ```

2. **Complex conflict scenarios**
   ```
   Example: Distributed inventory system
   - Multiple warehouses updating stock
   - Need to preserve all updates
   - Complex merge logic

   Your case: Simple timestamp comparison works ‚ùå
   ```

3. **Offline-first apps with unpredictable network**
   ```
   Example: Mobile field service app
   - Technicians work offline all day
   - Sync when back to office
   - Many concurrent offline users

   Your case: Only you, predictable schedule ‚ùå
   ```

#### ‚ùå Overkill for your project:

**Your requirements**:
- 3 nodes (Desktop, MacBook, Ubuntu)
- **Single user at a time** (no concurrent writes)
- Simple schema (words, user_dicts, users)
- Low change frequency (~10 words/day)
- Simple conflict rule: "keep newer"

**Why cr-sqlite is overkill**:

1. **No concurrent writes**: You never write to 2 nodes simultaneously
   - CRDT's main benefit is handling concurrent conflicts
   - You don't have concurrent conflicts (only sequential)

2. **Simple conflict resolution**: Timestamp comparison is sufficient
   ```
   Current: if remote.update_datetime > local.update_datetime ‚Üí keep remote
   cr-sqlite: Complex version vector comparison ‚Üí same result
   ```

3. **Storage overhead**: +30% for metadata you don't need
   ```
   Current: 1000 words = ~500KB
   cr-sqlite: 1000 words = ~650KB (150KB wasted on version vectors)
   ```

4. **Complexity**: CGo + CRDT concepts vs simple SQL
   ```
   Current: SELECT * FROM words WHERE update_datetime > ?
   cr-sqlite: Understand version vectors, site IDs, column versions
   ```

### Comparison Table

| Aspect | Timestamp (Current) | cr-sqlite |
|--------|---------------------|-----------|
| **Concurrent writes** | ‚ùå Last write wins | ‚úÖ CRDT merge |
| **Your use case** | ‚úÖ Sequential writes | ‚ùå Overkill |
| **Complexity** | ‚≠ê‚≠ê Simple | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Complex |
| **Build** | ‚úÖ Pure Go | ‚ùå CGo required |
| **Storage** | ‚úÖ No overhead | ‚ùå +30% overhead |
| **Maturity** | ‚úÖ Battle-tested | ‚ö†Ô∏è New (2021) |
| **Debugging** | ‚úÖ SQL + timestamps | ‚ö†Ô∏è Version vectors |
| **Restart-safe** | ‚úÖ Persistent | ‚úÖ Persistent |
| **Offline-safe** | ‚úÖ Works | ‚úÖ Works |

### Installation Requirements

If you still want to try cr-sqlite:

```bash
# 1. Clone and build cr-sqlite
git clone https://github.com/vlcn-io/cr-sqlite
cd cr-sqlite
make loadable

# 2. Copy extension to system
cp crsqlite.so /usr/local/lib/

# 3. Load in Go
import _ "github.com/mattn/go-sqlite3"

db, err := sql.Open("sqlite3", "enx.db?_extensions=/usr/local/lib/crsqlite")

# 4. Enable for tables
db.Exec("SELECT crsql_as_crr('words')")
```

### Real-World Example: When cr-sqlite Shines

**Notion-like collaborative app**:
```
Scenario: 5 users editing same document
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
User A: Types "Hello" at 10:00:00.000
User B: Types "World" at 10:00:00.001
User C: Deletes char at 10:00:00.002
User D: Formats text at 10:00:00.003
User E: Inserts image at 10:00:00.004

All happening simultaneously, offline/online

cr-sqlite: ‚úÖ Handles perfectly with CRDTs
Timestamp: ‚ùå Would lose some edits
```

**Your scenario: ENX vocabulary learning**:
```
Monday:    Desktop adds "hello"     (you)
Tuesday:   Desktop adds "world"     (you)
Wednesday: MacBook adds "goodbye"   (you, while traveling)

Only ONE writer at a time

cr-sqlite: Overkill (CRDT for single user?)
Timestamp: ‚úÖ Perfect fit
```

### Final Decision: Timestamp-Based LWW ‚úÖ

**ENX uses simple timestamp-based Last-Write-Wins (LWW)**:

```
Why this is the right choice:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚úÖ Development-phase sync only (not production)
‚úÖ Single user (developer), no concurrent writes
‚úÖ Simple and maintainable
‚úÖ Pure Go, no CGo complexity
‚úÖ Zero storage overhead
‚úÖ Easy to debug and understand
‚úÖ NTP synchronization handles clock accuracy

CRDT is NOT needed:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚ùå Too complex for single-user scenario
‚ùå Designed for real-time multi-user collaboration
‚ùå Adds 30% storage overhead for unused features
‚ùå Requires CGo (complex builds)
‚ùå Overkill for development sync

Bottom line:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Keep it simple. Timestamp comparison works perfectly
for the ENX development workflow.
```

**If requirements change** (e.g., adding multi-user collaboration), revisit CRDT then. Don't over-engineer now.

### If You Still Want to Try

Here's a minimal example repo structure:

```
enx-data-service/
‚îú‚îÄ‚îÄ go.mod
‚îú‚îÄ‚îÄ session/
‚îÇ   ‚îú‚îÄ‚îÄ manager.go         # CGo wrapper for Session API
‚îÇ   ‚îú‚îÄ‚îÄ manager.h          # C headers
‚îÇ   ‚îî‚îÄ‚îÄ manager_test.go
‚îú‚îÄ‚îÄ storage/
‚îÇ   ‚îî‚îÄ‚îÄ changeset_store.go # Persistent changeset storage
‚îî‚îÄ‚îÄ sync/
    ‚îî‚îÄ‚îÄ session_sync.go    # Sync using Session Extension
```

Let me know if you want me to create a proof-of-concept implementation!

## Deployment Configuration

### Single Node (Development)

```yaml
# docker-compose.yml
version: '3.8'

services:
  data-service:
    image: enx-data-service:latest
    ports:
      - "8091:8091"
    volumes:
      - ./data:/data
    environment:
      - PORT=8091
      - DB_PATH=/data/enx.db
      - LOG_LEVEL=debug

  api:
    image: enx-api:latest
    ports:
      - "8090:8090"
    environment:
      - DATA_SERVICE_URL=data-service:8091
    depends_on:
      - data-service
```

### Multi-Node (Production)

```yaml
# Host A
services:
  data-service-a:
    image: enx-data-service:latest
    ports:
      - "8091:8091"
    environment:
      - NODE_ID=host-a
      - PEERS=host-b:8091,host-c:8091
      - SYNC_INTERVAL=300  # 5 minutes

# Host B
services:
  data-service-b:
    image: enx-data-service:latest
    ports:
      - "8091:8091"
    environment:
      - NODE_ID=host-b
      - PEERS=host-a:8091,host-c:8091
      - SYNC_INTERVAL=300

# Host C
services:
  data-service-c:
    image: enx-data-service:latest
    ports:
      - "8091:8091"
    environment:
      - NODE_ID=host-c
      - PEERS=host-a:8091,host-b:8091
      - SYNC_INTERVAL=300
```

## Implementation Timeline

### Phase 1: Foundation (Week 1-2)

- [ ] Create enx-data-service project structure
- [ ] Define Protocol Buffers / REST API
- [ ] Implement basic CRUD operations
- [ ] Create client library for enx-api
- [ ] Update enx-api to use data client
- [ ] Single-node testing

### Phase 2: Sync Implementation (Week 3-4)

- [ ] Implement change tracking
- [ ] Implement GetChanges API
- [ ] Implement PushChanges API
- [ ] Node discovery and registration
- [ ] P2P sync logic
- [ ] Conflict resolution
- [ ] Multi-node testing

### Phase 3: Optimization (Week 5-6)

- [ ] Add caching layer
- [ ] Implement connection pooling
- [ ] Add retry mechanisms
- [ ] Performance testing
- [ ] Load testing
- [ ] Optimization

### Phase 4: Production Readiness (Week 7-8)

- [ ] Add authentication
- [ ] Enable TLS
- [ ] Monitoring and metrics
- [ ] Logging and tracing
- [ ] Documentation
- [ ] Deployment automation
- [ ] Production deployment

## Future Enhancements

### Short-term (3-6 months)

- [ ] Web UI for monitoring sync status
- [ ] Conflict resolution UI
- [ ] Data backup and restore
- [ ] Metrics dashboard

### Medium-term (6-12 months)

- [ ] Support for PostgreSQL backend
- [ ] Redis caching layer
- [ ] Multi-tenancy support
- [ ] Advanced analytics

### Long-term (12+ months)

- [ ] Global distribution with geo-replication
- [ ] Event sourcing architecture
- [ ] Machine learning for conflict resolution
- [ ] Mobile client support

## Generalization: SQLite Sync as Open Source Project

### üí° Vision: Universal SQLite P2P Sync Tool

**The insight**: This data service design is actually **business-agnostic** and could be extracted into a standalone open-source project: **`sqlite-p2p-sync`**

### Core Concept

A **generic SQLite synchronization service** that can sync any SQLite database across multiple nodes using timestamp-based conflict resolution.

### Key Features of Generic Tool

#### 1. **Configuration-Driven** (No Code Changes)

```yaml
# sync-config.yaml
database:
  path: "./my-app.db"

tables:
  - name: "users"
    timestamp_column: "updated_at"
    primary_key: "id"

  - name: "posts"
    timestamp_column: "modified_time"
    primary_key: "post_id"

  - name: "comments"
    timestamp_column: "update_datetime"
    primary_key: ["post_id", "comment_id"]  # Composite key

sync:
  interval: "5m"
  conflict_resolution: "latest_wins"  # or: manual, custom

nodes:
  - name: "desktop"
    address: "192.168.1.100:8091"
  - name: "laptop"
    address: "192.168.1.101:8091"
```

#### 2. **Generic Query API Design** üîç

For a universal data service, the query API must be flexible enough to handle any table schema without hardcoding specific fields.

**‚úÖ Design Decision Summary:**

```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
CHOSEN APPROACH: Hybrid (Option C)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Why Hybrid?
‚Ä¢ Structured APIs (Find/Insert/Update/Delete) for 80% of use cases
  ‚Üí Type-safe, secure, easy to use
  ‚Üí Automatic query building from JSON filters

‚Ä¢ Raw SQL (Query/Execute) for remaining 20% complex cases
  ‚Üí JOINs, aggregations, subqueries, CTEs
  ‚Üí Full SQL power when needed

‚Ä¢ Best of both worlds:
  ‚úÖ Security: Structured APIs prevent most SQL injection
  ‚úÖ Flexibility: Raw SQL handles edge cases
  ‚úÖ Performance: Both approaches equally fast
  ‚úÖ Developer experience: Easy for simple, powerful for complex

Implementation:
  1. Start with structured APIs (safer)
  2. Fall back to raw SQL only when necessary
  3. Both share same security validation layer
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
```

**Design Approaches:**

##### **Option A: SQL Passthrough (Simplest)** ‚≠ê‚≠ê‚≠ê

Direct SQL query execution with parameter binding:

```protobuf
service GenericDataService {
  // Execute arbitrary SQL query
  rpc Query(QueryRequest) returns (QueryResponse);

  // Execute write operations (INSERT/UPDATE/DELETE)
  rpc Execute(ExecuteRequest) returns (ExecuteResponse);
}

message QueryRequest {
  string sql = 1;                    // SQL query
  repeated QueryParam params = 2;    // Bound parameters
  int32 limit = 3;                   // Optional row limit
  int32 offset = 4;                  // Optional offset for pagination
}

message QueryParam {
  oneof value {
    string string_value = 1;
    int64 int_value = 2;
    double double_value = 3;
    bool bool_value = 4;
    bytes bytes_value = 5;
  }
}

message QueryResponse {
  repeated string columns = 1;       // Column names
  repeated Row rows = 2;             // Result rows
  int32 rows_affected = 3;           // For write operations
}

message Row {
  repeated Cell cells = 1;           // Row values
}

message Cell {
  oneof value {
    string string_value = 1;
    int64 int_value = 2;
    double double_value = 3;
    bool bool_value = 4;
    bytes bytes_value = 5;
    bool is_null = 6;                // NULL value indicator
  }
}
```

**Usage Example:**

```go
// Query with parameters
resp, err := client.Query(ctx, &pb.QueryRequest{
    Sql: "SELECT * FROM users WHERE age > ? AND city = ?",
    Params: []*pb.QueryParam{
        {Value: &pb.QueryParam_IntValue{IntValue: 18}},
        {Value: &pb.QueryParam_StringValue{StringValue: "Beijing"}},
    },
    Limit: 100,
})

// Insert with parameters
resp, err := client.Execute(ctx, &pb.ExecuteRequest{
    Sql: "INSERT INTO users (name, age, email) VALUES (?, ?, ?)",
    Params: []*pb.QueryParam{
        {Value: &pb.QueryParam_StringValue{StringValue: "Alice"}},
        {Value: &pb.QueryParam_IntValue{IntValue: 25}},
        {Value: &pb.QueryParam_StringValue{StringValue: "alice@example.com"}},
    },
})
```

**Pros:**
- ‚úÖ Maximum flexibility - supports any SQL query
- ‚úÖ No need to define schema in protobuf
- ‚úÖ Works with any table structure
- ‚úÖ Simple implementation

**Cons:**
- ‚ö†Ô∏è SQL injection risk (mitigated by parameterized queries)
- ‚ö†Ô∏è No type safety at compile time
- ‚ö†Ô∏è Client needs to know SQL syntax

##### **Option B: JSON-Based Query Builder** ‚≠ê‚≠ê‚≠ê‚≠ê

Structured query using JSON-like filters (similar to MongoDB):

```protobuf
service GenericDataService {
  rpc Find(FindRequest) returns (FindResponse);
  rpc Insert(InsertRequest) returns (InsertResponse);
  rpc Update(UpdateRequest) returns (UpdateResponse);
  rpc Delete(DeleteRequest) returns (DeleteResponse);
}

message FindRequest {
  string table = 1;                  // Table name
  string filter = 2;                 // JSON filter: {"age": {"$gt": 18}}
  string projection = 3;             // JSON fields: {"name": 1, "email": 1}
  string sort = 4;                   // JSON sort: {"age": -1}
  int32 limit = 5;
  int32 offset = 6;
}

message InsertRequest {
  string table = 1;
  repeated string records = 2;       // JSON records
}

message UpdateRequest {
  string table = 1;
  string filter = 2;                 // JSON filter
  string update = 3;                 // JSON update: {"$set": {"age": 26}}
}

message DeleteRequest {
  string table = 1;
  string filter = 2;                 // JSON filter
}
```

**Usage Example:**

```go
// Find with filter
resp, err := client.Find(ctx, &pb.FindRequest{
    Table: "users",
    Filter: `{"age": {"$gt": 18}, "city": "Beijing"}`,
    Projection: `{"name": 1, "email": 1}`,  // Only return name and email
    Sort: `{"age": -1}`,                    // Sort by age descending
    Limit: 100,
})

// Update with filter
resp, err := client.Update(ctx, &pb.UpdateRequest{
    Table: "users",
    Filter: `{"email": "alice@example.com"}`,
    Update: `{"$set": {"age": 26, "city": "Shanghai"}}`,
})
```

**Pros:**
- ‚úÖ More structured than raw SQL
- ‚úÖ Familiar to NoSQL users
- ‚úÖ Type-safe operators ($gt, $lt, $in, etc.)
- ‚úÖ No SQL injection risk

**Cons:**
- ‚ö†Ô∏è Need to implement query parser
- ‚ö†Ô∏è Limited to supported operators
- ‚ö†Ô∏è Still uses JSON strings (no compile-time checking)

##### **Option C: Hybrid Approach** (‚úÖ **CHOSEN**) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**Decision: This is the selected approach for the generic SQLite sync service**

Combine both approaches for maximum flexibility:

```protobuf
service GenericDataService {
  // Simple CRUD (type-safe, recommended for common operations)
  rpc Find(FindRequest) returns (FindResponse);
  rpc Insert(InsertRequest) returns (InsertResponse);
  rpc Update(UpdateRequest) returns (UpdateResponse);
  rpc Delete(DeleteRequest) returns (DeleteResponse);

  // Raw SQL (flexible, for complex queries)
  rpc Query(QueryRequest) returns (QueryResponse);
  rpc Execute(ExecuteRequest) returns (ExecuteResponse);

  // Batch operations
  rpc BatchExecute(stream BatchRequest) returns (BatchResponse);
}
```

**When to use each:**

```
Common CRUD operations:
  ‚Üí Use Find/Insert/Update/Delete (structured, safer)

Complex queries (JOINs, aggregations, subqueries):
  ‚Üí Use Query/Execute (raw SQL, more powerful)

Bulk operations:
  ‚Üí Use BatchExecute (efficient)

Example decision tree:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Query type                           ‚Üí API to use
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
SELECT * FROM users WHERE age > 18   ‚Üí Find()
INSERT INTO users VALUES (...)       ‚Üí Insert()
UPDATE users SET age = 26 WHERE ...  ‚Üí Update()
DELETE FROM users WHERE ...          ‚Üí Delete()

SELECT u.*, p.title FROM users u
  JOIN posts p ON u.id = p.user_id   ‚Üí Query() (complex JOIN)

SELECT COUNT(*), AVG(age)
  FROM users GROUP BY city           ‚Üí Query() (aggregation)

WITH RECURSIVE ... (CTE query)       ‚Üí Query() (advanced SQL)
```

**Complete Implementation Example:**

```go
package main

import (
    "context"
    "database/sql"
    "encoding/json"
    "fmt"

    _ "github.com/mattn/go-sqlite3"
)

// ==================== Server Implementation ====================

type GenericDataService struct {
    db *sql.DB
}

// Find - Structured query (JSON filter)
func (s *GenericDataService) Find(ctx context.Context, req *pb.FindRequest) (*pb.FindResponse, error) {
    // Parse JSON filter
    var filter map[string]interface{}
    if err := json.Unmarshal([]byte(req.Filter), &filter); err != nil {
        return nil, fmt.Errorf("invalid filter: %w", err)
    }

    // Build SQL query
    query, args := buildSelectQuery(req.Table, filter, req.Projection, req.Sort, req.Limit, req.Offset)

    // Execute query
    rows, err := s.db.QueryContext(ctx, query, args...)
    if err != nil {
        return nil, err
    }
    defer rows.Close()

    // Convert rows to response
    return rowsToResponse(rows)
}

// Query - Raw SQL execution
func (s *GenericDataService) Query(ctx context.Context, req *pb.QueryRequest) (*pb.QueryResponse, error) {
    // Validate SQL (prevent destructive operations)
    if err := validateSQL(req.Sql); err != nil {
        return nil, err
    }

    // Convert protobuf params to []interface{}
    args := make([]interface{}, len(req.Params))
    for i, param := range req.Params {
        args[i] = extractParamValue(param)
    }

    // Execute query
    rows, err := s.db.QueryContext(ctx, req.Sql, args...)
    if err != nil {
        return nil, err
    }
    defer rows.Close()

    return rowsToResponse(rows)
}

// Helper: Build SELECT query from JSON filter
func buildSelectQuery(table string, filter map[string]interface{},
                      projection, sort string, limit, offset int32) (string, []interface{}) {

    query := fmt.Sprintf("SELECT * FROM %s", sanitizeIdentifier(table))
    args := []interface{}{}

    // Add WHERE clause
    if len(filter) > 0 {
        where, whereArgs := buildWhereClause(filter)
        query += " WHERE " + where
        args = append(args, whereArgs...)
    }

    // Add ORDER BY
    if sort != "" {
        var sortMap map[string]int
        json.Unmarshal([]byte(sort), &sortMap)
        query += buildOrderBy(sortMap)
    }

    // Add LIMIT/OFFSET
    if limit > 0 {
        query += fmt.Sprintf(" LIMIT %d", limit)
        if offset > 0 {
            query += fmt.Sprintf(" OFFSET %d", offset)
        }
    }

    return query, args
}

// Helper: Build WHERE clause from filter
func buildWhereClause(filter map[string]interface{}) (string, []interface{}) {
    var conditions []string
    var args []interface{}

    for field, value := range filter {
        switch v := value.(type) {
        case map[string]interface{}:
            // Operators: {"age": {"$gt": 18}}
            for op, val := range v {
                switch op {
                case "$gt":
                    conditions = append(conditions, fmt.Sprintf("%s > ?", sanitizeIdentifier(field)))
                    args = append(args, val)
                case "$gte":
                    conditions = append(conditions, fmt.Sprintf("%s >= ?", sanitizeIdentifier(field)))
                    args = append(args, val)
                case "$lt":
                    conditions = append(conditions, fmt.Sprintf("%s < ?", sanitizeIdentifier(field)))
                    args = append(args, val)
                case "$lte":
                    conditions = append(conditions, fmt.Sprintf("%s <= ?", sanitizeIdentifier(field)))
                    args = append(args, val)
                case "$ne":
                    conditions = append(conditions, fmt.Sprintf("%s != ?", sanitizeIdentifier(field)))
                    args = append(args, val)
                case "$in":
                    // Handle IN operator with multiple values
                    if arr, ok := val.([]interface{}); ok {
                        placeholders := strings.Repeat("?,", len(arr))
                        placeholders = placeholders[:len(placeholders)-1]
                        conditions = append(conditions, fmt.Sprintf("%s IN (%s)", sanitizeIdentifier(field), placeholders))
                        args = append(args, arr...)
                    }
                }
            }
        default:
            // Simple equality: {"city": "Beijing"}
            conditions = append(conditions, fmt.Sprintf("%s = ?", sanitizeIdentifier(field)))
            args = append(args, value)
        }
    }

    return strings.Join(conditions, " AND "), args
}

// Helper: Sanitize SQL identifiers (prevent injection)
func sanitizeIdentifier(name string) string {
    // Only allow alphanumeric and underscore
    reg := regexp.MustCompile(`^[a-zA-Z0-9_]+$`)
    if !reg.MatchString(name) {
        panic(fmt.Sprintf("invalid identifier: %s", name))
    }
    return name
}

// Helper: Validate SQL (prevent dangerous operations)
func validateSQL(sql string) error {
    sql = strings.ToUpper(strings.TrimSpace(sql))

    // Allow only SELECT, INSERT, UPDATE, DELETE
    allowedPrefixes := []string{"SELECT", "INSERT", "UPDATE", "DELETE"}
    allowed := false
    for _, prefix := range allowedPrefixes {
        if strings.HasPrefix(sql, prefix) {
            allowed = true
            break
        }
    }

    if !allowed {
        return fmt.Errorf("SQL statement not allowed: must start with SELECT/INSERT/UPDATE/DELETE")
    }

    // Block dangerous keywords
    dangerousKeywords := []string{"DROP", "TRUNCATE", "ALTER", "CREATE", "PRAGMA"}
    for _, keyword := range dangerousKeywords {
        if strings.Contains(sql, keyword) {
            return fmt.Errorf("SQL contains forbidden keyword: %s", keyword)
        }
    }

    return nil
}

// ==================== Client Usage ====================

func ExampleClientUsage() {
    conn, _ := grpc.Dial("localhost:8091", grpc.WithInsecure())
    client := pb.NewGenericDataServiceClient(conn)

    // Example 1: Simple structured query
    resp, err := client.Find(context.Background(), &pb.FindRequest{
        Table: "users",
        Filter: `{"age": {"$gt": 18}, "city": "Beijing"}`,
        Limit: 100,
    })

    // Example 2: Complex JOIN with raw SQL
    resp, err = client.Query(context.Background(), &pb.QueryRequest{
        Sql: `
            SELECT u.name, u.email, COUNT(p.id) as post_count
            FROM users u
            LEFT JOIN posts p ON u.id = p.user_id
            WHERE u.age > ?
            GROUP BY u.id
            HAVING post_count > ?
            ORDER BY post_count DESC
            LIMIT ?
        `,
        Params: []*pb.QueryParam{
            {Value: &pb.QueryParam_IntValue{IntValue: 18}},
            {Value: &pb.QueryParam_IntValue{IntValue: 5}},
            {Value: &pb.QueryParam_IntValue{IntValue: 10}},
        },
    })

    // Example 3: Insert with structured API
    resp, err = client.Insert(context.Background(), &pb.InsertRequest{
        Table: "users",
        Records: []string{
            `{"name": "Alice", "age": 25, "email": "alice@example.com"}`,
            `{"name": "Bob", "age": 30, "email": "bob@example.com"}`,
        },
    })
}
```

**Security Considerations for Generic Query API:**

```go
// ==================== Security Best Practices ====================

// 1. Input Validation
func validateTableName(table string) error {
    // Only allow configured tables
    allowedTables := config.GetAllowedTables()
    if !contains(allowedTables, table) {
        return fmt.Errorf("table not allowed: %s", table)
    }
    return nil
}

// 2. Query Complexity Limits
func validateQueryComplexity(sql string) error {
    // Limit number of JOINs
    joinCount := strings.Count(strings.ToUpper(sql), "JOIN")
    if joinCount > 5 {
        return fmt.Errorf("too many JOINs: %d (max: 5)", joinCount)
    }

    // Limit subquery depth
    subqueryDepth := strings.Count(sql, "(SELECT")
    if subqueryDepth > 3 {
        return fmt.Errorf("subquery too deep: %d (max: 3)", subqueryDepth)
    }

    return nil
}

// 3. Rate Limiting per Client
type QueryRateLimiter struct {
    limiters map[string]*rate.Limiter
    mu       sync.RWMutex
}

func (r *QueryRateLimiter) Allow(clientID string) bool {
    r.mu.RLock()
    limiter, exists := r.limiters[clientID]
    r.mu.RUnlock()

    if !exists {
        r.mu.Lock()
        limiter = rate.NewLimiter(rate.Limit(100), 10) // 100 req/sec, burst 10
        r.limiters[clientID] = limiter
        r.mu.Unlock()
    }

    return limiter.Allow()
}

// 4. Query Timeout
func executeWithTimeout(ctx context.Context, db *sql.DB, query string, args ...interface{}) (*sql.Rows, error) {
    ctx, cancel := context.WithTimeout(ctx, 30*time.Second)
    defer cancel()

    return db.QueryContext(ctx, query, args...)
}

// 5. Row Limit Enforcement
const MaxRowsPerQuery = 10000

func enforceRowLimit(req *pb.QueryRequest) {
    if req.Limit == 0 || req.Limit > MaxRowsPerQuery {
        req.Limit = MaxRowsPerQuery
    }
}
```

**Configuration for Security:**

```yaml
# sync-config.yaml
security:
  # Allow only specific tables
  allowed_tables:
    - "users"
    - "posts"
    - "comments"

  # Query limits
  max_query_complexity: 5        # Max JOINs
  max_subquery_depth: 3
  max_rows_per_query: 10000
  query_timeout_seconds: 30

  # Rate limiting
  rate_limit_per_client: 100     # Requests per second
  rate_limit_burst: 10

  # SQL restrictions
  allow_raw_sql: true            # Enable/disable Query() API
  forbidden_keywords:
    - "DROP"
    - "TRUNCATE"
    - "ALTER"
    - "CREATE"
    - "PRAGMA"
    - "ATTACH"
    - "DETACH"
```

#### 3. **Automatic Metadata Table**

The tool automatically creates a sync tracking table:

```sql
-- Auto-created by sqlite-p2p-sync
CREATE TABLE IF NOT EXISTS _sync_metadata (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    table_name TEXT NOT NULL,
    node_id TEXT NOT NULL,
    last_sync_time TEXT NOT NULL,  -- RFC3339 timestamp
    last_sync_checksum TEXT,       -- Optional: verify integrity
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(table_name, node_id)
);

-- Track sync status per table per node
-- Example rows:
-- table_name | node_id  | last_sync_time           | last_sync_checksum
-- -----------|----------|--------------------------|-------------------
-- users      | desktop  | 2025-11-12T10:00:00Z    | abc123def456
-- users      | laptop   | 2025-11-12T09:55:00Z    | abc123def456
-- posts      | desktop  | 2025-11-12T10:05:00Z    | 789ghi012jkl
```

#### 3. **Flexible Timestamp Detection**

```yaml
# Supports multiple timestamp column naming conventions
timestamp_column_patterns:
  - "updated_at"
  - "modified_at"
  - "update_time"
  - "update_datetime"
  - "last_modified"

# Or custom per table
tables:
  - name: "legacy_table"
    timestamp_column: "LAST_UPD_TS"  # Custom column name
```

#### 4. **Conflict Resolution Strategies**

```go
type ConflictResolver interface {
    Resolve(local, remote Record) (Record, error)
}

// Built-in strategies
strategies := map[string]ConflictResolver{
    "latest_wins":     &LatestWinsResolver{},      // Use newer timestamp
    "source_wins":     &SourceWinsResolver{},      // Prefer specific node
    "manual":          &ManualResolver{},          // Require user input
    "custom_function": &CustomFunctionResolver{},  // User-defined logic
}
```

### Architecture of Generic Tool

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    sqlite-p2p-sync (Generic Tool)               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ  ‚îÇ Config Loader  ‚îÇ      ‚îÇ  Sync Engine   ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ - Read YAML    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ - Timestamp    ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ - Validate     ‚îÇ      ‚îÇ - Conflict Res ‚îÇ                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îÇ                                   ‚îÇ                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ  ‚îÇ Metadata Mgmt  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  gRPC Server   ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ - Track sync   ‚îÇ      ‚îÇ - Serve data   ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ - Checksum     ‚îÇ      ‚îÇ - P2P sync     ‚îÇ                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îÇ          ‚îÇ                        ‚îÇ                             ‚îÇ
‚îÇ          ‚ñº                        ‚ñº                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ
‚îÇ  ‚îÇ        SQLite Database                ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ  - User tables (any schema)           ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ  - _sync_metadata (auto-created)      ‚îÇ                     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Usage Example

#### Installation
```bash
# Install as standalone tool
go install github.com/yourname/sqlite-p2p-sync@latest

# Or use Docker
docker pull yourname/sqlite-p2p-sync:latest
```

#### Configuration
```bash
# Initialize sync for existing database
sqlite-p2p-sync init --db ./myapp.db --auto-detect

# Output: Generated sync-config.yaml with detected tables:
# ‚úÖ Found table 'users' with timestamp column 'updated_at'
# ‚úÖ Found table 'posts' with timestamp column 'modified_time'
# ‚ö†Ô∏è  Table 'logs' has no timestamp column, skipped
```

#### Running
```bash
# Start sync service
sqlite-p2p-sync start --config sync-config.yaml

# Output:
# üöÄ SQLite P2P Sync v1.0.0
# üìÅ Database: ./myapp.db
# üìä Syncing tables: users, posts, comments
# üîÑ Sync interval: 5 minutes
# üåê Listening on :8091
# ‚úÖ Ready for sync
```

#### Integration with Existing App

```go
// Your existing application (enx-api)
package main

import (
    "database/sql"
    syncpb "github.com/yourname/sqlite-p2p-sync/proto"
    "google.golang.org/grpc"
)

func main() {
    // Option 1: Direct database access (local only)
    db, _ := sql.Open("sqlite3", "./enx.db")

    // Option 2: Use sync service (for sync-enabled access)
    conn, _ := grpc.Dial("localhost:8091", grpc.WithInsecure())
    syncClient := syncpb.NewSyncServiceClient(conn)

    // Your app code remains unchanged!
    // Sync happens in background automatically
}
```

### Optimization Suggestions

#### 1. **Smart Sync: Only Changed Records**

Instead of scanning all records every time:

```sql
-- Current approach (full scan)
SELECT * FROM users WHERE updated_at > ?

-- Optimized: Use change tracking table
CREATE TABLE _sync_changes (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    table_name TEXT NOT NULL,
    record_id TEXT NOT NULL,
    operation TEXT NOT NULL,  -- INSERT, UPDATE, DELETE
    timestamp TEXT NOT NULL,
    synced INTEGER DEFAULT 0,
    UNIQUE(table_name, record_id, timestamp)
);

-- Populate with triggers
CREATE TRIGGER track_user_changes
AFTER INSERT OR UPDATE OR DELETE ON users
BEGIN
    INSERT INTO _sync_changes (table_name, record_id, operation, timestamp)
    VALUES ('users', NEW.id, 'UPDATE', NEW.updated_at);
END;
```

**Benefit**: Only sync changed records, not full table scan every time.

#### 2. **Batch Sync with Checksum**

```go
// Send batch with checksum for integrity verification
type SyncBatch struct {
    TableName string
    Records   []Record
    Checksum  string  // SHA256 of all records
    TimeRange TimeRange
}

// Receiver verifies checksum
func (s *SyncService) ReceiveBatch(batch SyncBatch) error {
    calculatedChecksum := sha256(batch.Records)
    if calculatedChecksum != batch.Checksum {
        return ErrChecksumMismatch  // Request re-sync
    }
    // Apply changes...
}
```

#### 3. **Delta Sync (Advanced)**

For large tables, send only diffs:

```go
type RecordDelta struct {
    RecordID   string
    ChangedFields map[string]interface{}  // Only changed columns
    Timestamp  time.Time
}

// Example: User changed email only
// Before: {id: 1, name: "Alice", email: "old@example.com", updated_at: "10:00"}
// After:  {id: 1, name: "Alice", email: "new@example.com", updated_at: "10:05"}
// Delta:  {id: 1, changes: {email: "new@example.com"}, timestamp: "10:05"}
```

#### 4. **Compression for Large Datasets**

```go
// Compress sync payload
func (s *SyncService) GetChanges(req *SyncRequest) (*SyncResponse, error) {
    changes := s.fetchChanges(req.Since)

    // Compress if large
    if len(changes) > 1000 {
        compressed := gzip.Compress(changes)
        return &SyncResponse{
            Data:       compressed,
            Compressed: true,
        }
    }

    return &SyncResponse{Data: changes}
}
```

#### 5. **Schema Version Tracking**

```sql
CREATE TABLE _sync_schema_version (
    version INTEGER PRIMARY KEY,
    applied_at TEXT NOT NULL,
    description TEXT
);

-- Prevent sync between incompatible schema versions
-- Node A: schema v1.2.0
-- Node B: schema v1.1.0
-- ‚Üí Sync blocked until Node B upgrades
```

### Comparison: ENX-Specific vs Generic Tool

| Feature | ENX-Specific | Generic Tool |
|---------|--------------|--------------|
| **Coupling** | Tightly coupled to ENX schema | Schema-agnostic |
| **Configuration** | Hardcoded tables | YAML config |
| **Reusability** | ENX only | Any SQLite app |
| **Maintenance** | Custom code | Community-driven |
| **Learning Curve** | ENX domain knowledge | SQLite + config |
| **Flexibility** | ENX-optimized | Universal |

### Recommended Approach

**‚úÖ Build Generic Tool First, Then Use in ENX**

```
Phase 1: Build sqlite-p2p-sync (Generic) - 4-6 weeks
‚îú‚îÄ‚îÄ Week 1-2: Core Components
‚îÇ   ‚îú‚îÄ‚îÄ Config parser (YAML ‚Üí Table metadata)
‚îÇ   ‚îú‚îÄ‚îÄ SQLite WAL integration
‚îÇ   ‚îú‚îÄ‚îÄ Metadata table auto-creation
‚îÇ   ‚îî‚îÄ‚îÄ Basic CRUD operations
‚îÇ
‚îú‚îÄ‚îÄ Week 3-4: Query API (Hybrid Approach) ‚úÖ CHOSEN
‚îÇ   ‚îú‚îÄ‚îÄ Structured APIs: Find/Insert/Update/Delete
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ JSON filter parser
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ WHERE clause builder
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Operator support ($gt, $lt, $in, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ Raw SQL APIs: Query/Execute
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SQL validation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Parameterized queries
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Query complexity limits
‚îÇ   ‚îî‚îÄ‚îÄ Security layer
‚îÇ       ‚îî‚îÄ‚îÄ Table whitelist
‚îÇ       ‚îî‚îÄ‚îÄ SQL keyword blocking
‚îÇ       ‚îî‚îÄ‚îÄ Rate limiting
‚îÇ
‚îî‚îÄ‚îÄ Week 5-6: Sync Engine (Basic)
    ‚îú‚îÄ‚îÄ P2P node discovery
    ‚îú‚îÄ‚îÄ Timestamp-based conflict resolution (assumes NTP configured)
    ‚îú‚îÄ‚îÄ Change tracking (trigger-based)
    ‚îú‚îÄ‚îÄ gRPC streaming for sync
    ‚îî‚îÄ‚îÄ ‚ùå No automatic clock checking (Phase 2)

Phase 2: Use in ENX - 1-2 weeks
‚îú‚îÄ‚îÄ Create enx-sync-config.yaml
‚îÇ   ‚îî‚îÄ‚îÄ Configure words, user_dicts, users tables
‚îÇ   ‚îî‚îÄ‚îÄ Set timestamp columns
‚îÇ   ‚îî‚îÄ‚îÄ Define conflict resolution rules
‚îú‚îÄ‚îÄ Start sqlite-p2p-sync service
‚îÇ   ‚îî‚îÄ‚îÄ Port 8091 (data service)
‚îÇ   ‚îî‚îÄ‚îÄ Auto-detect existing enx.db
‚îî‚îÄ‚îÄ Integrate enx-api
    ‚îî‚îÄ‚îÄ Replace direct SQLite calls with gRPC
    ‚îî‚îÄ‚îÄ Use Find() for simple queries
    ‚îî‚îÄ‚îÄ Use Query() for complex JOINs

Phase 3: Testing & Validation - 1 week
‚îú‚îÄ‚îÄ Unit tests (90% coverage target)
‚îú‚îÄ‚îÄ Integration tests (E2E sync scenarios)
‚îú‚îÄ‚îÄ Performance benchmarks
‚îÇ   ‚îî‚îÄ‚îÄ Simple queries: < 10ms
‚îÇ   ‚îî‚îÄ‚îÄ Complex queries: < 100ms
‚îÇ   ‚îî‚îÄ‚îÄ Sync operations: < 1s for 1000 records
‚îî‚îÄ‚îÄ Security audit
    ‚îî‚îÄ‚îÄ SQL injection tests
    ‚îî‚îÄ‚îÄ Rate limit validation
    ‚îî‚îÄ‚îÄ Access control verification

Phase 4: Open Source - 2-3 weeks
‚îú‚îÄ‚îÄ Documentation
‚îÇ   ‚îî‚îÄ‚îÄ README with quick start
‚îÇ   ‚îî‚îÄ‚îÄ API reference (all endpoints)
‚îÇ   ‚îî‚îÄ‚îÄ Configuration guide
‚îÇ   ‚îî‚îÄ‚îÄ Migration examples
‚îú‚îÄ‚îÄ Example projects
‚îÇ   ‚îî‚îÄ‚îÄ Simple note-taking app
‚îÇ   ‚îî‚îÄ‚îÄ Todo list with sync
‚îÇ   ‚îî‚îÄ‚îÄ Blog with multi-device editing
‚îú‚îÄ‚îÄ Publish to GitHub
‚îÇ   ‚îî‚îÄ‚îÄ Apache 2.0 license
‚îÇ   ‚îî‚îÄ‚îÄ CI/CD setup (GitHub Actions)
‚îÇ   ‚îî‚îÄ‚îÄ Docker images
‚îî‚îÄ‚îÄ Community building
    ‚îî‚îÄ‚îÄ Blog post announcement
    ‚îî‚îÄ‚îÄ Reddit/HackerNews post
    ‚îî‚îÄ‚îÄ Documentation website
```

**Total Timeline: 8-12 weeks from start to open source release**

### Benefits of Generic Approach

1. **‚úÖ Broader Impact**: Help other SQLite users with same problem
2. **‚úÖ Better Design**: Forced to think generically = cleaner architecture
3. **‚úÖ Community Support**: Others contribute features/bug fixes
4. **‚úÖ Portfolio Project**: Demonstrates architectural thinking
5. **‚úÖ Dogfooding**: ENX becomes first real-world user
6. **‚úÖ Learning**: Forces you to handle edge cases from different use cases

### Potential Project Name Ideas

- `sqlite-p2p-sync` - Clear and descriptive
- `sqlitesync` - Simple and memorable
- `litesync` - Short and catchy
- `dbsync` - Generic but might conflict
- `syncql` - Creative but less clear

### Next Steps

1. **Validate Design**: Review current ENX sync design
2. **Extract Generic Parts**: Identify business-agnostic components
3. **Define Config Schema**: Design YAML configuration format
4. **Build MVP**: Basic sync with single table
5. **Test with ENX**: Use ENX as first real user
6. **Open Source**: Publish when stable

## Conclusion

This architecture provides:

1. **Clear separation of concerns**: Business logic vs data management
2. **Flexible communication**: gRPC for performance, REST for debugging
3. **P2P synchronization**: No single point of failure
4. **Offline support**: Continue working, sync when online
5. **Scalability**: Easy to add nodes or upgrade storage
6. **Maintainability**: Well-defined interfaces and protocols

### Key Design Decisions ‚úÖ

**Protocol Layer:**
- ‚úÖ **Hybrid Approach** (gRPC + REST) gives us the best of both worlds: performance where it matters and ease of use for development and debugging

**Query API:**
- ‚úÖ **Hybrid Query API** (Structured + Raw SQL)
  - **80% use cases**: Structured APIs (Find/Insert/Update/Delete) for type safety and security
  - **20% edge cases**: Raw SQL (Query/Execute) for complex operations (JOINs, aggregations)
  - **Security**: Multi-layer validation, rate limiting, query complexity limits
  - **Performance**: Both approaches equally fast, optimized for different scenarios

**Why Hybrid Wins:**
```
Structured APIs (Find/Insert/Update/Delete):
  ‚úÖ Type-safe JSON filters
  ‚úÖ Automatic SQL generation
  ‚úÖ Built-in security (no SQL injection)
  ‚úÖ Easy to use for common cases

Raw SQL APIs (Query/Execute):
  ‚úÖ Full SQL power (JOINs, CTEs, aggregations)
  ‚úÖ Handle 20% edge cases
  ‚úÖ No feature limitations
  ‚úÖ Flexibility when needed

Best of both worlds:
  üéØ 80% of operations use safe, structured APIs
  üéØ 20% complex cases use powerful SQL APIs
  üéØ Same security layer protects both
  üéØ Developers choose the right tool for each job
```

### Future Vision üöÄ

**Generic Open Source Tool: `sqlite-p2p-sync`**

Extract the core sync logic into a generic open-source tool that can benefit the broader SQLite community while serving as the foundation for ENX's data synchronization needs.

**Target Timeline**: 8-12 weeks from start to open source release

**Expected Impact**:
- Help thousands of developers solve SQLite sync problems
- Build reputation in open source community
- Receive contributions and improvements from users
- Validate architecture with real-world use cases
- Create portfolio project demonstrating system design skills

### Existing Similar Projects üîç

Before building from scratch, let's examine existing SQLite synchronization solutions:

#### 1. **Litestream** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- **URL**: https://litestream.io/
- **Approach**: Streaming replication to cloud storage (S3, Azure Blob, etc.)
- **Pros**:
  - ‚úÖ Production-ready, battle-tested
  - ‚úÖ Continuous replication (near real-time)
  - ‚úÖ Point-in-time recovery
  - ‚úÖ Written in Go (good performance)
- **Cons**:
  - ‚ùå Not P2P (requires cloud storage)
  - ‚ùå One-way replication (master ‚Üí replica)
  - ‚ùå No conflict resolution (single writer only)
  - ‚ùå Doesn't work offline
- **Use Case**: Single-master with cloud backup, not suitable for multi-node sync

#### 2. **rqlite** ‚≠ê‚≠ê‚≠ê‚≠ê
- **URL**: https://github.com/rqlite/rqlite
- **Approach**: Distributed SQLite using Raft consensus
- **Pros**:
  - ‚úÖ Multi-node cluster
  - ‚úÖ Strong consistency (Raft)
  - ‚úÖ Fault tolerance
  - ‚úÖ HTTP API
- **Cons**:
  - ‚ùå Requires cluster (min 3 nodes)
  - ‚ùå Not offline-capable
  - ‚ùå Synchronous replication (higher latency)
  - ‚ùå Overkill for simple use cases
- **Use Case**: Distributed database cluster, not for offline P2P sync

#### 3. **LiteFS** ‚≠ê‚≠ê‚≠ê‚≠ê
- **URL**: https://github.com/superfly/litefs
- **Approach**: FUSE-based SQLite replication
- **Pros**:
  - ‚úÖ Transparent replication
  - ‚úÖ Multi-region support
  - ‚úÖ Read replicas
  - ‚úÖ Fast failover
- **Cons**:
  - ‚ùå Requires Consul for coordination
  - ‚ùå Single writer (no P2P)
  - ‚ùå Not offline-capable
  - ‚ùå FUSE overhead
- **Use Case**: Fly.io multi-region deployments, not for offline sync

#### 4. **cr-sqlite** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- **URL**: https://github.com/vlcn-io/cr-sqlite
- **Approach**: CRDT (Conflict-free Replicated Data Types) for SQLite
- **Pros**:
  - ‚úÖ True multi-master
  - ‚úÖ Offline-capable
  - ‚úÖ Automatic conflict resolution
  - ‚úÖ P2P sync ready
- **Cons**:
  - ‚ö†Ô∏è Requires schema changes (CRDT columns)
  - ‚ö†Ô∏è Complex CRDT semantics
  - ‚ö†Ô∏è Still in development (not 1.0)
  - ‚ö†Ô∏è SQLite extension (native code)
- **Use Case**: **Most similar to our needs**, but requires CRDT knowledge

#### 5. **ElectricSQL** ‚≠ê‚≠ê‚≠ê‚≠ê
- **URL**: https://electric-sql.com/
- **Approach**: Local-first SQLite with PostgreSQL sync
- **Pros**:
  - ‚úÖ Offline-first
  - ‚úÖ Multi-device sync
  - ‚úÖ Conflict resolution
  - ‚úÖ TypeScript SDK
- **Cons**:
  - ‚ùå Requires PostgreSQL backend
  - ‚ùå Not pure P2P (needs central server)
  - ‚ùå Complex setup
  - ‚ùå Opinionated architecture
- **Use Case**: Full-stack local-first apps with central DB

#### 6. **PouchDB/CouchDB** ‚≠ê‚≠ê‚≠ê‚≠ê
- **URL**: https://pouchdb.com/
- **Approach**: JavaScript document database with sync
- **Pros**:
  - ‚úÖ Battle-tested sync protocol
  - ‚úÖ Offline-first
  - ‚úÖ Bidirectional sync
  - ‚úÖ Conflict resolution
- **Cons**:
  - ‚ùå Not SQLite (document store)
  - ‚ùå JavaScript only
  - ‚ùå Different data model
  - ‚ùå Requires CouchDB server for multi-device
- **Use Case**: Web apps with offline sync, different paradigm

### Comparison Matrix

| Project | P2P | Offline | Conflict Resolution | SQLite Native | Complexity | Our Match |
|---------|-----|---------|-------------------|---------------|------------|-----------|
| **Litestream** | ‚ùå | ‚ùå | ‚ùå | ‚úÖ | Low | ‚≠ê‚≠ê |
| **rqlite** | ‚ö†Ô∏è (cluster) | ‚ùå | ‚úÖ | ‚úÖ | High | ‚≠ê‚≠ê |
| **LiteFS** | ‚ùå | ‚ùå | ‚ùå | ‚úÖ | Medium | ‚≠ê‚≠ê |
| **cr-sqlite** | ‚úÖ | ‚úÖ | ‚úÖ | ‚ö†Ô∏è (ext) | High | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **ElectricSQL** | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ | High | ‚≠ê‚≠ê‚≠ê |
| **PouchDB** | ‚úÖ | ‚úÖ | ‚úÖ | ‚ùå | Medium | ‚≠ê‚≠ê |
| **Our Design** | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | Medium | - |

### Why Build Our Own? ü§î

**None of the existing solutions perfectly match our requirements:**

```
Our Unique Requirements:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚úÖ Must have:
  1. True P2P sync (no central server required)
  2. Offline-capable (network-isolated environments)
  3. Works with existing SQLite databases (no schema changes)
  4. Timestamp-based conflict resolution (simple, predictable)
  5. Configuration-driven (no code changes)
  6. Simple deployment (single binary)

Existing solutions fall short:
  ‚Ä¢ Litestream: Not P2P, requires cloud storage
  ‚Ä¢ rqlite: Needs cluster, not offline-capable
  ‚Ä¢ LiteFS: Single writer, needs Consul
  ‚Ä¢ cr-sqlite: ‚≠ê Close, but requires CRDT schema changes
  ‚Ä¢ ElectricSQL: Needs PostgreSQL, not pure P2P
  ‚Ä¢ PouchDB: Not SQLite, different data model

Our sweet spot:
  üéØ Simple timestamp-based sync (no CRDT complexity)
  üéØ Works with existing databases (no migration)
  üéØ P2P without infrastructure (direct node-to-node)
  üéØ Offline-first by design
  üéØ Configuration over code
```

**Closest Match: cr-sqlite**

If we don't want to build from scratch, **cr-sqlite** is the closest match:

```bash
# Using cr-sqlite (if we choose not to build)
Pros:
  ‚úÖ Battle-tested CRDT implementation
  ‚úÖ True P2P sync
  ‚úÖ Offline-capable
  ‚úÖ Automatic conflict resolution
  ‚úÖ Active development

Cons:
  ‚ö†Ô∏è Requires modifying existing schema:
     ALTER TABLE words ADD COLUMN __crsql_version INTEGER;
     ALTER TABLE words ADD COLUMN __crsql_site_id BLOB;
  ‚ö†Ô∏è More complex than timestamp-based
  ‚ö†Ô∏è CRDT semantics can be confusing
  ‚ö†Ô∏è Requires C extension compilation

Decision:
  If simplicity > features ‚Üí Build our own (timestamp-based)
  If features > simplicity ‚Üí Use cr-sqlite (CRDT-based)
```

### Recommendation üí°

**Build Our Own, Learn from Existing Projects:**

```
Phase 1: MVP (4 weeks)
  ‚Ä¢ Study cr-sqlite's sync protocol
  ‚Ä¢ Borrow Litestream's WAL streaming approach
  ‚Ä¢ Implement simple timestamp-based sync
  ‚Ä¢ Prove concept with ENX

Phase 2: Production (4 weeks)
  ‚Ä¢ Add cr-sqlite's CRDT as optional advanced mode
  ‚Ä¢ Learn from rqlite's consistency guarantees
  ‚Ä¢ Implement LiteFS's failover patterns
  ‚Ä¢ Battle-test with real usage

Phase 3: Open Source (4 weeks)
  ‚Ä¢ Document differences from existing solutions
  ‚Ä¢ Explain why timestamp-based is simpler
  ‚Ä¢ Provide migration paths from other tools
  ‚Ä¢ Build community around simplicity

Total: 12 weeks to production-ready open source tool
```

**Unique Value Proposition:**

```
Our Tool vs. Existing Solutions:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Litestream: "We do P2P, not cloud-only"
rqlite: "We work offline, not cluster-only"
LiteFS: "We support multi-writer, not single-writer"
cr-sqlite: "We're simpler, no schema changes needed"
ElectricSQL: "We're pure P2P, no server required"
PouchDB: "We're SQLite-native, not document store"

Our niche:
  üéØ Simplest P2P sync for existing SQLite databases
  üéØ Zero schema changes, zero infrastructure
  üéØ Works offline, syncs when online
  üéØ Configuration over code
```

## Appendix: Alternative Protocol Options

### Option 1: Pure gRPC ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**Advantages**:
- ‚úÖ High performance (binary protocol, HTTP/2)
- ‚úÖ Strong typing (Protocol Buffers)
- ‚úÖ Built-in code generation (client/server)
- ‚úÖ Streaming support (real-time sync)
- ‚úÖ Cross-language support
- ‚úÖ Automatic serialization/deserialization

**Disadvantages**:
- ‚ö†Ô∏è Slightly more complex setup than REST
- ‚ö†Ô∏è Requires .proto file definitions
- ‚ö†Ô∏è More difficult to debug (binary protocol)

**Use Cases**:
- **Inter-service communication** (enx-api ‚Üî enx-data-service)
- **Node-to-node sync** (data-service ‚Üî data-service)
- High-frequency, low-latency operations

**Example Proto Definition**:

```protobuf
syntax = "proto3";

package enx.data;

service DataService {
  // Word operations
  rpc GetWord(GetWordRequest) returns (Word);
  rpc CreateWord(CreateWordRequest) returns (Word);
  rpc UpdateWord(UpdateWordRequest) returns (Word);
  rpc SearchWords(SearchWordsRequest) returns (SearchWordsResponse);

  // User dict operations
  rpc GetUserWords(GetUserWordsRequest) returns (GetUserWordsResponse);
  rpc MarkWord(MarkWordRequest) returns (Word);

  // Sync operations
  rpc GetChanges(GetChangesRequest) returns (stream Change);
  rpc PushChanges(stream Change) returns (PushChangesResponse);
}

message Word {
  int64 id = 1;
  string english = 2;
  string chinese = 3;
  string pronunciation = 4;
  string update_datetime = 5;
  int32 load_count = 6;
}

message GetWordRequest {
  string english = 1;
}

message GetChangesRequest {
  string since = 1;  // RFC3339 timestamp
  repeated string tables = 2;
}

message Change {
  string table = 1;
  string action = 2;  // insert, update, delete
  string data = 3;    // JSON payload
  string timestamp = 4;
}
```

**Why not chosen**: While gRPC offers excellent performance, the lack of easy debugging capabilities (binary protocol) makes it harder to troubleshoot issues during development and operations. The hybrid approach provides the same performance benefits while maintaining REST endpoints for debugging.

### Option 2: Pure REST/HTTP+JSON ‚≠ê‚≠ê‚≠ê‚≠ê

**Advantages**:
- ‚úÖ Simple and familiar
- ‚úÖ Easy to debug (curl, browser)
- ‚úÖ Human-readable (JSON)
- ‚úÖ Wide tooling support
- ‚úÖ No code generation needed

**Disadvantages**:
- ‚ö†Ô∏è Lower performance than gRPC
- ‚ö†Ô∏è No strong typing (runtime errors)
- ‚ö†Ô∏è Larger payload size (text vs binary)
- ‚ö†Ô∏è No streaming support (polling required)

**Use Cases**:
- Development and debugging
- Admin/monitoring endpoints
- Less critical operations

**Example API**:

```http
# Word operations
GET  /api/v1/words/:word
POST /api/v1/words
PUT  /api/v1/words/:id

# User dict operations
GET  /api/v1/user-dicts/:userId/words
POST /api/v1/user-dicts/mark

# Sync operations
GET  /api/v1/sync/changes?since=2025-11-12T10:00:00Z
POST /api/v1/sync/push
```

**Why not chosen**: REST is great for simplicity but lacks the performance and streaming capabilities needed for efficient P2P sync operations. For a data synchronization service handling frequent updates, gRPC's binary protocol and streaming support are essential. The hybrid approach keeps REST for non-critical operations while using gRPC where performance matters.

## References

- [gRPC Documentation](https://grpc.io/docs/)
- [Protocol Buffers](https://developers.google.com/protocol-buffers)
- [Microservices Patterns](https://microservices.io/patterns/)
- [Database Replication Strategies](https://en.wikipedia.org/wiki/Replication_(computing))
